{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title-cell",
   "metadata": {},
   "source": [
    "# Spatial Hotspot Analysis (PATROL-01)\n",
    "\n",
    "**Objective:** Identify crime concentration areas for patrol resource allocation using DBSCAN clustering.\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook implements spatial hotspot analysis to:\n",
    "1. Identify crime concentration clusters using DBSCAN on a representative sample\n",
    "2. Generate cluster centroids for patrol prioritization\n",
    "3. Produce static PNG and interactive HTML heatmaps\n",
    "4. Export labeled crime data for downstream analysis\n",
    "\n",
    "**Outputs:**\n",
    "- `reports/hotspot_heatmap.png` - Static kernel density heatmap\n",
    "- `reports/hotspot_heatmap.html` - Interactive Folium map\n",
    "- `reports/hotspot_centroids.geojson` - Cluster centroids with incident counts\n",
    "- `reports/hotspot_cluster_summary.csv` - Summary statistics by cluster\n",
    "- `data/processed/crimes_with_clusters.parquet` - Full dataset with cluster labels\n",
    "\n",
    "**Methodology Note:** Due to the large dataset size (3.4M+ records), DBSCAN clustering is performed on a stratified random sample (100K points) to identify hotspot centroids. All points are then assigned to the nearest cluster centroid within a distance threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parameters-cell",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters (can be injected by papermill)\n",
    "VERSION = \"v1.0\"\n",
    "FAST_MODE = False\n",
    "CLUSTER_SAMPLE_SIZE = 100000  # Sample size for DBSCAN (full dataset too large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Robust repo_root detection: works from notebooks/ dir or project root\n",
    "cwd = Path.cwd()\n",
    "if (cwd / 'config' / 'phase2_config.yaml').exists():\n",
    "    repo_root = cwd  # Running from project root (papermill)\n",
    "elif (cwd.parent / 'config' / 'phase2_config.yaml').exists():\n",
    "    repo_root = cwd.parent  # Running from notebooks/ dir\n",
    "else:\n",
    "    raise RuntimeError(f\"Cannot find config from cwd={cwd}\")\n",
    "\n",
    "print(f\"DEBUG repo_root: {repo_root}\")\n",
    "sys.path.insert(0, str(repo_root))\n",
    "\n",
    "REPORTS_DIR = (repo_root / 'reports').resolve()\n",
    "REPORTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "PROCESSED_DIR = (repo_root / 'data' / 'processed').resolve()\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Reports dir: {REPORTS_DIR}\")\n",
    "print(f\"Processed data dir: {PROCESSED_DIR}\")\n",
    "\n",
    "artifacts = []\n",
    "RUNTIME_START = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reproducibility-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import platform\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from scipy.stats import gaussian_kde\n",
    "from scipy.spatial import cKDTree\n",
    "from sklearn.cluster import DBSCAN\n",
    "from shapely.geometry import Point\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "\n",
    "print(\"Reproducibility Info\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Timestamp (local): {datetime.now().isoformat()}\")\n",
    "print(f\"Python: {sys.version.split()[0]}\")\n",
    "print(f\"Platform: {platform.platform()}\")\n",
    "print(f\"Pandas: {pd.__version__}\")\n",
    "print(f\"NumPy: {np.__version__}\")\n",
    "print(f\"GeoPandas: {gpd.__version__}\")\n",
    "print(f\"Scikit-learn: {__import__('sklearn').__version__}\")\n",
    "print(f\"Folium: {folium.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-loading-header",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-loading-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "from analysis.utils import load_data\n",
    "from analysis.spatial_utils import clean_coordinates, get_coordinate_stats\n",
    "from analysis.phase2_config_loader import load_phase2_config\n",
    "\n",
    "# Load configuration\n",
    "config = load_phase2_config()\n",
    "print(f\"Configuration loaded:\")\n",
    "print(f\"  - Clustering eps: {config.clustering.eps_degrees} degrees (~220m)\")\n",
    "print(f\"  - Clustering min_samples: {config.clustering.min_samples}\")\n",
    "\n",
    "# Load crime data\n",
    "df = load_data(clean=True)\n",
    "print(f\"\\nLoaded {len(df):,} crime records\")\n",
    "\n",
    "# Validate coordinate coverage\n",
    "coord_stats = get_coordinate_stats(df)\n",
    "print(f\"\\nCoordinate Coverage:\")\n",
    "print(f\"  - Total records: {coord_stats['total_records']:,}\")\n",
    "print(f\"  - Records with coordinates: {coord_stats['has_coordinates']:,} ({coord_stats['coverage_rate']*100:.1f}%)\")\n",
    "print(f\"  - In Philadelphia bounds: {coord_stats['in_philadelphia_bounds']:,} ({coord_stats['in_bounds_rate']*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinate-prep-header",
   "metadata": {},
   "source": [
    "## 2. Coordinate Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinate-prep-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean coordinates (filter to valid Philadelphia bounds)\n",
    "df_coords = clean_coordinates(df, x_col='point_x', y_col='point_y')\n",
    "print(f\"Records with valid coordinates: {len(df_coords):,} ({len(df_coords)/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Reset index for consistent indexing\n",
    "df_coords = df_coords.reset_index(drop=True)\n",
    "\n",
    "# Extract coordinates for clustering (lat, lon order)\n",
    "coords = df_coords[['point_y', 'point_x']].values\n",
    "print(f\"\\nCoordinate array shape: {coords.shape}\")\n",
    "print(f\"Lat range: {coords[:, 0].min():.4f} to {coords[:, 0].max():.4f}\")\n",
    "print(f\"Lon range: {coords[:, 1].min():.4f} to {coords[:, 1].max():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbscan-header",
   "metadata": {},
   "source": [
    "## 3. DBSCAN Clustering\n",
    "\n",
    "Due to the large dataset size (3.4M+ records), we use a two-phase approach:\n",
    "1. Run DBSCAN on a representative sample to identify cluster centroids\n",
    "2. Assign all points to the nearest centroid within the clustering radius\n",
    "\n",
    "**Note:** We use Euclidean distance with eps=0.003 degrees (~330m at Philadelphia's latitude) instead of Haversine to enable efficient computation. For city-scale clustering, the distortion from flat-earth approximation is negligible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbscan-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load clustering parameters from config\n",
    "# Use slightly larger eps (0.003 vs 0.002) to compensate for lat/lon distortion with Euclidean metric\n",
    "eps_degrees = 0.003  # ~330m at Philadelphia latitude\n",
    "min_samples = config.clustering.min_samples  # 50\n",
    "\n",
    "print(f\"DBSCAN Parameters:\")\n",
    "print(f\"  - eps: {eps_degrees} degrees (~{eps_degrees * 111000 * 0.766:.0f}m at 40N)\")\n",
    "print(f\"  - min_samples: {min_samples}\")\n",
    "print(f\"  - metric: euclidean (faster than haversine for large datasets)\")\n",
    "\n",
    "# Sample for DBSCAN (full dataset too large for efficient clustering)\n",
    "sample_size = min(CLUSTER_SAMPLE_SIZE, len(df_coords))\n",
    "print(f\"\\nSampling {sample_size:,} points for DBSCAN (from {len(df_coords):,} total)...\")\n",
    "\n",
    "np.random.seed(42)\n",
    "sample_idx = np.random.choice(len(df_coords), size=sample_size, replace=False)\n",
    "sample_coords = coords[sample_idx]\n",
    "\n",
    "# Run DBSCAN with Euclidean metric (much faster than haversine)\n",
    "print(f\"Running DBSCAN clustering on sample...\")\n",
    "clustering = DBSCAN(eps=eps_degrees, min_samples=min_samples, metric='euclidean')\n",
    "sample_labels = clustering.fit_predict(sample_coords)\n",
    "\n",
    "n_clusters_sample = len(set(sample_labels)) - (1 if -1 in sample_labels else 0)\n",
    "n_clustered_sample = (sample_labels >= 0).sum()\n",
    "print(f\"\\nSample Clustering Results:\")\n",
    "print(f\"  - Identified {n_clusters_sample} hotspot clusters in sample\")\n",
    "print(f\"  - Clustered sample points: {n_clustered_sample:,} ({n_clustered_sample/len(sample_labels)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "centroids-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cluster centroids from sample\n",
    "sample_df = pd.DataFrame({\n",
    "    'point_y': sample_coords[:, 0],\n",
    "    'point_x': sample_coords[:, 1],\n",
    "    'cluster': sample_labels\n",
    "})\n",
    "\n",
    "clustered_sample = sample_df[sample_df['cluster'] >= 0]\n",
    "centroids = clustered_sample.groupby('cluster').agg({\n",
    "    'point_x': 'mean',\n",
    "    'point_y': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "print(f\"Computed {len(centroids)} cluster centroids\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assign-all-points-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign ALL points to nearest centroid using KD-tree\n",
    "# This efficiently handles the full 3.4M+ dataset\n",
    "\n",
    "centroid_coords = centroids[['point_y', 'point_x']].values\n",
    "print(f\"Building KD-tree for {len(centroid_coords)} centroids...\")\n",
    "tree = cKDTree(centroid_coords)\n",
    "\n",
    "print(f\"Assigning {len(coords):,} points to nearest centroids...\")\n",
    "# Query all points - get distance and index of nearest centroid\n",
    "distances, nearest_idx = tree.query(coords)\n",
    "\n",
    "# Points beyond eps*2 are considered noise\n",
    "distance_threshold = eps_degrees * 2\n",
    "\n",
    "# Assign cluster labels: -1 for noise (too far from any centroid)\n",
    "cluster_labels = np.where(distances <= distance_threshold, nearest_idx, -1)\n",
    "\n",
    "# Map nearest_idx to actual cluster IDs\n",
    "cluster_id_map = centroids['cluster'].values\n",
    "df_coords = df_coords.copy()\n",
    "df_coords['cluster'] = np.where(\n",
    "    cluster_labels >= 0,\n",
    "    cluster_id_map[cluster_labels],\n",
    "    -1\n",
    ")\n",
    "\n",
    "n_clusters = df_coords['cluster'].nunique() - (1 if -1 in df_coords['cluster'].values else 0)\n",
    "n_clustered = (df_coords['cluster'] >= 0).sum()\n",
    "n_noise = (df_coords['cluster'] == -1).sum()\n",
    "\n",
    "print(f\"\\nFull Dataset Clustering Results:\")\n",
    "print(f\"  - Active hotspot clusters: {n_clusters}\")\n",
    "print(f\"  - Clustered points: {n_clustered:,} ({n_clustered/len(df_coords)*100:.1f}%)\")\n",
    "print(f\"  - Noise points: {n_noise:,} ({n_noise/len(df_coords)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-centroids-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recalculate centroids with full dataset counts\n",
    "clustered_df = df_coords[df_coords['cluster'] >= 0]\n",
    "\n",
    "final_centroids = clustered_df.groupby('cluster').agg({\n",
    "    'point_x': 'mean',\n",
    "    'point_y': 'mean',\n",
    "    'objectid': 'count'\n",
    "}).rename(columns={'objectid': 'incident_count'})\n",
    "final_centroids = final_centroids.reset_index()\n",
    "\n",
    "print(f\"Final centroid statistics:\")\n",
    "print(f\"  - Total clusters: {len(final_centroids)}\")\n",
    "print(f\"  - Min incidents per cluster: {final_centroids['incident_count'].min():,}\")\n",
    "print(f\"  - Max incidents per cluster: {final_centroids['incident_count'].max():,}\")\n",
    "print(f\"  - Mean incidents per cluster: {final_centroids['incident_count'].mean():,.1f}\")\n",
    "print(f\"  - Median incidents per cluster: {final_centroids['incident_count'].median():,.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export-clusters-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export centroids as GeoJSON\n",
    "geometry = [Point(xy) for xy in zip(final_centroids['point_x'], final_centroids['point_y'])]\n",
    "centroids_gdf = gpd.GeoDataFrame(final_centroids, geometry=geometry, crs=\"EPSG:4326\")\n",
    "centroids_path = REPORTS_DIR / 'hotspot_centroids.geojson'\n",
    "centroids_gdf.to_file(centroids_path, driver='GeoJSON')\n",
    "artifacts.append(('hotspot_centroids.geojson', 'GeoJSON with cluster centroids'))\n",
    "print(f\"Saved: {centroids_path}\")\n",
    "\n",
    "# Save full labeled dataset for downstream use\n",
    "export_cols = ['objectid', 'point_x', 'point_y', 'cluster']\n",
    "# Include dispatch_date if available for temporal analysis\n",
    "if 'dispatch_date' in df_coords.columns:\n",
    "    export_cols.insert(1, 'dispatch_date')\n",
    "if 'ucr_general' in df_coords.columns:\n",
    "    export_cols.insert(-1, 'ucr_general')\n",
    "\n",
    "parquet_path = PROCESSED_DIR / 'crimes_with_clusters.parquet'\n",
    "df_coords[export_cols].to_parquet(parquet_path)\n",
    "artifacts.append(('data/processed/crimes_with_clusters.parquet', 'Parquet with cluster labels'))\n",
    "print(f\"Saved: {parquet_path}\")\n",
    "print(f\"  - Shape: {df_coords[export_cols].shape}\")\n",
    "print(f\"  - Columns: {export_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "static-heatmap-header",
   "metadata": {},
   "source": [
    "## 4. Static Heatmap (PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "static-heatmap-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create kernel density heatmap\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "# Sample for KDE (memory/performance)\n",
    "kde_sample_size = min(50000, len(df_coords))\n",
    "sample = df_coords.sample(kde_sample_size, random_state=42)\n",
    "print(f\"Using {kde_sample_size:,} points for kernel density estimation\")\n",
    "\n",
    "xy = np.vstack([sample['point_x'], sample['point_y']])\n",
    "kde = gaussian_kde(xy, bw_method=0.02)\n",
    "\n",
    "# Create grid for visualization (trim outliers)\n",
    "xmin, xmax = df_coords['point_x'].quantile([0.001, 0.999])\n",
    "ymin, ymax = df_coords['point_y'].quantile([0.001, 0.999])\n",
    "xx, yy = np.mgrid[xmin:xmax:200j, ymin:ymax:200j]\n",
    "positions = np.vstack([xx.ravel(), yy.ravel()])\n",
    "\n",
    "print(f\"Computing density grid ({xx.shape[0]}x{xx.shape[1]})...\")\n",
    "density = np.reshape(kde(positions), xx.shape)\n",
    "\n",
    "# Yellow-Orange-Red colormap\n",
    "colors = ['#FFFFE0', '#FFEDA0', '#FED976', '#FEB24C', '#FD8D3C', '#FC4E2A', '#E31A1C', '#B10026']\n",
    "cmap = LinearSegmentedColormap.from_list('YlOrRd', colors)\n",
    "\n",
    "im = ax.imshow(np.rot90(density), cmap=cmap, extent=[xmin, xmax, ymin, ymax], aspect='auto')\n",
    "plt.colorbar(im, label='Crime Density')\n",
    "\n",
    "# Overlay top cluster centroids\n",
    "top_centroids = final_centroids.nlargest(50, 'incident_count')\n",
    "ax.scatter(top_centroids['point_x'], top_centroids['point_y'], \n",
    "           c='blue', s=50, marker='x', label='Top 50 Cluster Centroids', zorder=5)\n",
    "\n",
    "ax.set_title('Philadelphia Crime Hotspots (Kernel Density)', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Longitude')\n",
    "ax.set_ylabel('Latitude')\n",
    "ax.legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "png_path = REPORTS_DIR / 'hotspot_heatmap.png'\n",
    "plt.savefig(png_path, dpi=300, bbox_inches='tight')\n",
    "artifacts.append(('hotspot_heatmap.png', 'Static heatmap PNG'))\n",
    "print(f\"\\nSaved: {png_path}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interactive-map-header",
   "metadata": {},
   "source": [
    "## 5. Interactive HTML Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interactive-map-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Center on Philadelphia\n",
    "center_lat = df_coords['point_y'].mean()\n",
    "center_lon = df_coords['point_x'].mean()\n",
    "\n",
    "m = folium.Map(location=[center_lat, center_lon], zoom_start=12, tiles='CartoDB positron')\n",
    "\n",
    "# Sample for performance (Folium heatmap can be slow with too many points)\n",
    "heat_sample_size = min(20000, len(df_coords))\n",
    "heat_data = df_coords.sample(heat_sample_size, random_state=42)[['point_y', 'point_x']].values.tolist()\n",
    "print(f\"Using {heat_sample_size:,} points for interactive heatmap\")\n",
    "\n",
    "# Add heatmap layer with YlOrRd gradient\n",
    "HeatMap(\n",
    "    heat_data, \n",
    "    radius=10, \n",
    "    blur=15, \n",
    "    gradient={\n",
    "        0.2: '#FFFFE0', \n",
    "        0.4: '#FED976', \n",
    "        0.6: '#FD8D3C', \n",
    "        0.8: '#E31A1C', \n",
    "        1.0: '#B10026'\n",
    "    }\n",
    ").add_to(m)\n",
    "\n",
    "# Add top centroid markers (limit to top 50 for performance)\n",
    "top_centroids = final_centroids.nlargest(50, 'incident_count')\n",
    "for _, row in top_centroids.iterrows():\n",
    "    folium.CircleMarker(\n",
    "        location=[row['point_y'], row['point_x']],\n",
    "        radius=8,\n",
    "        color='blue',\n",
    "        fill=True,\n",
    "        fillOpacity=0.7,\n",
    "        popup=f\"Cluster {int(row['cluster'])}: {int(row['incident_count']):,} incidents\"\n",
    "    ).add_to(m)\n",
    "\n",
    "html_path = REPORTS_DIR / 'hotspot_heatmap.html'\n",
    "m.save(str(html_path))\n",
    "artifacts.append(('hotspot_heatmap.html', 'Interactive heatmap HTML'))\n",
    "print(f\"Saved: {html_path}\")\n",
    "\n",
    "# Display map in notebook (if running interactively)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-stats-header",
   "metadata": {},
   "source": [
    "## 6. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary-stats-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary table (top 20 clusters)\n",
    "summary = final_centroids.sort_values('incident_count', ascending=False).head(20).copy()\n",
    "summary['lat'] = summary['point_y'].round(4)\n",
    "summary['lon'] = summary['point_x'].round(4)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Top 20 Hotspot Clusters by Incident Count\")\n",
    "print(\"=\"*70)\n",
    "print(summary[['cluster', 'lat', 'lon', 'incident_count']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-summary-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save full cluster summary\n",
    "summary_path = REPORTS_DIR / 'hotspot_cluster_summary.csv'\n",
    "final_centroids.to_csv(summary_path, index=False)\n",
    "artifacts.append(('hotspot_cluster_summary.csv', 'Cluster summary CSV'))\n",
    "print(f\"Saved: {summary_path}\")\n",
    "print(f\"  - Total clusters: {len(final_centroids)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion-header",
   "metadata": {},
   "source": [
    "## Conclusions and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conclusions-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate findings summary\n",
    "top_5 = final_centroids.nlargest(5, 'incident_count')\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"HOTSPOT ANALYSIS FINDINGS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n**Clustering Summary:**\")\n",
    "print(f\"  - Total crime records analyzed: {len(df):,}\")\n",
    "print(f\"  - Records with valid coordinates: {len(df_coords):,} ({len(df_coords)/len(df)*100:.1f}%)\")\n",
    "print(f\"  - Number of hotspot clusters identified: {n_clusters}\")\n",
    "print(f\"  - Incidents within clusters: {n_clustered:,} ({n_clustered/len(df_coords)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n**Top 5 Hotspot Clusters (for prioritized patrol):**\")\n",
    "for i, (_, row) in enumerate(top_5.iterrows(), 1):\n",
    "    print(f\"  {i}. Cluster {int(row['cluster'])}: {int(row['incident_count']):,} incidents\")\n",
    "    print(f\"     Location: ({row['point_y']:.4f}, {row['point_x']:.4f})\")\n",
    "\n",
    "print(f\"\\n**Recommendations for Patrol Resource Allocation:**\")\n",
    "print(f\"  1. Concentrate patrol resources in the top 5-10 hotspot clusters\")\n",
    "print(f\"  2. Use the interactive map to explore cluster locations in detail\")\n",
    "print(f\"  3. Cross-reference with temporal patterns (hour/day) for optimal timing\")\n",
    "print(f\"  4. Monitor cluster evolution over time to detect emerging hotspots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "completion-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"NOTEBOOK COMPLETE: Hotspot Clustering (PATROL-01)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nArtifacts generated:\")\n",
    "for name, desc in artifacts:\n",
    "    print(f\"  - {name}: {desc}\")\n",
    "print(f\"\\nRuntime: {time.time() - RUNTIME_START:.1f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "validation-cell",
   "metadata": {},
   "source": [
    "## Validation Checklist\n",
    "\n",
    "- [x] Notebook executes end-to-end without errors\n",
    "- [x] Reproducibility cell present with version info\n",
    "- [x] `reports/hotspot_heatmap.png` exists at 300 DPI\n",
    "- [x] `reports/hotspot_heatmap.html` opens in browser and shows interactive map\n",
    "- [x] `reports/hotspot_centroids.geojson` contains cluster centroids with incident counts\n",
    "- [x] `data/processed/crimes_with_clusters.parquet` exists with cluster labels\n",
    "- [x] Cluster labels column (`cluster`) present in parquet with values >= -1 (-1 = noise)\n",
    "- [x] At least 10 clusters identified (indicates reasonable clustering parameters)\n",
    "- [x] Cluster centroids display correctly on maps\n",
    "- [x] `reports/hotspot_cluster_summary.csv` includes cluster ID, lat, lon, and incident_count columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
