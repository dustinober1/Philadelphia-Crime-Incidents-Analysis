# Plan 03-01: Infrastructure & External Data

**Phase:** 3 â€” Policy Deep Dives & Event Impacts
**Wave:** 1 (Infrastructure)
**Depends on:** Phase 2 complete

## Goal

Set up Phase 3 infrastructure including configuration file, corridor/transit data for vehicle crime analysis, and event calendar data for impact analysis.

## Context

From Phase 2, this plan will reuse:
- `analysis/spatial_utils.py` for coordinate operations
- `analysis/phase2_config_loader.py` pattern for config loading
- `data/boundaries/police_districts.geojson` for spatial reference

Implementation decisions from 03-CONTEXT.md:
- Corridor buffer distance: 500m
- Event categories: Sports (Eagles, Phillies, 76ers, Flyers) and Holidays
- Data sources: Claude's discretion (OSM, APIs, or manual GeoJSON)

## Tasks

### 1. Create Phase 3 Configuration File

**1.1 Create `config/phase3_config.yaml`**
```yaml
# Phase 3: Policy Deep Dives & Event Impacts
version: "1.0"

retail_theft:
  # "Thefts" general category - no separate retail code in data
  text_codes: ["Thefts"]
  ucr_codes: [600]
  analysis_years: [2019, 2020, 2021, 2022, 2023, 2024]
  baseline_years: [2018, 2019]
  verdict_threshold: 0.25  # 25% increase = "supported"

vehicle_crimes:
  text_codes: ["Theft from Vehicle", "Motor Vehicle Theft"]
  ucr_codes: [600, 700]  # UCR 600 includes theft from vehicle
  buffer_meters: 500  # ~5 city blocks
  analysis_years: [2019, 2020, 2021, 2022, 2023, 2024]

crime_composition:
  # UCR hundred-bands for violent crimes
  violent_ucr_bands: [1, 2, 3, 4]  # Homicide, Rape, Robbery, Agg Assault
  property_ucr_bands: [5, 6, 7]    # Burglary, Theft, Motor Vehicle Theft
  analysis_years: [2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024]

events:
  # Sports venues in Philadelphia
  venues:
    lincoln_financial_field:
      lat: 39.9008
      lon: -75.1675
      teams: ["Eagles"]
    citizens_bank_park:
      lat: 39.9056
      lon: -75.1665
      teams: ["Phillies"]
    wells_fargo_center:
      lat: 39.9012
      lon: -75.1720
      teams: ["76ers", "Flyers"]

  # Major holidays to analyze
  holidays:
    - name: "New Year's Day"
      month: 1
      day: 1
    - name: "Memorial Day"
      month: 5
      last_monday: true
    - name: "Independence Day"
      month: 7
      day: 4
    - name: "Labor Day"
      month: 9
      first_monday: true
    - name: "Thanksgiving"
      month: 11
      fourth_thursday: true

  buffer_days: 1  # Include day before/after for spillover
  control_weeks: 4  # Compare to same day-of-week in adjacent weeks

corridors:
  highways:
    - name: "I-95"
      osm_ref: "I 95"
    - name: "I-76"
      osm_ref: "I 76"
    - name: "I-676"
      osm_ref: "I 676"
    - name: "US-1"
      osm_ref: "US 1"
  septa_lines:
    - name: "Market-Frankford Line"
      type: "subway"
    - name: "Broad Street Line"
      type: "subway"

# Philadelphia coordinate bounds (reused from Phase 2)
coordinate_bounds:
  min_lon: -75.30
  max_lon: -74.95
  min_lat: 39.85
  max_lat: 40.15
```

### 2. Create Phase 3 Config Loader

**2.1 Create `analysis/phase3_config_loader.py`**
```python
"""Configuration loader for Phase 3 analyses."""

from pathlib import Path
from typing import Dict, Any, List
import yaml

class Phase3Config:
    """Load and access Phase 3 configuration parameters."""

    def __init__(self, config_path: Path = None):
        if config_path is None:
            repo_root = Path(__file__).resolve().parent.parent
            config_path = repo_root / "config" / "phase3_config.yaml"

        with open(config_path, 'r') as f:
            self._config = yaml.safe_load(f)

    @property
    def retail_theft(self) -> Dict[str, Any]:
        return self._config.get('retail_theft', {})

    @property
    def vehicle_crimes(self) -> Dict[str, Any]:
        return self._config.get('vehicle_crimes', {})

    @property
    def crime_composition(self) -> Dict[str, Any]:
        return self._config.get('crime_composition', {})

    @property
    def events(self) -> Dict[str, Any]:
        return self._config.get('events', {})

    @property
    def corridors(self) -> Dict[str, Any]:
        return self._config.get('corridors', {})

    @property
    def coordinate_bounds(self) -> Dict[str, float]:
        return self._config.get('coordinate_bounds', {})
```

### 3. Download/Create Corridor Data

**3.1 Create `scripts/download_corridors.py`**

Script to download or create corridor GeoJSON:
- Query OpenStreetMap Overpass API for highway routes in Philadelphia
- Create buffered corridor geometries
- Save to `data/boundaries/corridors.geojson`

```python
"""Download highway and transit corridor data for Phase 3."""

import geopandas as gpd
import requests
from shapely.geometry import LineString, shape
from shapely.ops import unary_union
import json
from pathlib import Path

def download_highways_osm(bbox: tuple) -> gpd.GeoDataFrame:
    """Download highway data from OpenStreetMap."""
    # Overpass API query for major highways
    overpass_url = "https://overpass-api.de/api/interpreter"
    query = f"""
    [out:json];
    (
      way["highway"="motorway"]["ref"~"I 95|I 76|I 676|US 1"]({bbox[0]},{bbox[1]},{bbox[2]},{bbox[3]});
      way["highway"="trunk"]["ref"~"I 95|I 76|I 676|US 1"]({bbox[0]},{bbox[1]},{bbox[2]},{bbox[3]});
    );
    out geom;
    """
    response = requests.get(overpass_url, params={'data': query})
    data = response.json()

    # Convert to GeoDataFrame
    features = []
    for element in data.get('elements', []):
        if element.get('type') == 'way' and 'geometry' in element:
            coords = [(n['lon'], n['lat']) for n in element['geometry']]
            if len(coords) >= 2:
                features.append({
                    'geometry': LineString(coords),
                    'name': element.get('tags', {}).get('ref', 'Unknown'),
                    'type': 'highway'
                })

    return gpd.GeoDataFrame(features, crs="EPSG:4326")

def create_septa_lines() -> gpd.GeoDataFrame:
    """Create simplified SEPTA subway line geometries.

    Note: For production, use official SEPTA GTFS data.
    These are approximate centerlines for corridor analysis.
    """
    # Approximate Market-Frankford Line (east-west)
    mfl_coords = [
        (-75.0825, 40.0170),  # Frankford Transportation Center
        (-75.0977, 40.0184),  # Church St
        (-75.1186, 40.0070),  # Berks
        (-75.1318, 39.9997),  # Spring Garden
        (-75.1556, 39.9546),  # City Hall
        (-75.1763, 39.9552),  # 30th St
        (-75.2096, 39.9623),  # 69th St Terminal
    ]

    # Approximate Broad Street Line (north-south)
    bsl_coords = [
        (-75.1497, 39.9293),  # AT&T Station (south)
        (-75.1557, 39.9428),  # Snyder
        (-75.1619, 39.9546),  # City Hall
        (-75.1556, 39.9710),  # Spring Garden
        (-75.1556, 39.9901),  # Erie
        (-75.1556, 40.0335),  # Fern Rock
    ]

    features = [
        {'geometry': LineString(mfl_coords), 'name': 'Market-Frankford Line', 'type': 'subway'},
        {'geometry': LineString(bsl_coords), 'name': 'Broad Street Line', 'type': 'subway'},
    ]

    return gpd.GeoDataFrame(features, crs="EPSG:4326")

def main():
    repo_root = Path(__file__).resolve().parent.parent
    output_path = repo_root / "data" / "boundaries" / "corridors.geojson"

    # Philadelphia bounding box
    bbox = (39.85, -75.30, 40.15, -74.95)  # min_lat, min_lon, max_lat, max_lon

    # Try OSM first, fall back to manual if it fails
    try:
        highways = download_highways_osm(bbox)
        print(f"Downloaded {len(highways)} highway segments from OSM")
    except Exception as e:
        print(f"OSM download failed: {e}")
        print("Using fallback manual highway definitions...")
        # Fallback: create approximate I-95 centerline
        i95_coords = [
            (-75.0495, 39.8559),  # South Philadelphia
            (-75.1381, 39.9283),  # Center City
            (-75.0892, 40.0170),  # Northeast Philadelphia
            (-75.0175, 40.1356),  # Bucks County border
        ]
        i76_coords = [
            (-75.1556, 39.9552),  # Center City
            (-75.2252, 39.9596),  # West Philadelphia
            (-75.2879, 39.9783),  # Manayunk
        ]
        highways = gpd.GeoDataFrame([
            {'geometry': LineString(i95_coords), 'name': 'I-95', 'type': 'highway'},
            {'geometry': LineString(i76_coords), 'name': 'I-76', 'type': 'highway'},
        ], crs="EPSG:4326")

    # Create SEPTA lines
    septa = create_septa_lines()
    print(f"Created {len(septa)} SEPTA line segments")

    # Combine and save
    corridors = gpd.GeoDataFrame(pd.concat([highways, septa], ignore_index=True), crs="EPSG:4326")
    corridors.to_file(output_path, driver='GeoJSON')
    print(f"Saved corridors to {output_path}")

if __name__ == "__main__":
    import pandas as pd
    main()
```

### 4. Create Event Calendar Data

**4.1 Create `scripts/create_event_calendar.py`**

Script to generate sports and holiday calendars:
- Use web sources for historical game schedules
- Generate federal holidays programmatically
- Save to `data/external/event_calendar.parquet`

```python
"""Create event calendar for Phase 3 event impact analysis."""

import pandas as pd
from datetime import datetime, timedelta
from pathlib import Path

def generate_holidays(start_year: int, end_year: int) -> pd.DataFrame:
    """Generate federal holiday dates."""
    from pandas.tseries.holiday import USFederalHolidayCalendar

    cal = USFederalHolidayCalendar()
    holidays = cal.holidays(start=f'{start_year}-01-01', end=f'{end_year}-12-31')

    df = pd.DataFrame({
        'date': holidays,
        'event_type': 'holiday',
        'event_name': 'Federal Holiday',  # Will be enriched
    })
    return df

def generate_sports_schedule(start_year: int, end_year: int) -> pd.DataFrame:
    """Generate approximate sports game dates.

    Note: For production, use official team APIs or sports-reference data.
    This creates approximate schedules based on typical season patterns.
    """
    games = []

    for year in range(start_year, end_year + 1):
        # Eagles (Sept-Dec, ~8 home games)
        eagles_dates = pd.date_range(f'{year}-09-01', f'{year}-12-31', freq='W-SUN')[:8]
        for date in eagles_dates:
            games.append({'date': date, 'event_type': 'sports', 'event_name': 'Eagles Home Game', 'team': 'Eagles'})

        # Phillies (Apr-Sept, ~81 home games)
        phillies_dates = pd.date_range(f'{year}-04-01', f'{year}-09-30', periods=40)
        for date in phillies_dates:
            games.append({'date': date.normalize(), 'event_type': 'sports', 'event_name': 'Phillies Home Game', 'team': 'Phillies'})

        # 76ers (Oct-Apr, ~41 home games)
        sixers_dates = pd.date_range(f'{year}-10-15', f'{year+1}-04-15', periods=20)
        for date in sixers_dates:
            games.append({'date': date.normalize(), 'event_type': 'sports', 'event_name': '76ers Home Game', 'team': '76ers'})

        # Flyers (Oct-Apr, ~41 home games)
        flyers_dates = pd.date_range(f'{year}-10-01', f'{year+1}-04-01', periods=20)
        for date in flyers_dates:
            games.append({'date': date.normalize(), 'event_type': 'sports', 'event_name': 'Flyers Home Game', 'team': 'Flyers'})

    return pd.DataFrame(games)

def main():
    repo_root = Path(__file__).resolve().parent.parent
    output_path = repo_root / "data" / "external" / "event_calendar.parquet"

    start_year, end_year = 2015, 2025

    # Generate calendars
    holidays = generate_holidays(start_year, end_year)
    sports = generate_sports_schedule(start_year, end_year)

    # Combine and deduplicate
    calendar = pd.concat([holidays, sports], ignore_index=True)
    calendar['date'] = pd.to_datetime(calendar['date']).dt.normalize()
    calendar = calendar.drop_duplicates(subset=['date', 'event_type', 'event_name'])

    # Save
    calendar.to_parquet(output_path, index=False)
    print(f"Created event calendar with {len(calendar)} events")
    print(f"Date range: {calendar['date'].min()} to {calendar['date'].max()}")
    print(f"Events by type:\n{calendar['event_type'].value_counts()}")

if __name__ == "__main__":
    main()
```

### 5. Create Event Analysis Utilities

**5.1 Extend `analysis/event_utils.py`**

Utility functions for event impact analysis:
```python
"""Event impact analysis utilities for Phase 3."""

from typing import List, Tuple
import pandas as pd
import numpy as np

def identify_event_days(
    crime_df: pd.DataFrame,
    event_df: pd.DataFrame,
    date_col: str = 'dispatch_date',
    buffer_days: int = 0
) -> pd.DataFrame:
    """Tag crime records with event-day indicators.

    Parameters
    ----------
    crime_df : pd.DataFrame
        Crime incidents with date column
    event_df : pd.DataFrame
        Event calendar with 'date' and 'event_type' columns
    date_col : str
        Name of date column in crime_df
    buffer_days : int
        Include N days before/after events

    Returns
    -------
    pd.DataFrame
        Crime data with event indicator columns added
    """
    crime_df = crime_df.copy()
    crime_df[date_col] = pd.to_datetime(crime_df[date_col]).dt.normalize()

    # Create expanded event dates with buffer
    event_dates = set()
    for date in event_df['date']:
        for offset in range(-buffer_days, buffer_days + 1):
            event_dates.add(date + pd.Timedelta(days=offset))

    crime_df['is_event_day'] = crime_df[date_col].isin(event_dates)

    # Add specific event type indicators
    for event_type in event_df['event_type'].unique():
        type_dates = set(event_df[event_df['event_type'] == event_type]['date'])
        expanded_dates = set()
        for date in type_dates:
            for offset in range(-buffer_days, buffer_days + 1):
                expanded_dates.add(date + pd.Timedelta(days=offset))
        crime_df[f'is_{event_type}_day'] = crime_df[date_col].isin(expanded_dates)

    return crime_df

def get_control_days(
    event_date: pd.Timestamp,
    all_dates: pd.DatetimeIndex,
    event_dates: set,
    n_controls: int = 4
) -> List[pd.Timestamp]:
    """Get control days for an event (same day-of-week, non-event days).

    Parameters
    ----------
    event_date : pd.Timestamp
        The event date
    all_dates : pd.DatetimeIndex
        All available dates in the crime data
    event_dates : set
        Set of all event dates to exclude
    n_controls : int
        Number of control days to find (before and after combined)

    Returns
    -------
    List[pd.Timestamp]
        Control day dates
    """
    dow = event_date.dayofweek
    controls = []

    # Look for same day-of-week in adjacent weeks
    for weeks in range(1, n_controls + 1):
        before = event_date - pd.Timedelta(weeks=weeks)
        after = event_date + pd.Timedelta(weeks=weeks)

        if before in all_dates and before not in event_dates:
            controls.append(before)
        if after in all_dates and after not in event_dates:
            controls.append(after)

        if len(controls) >= n_controls:
            break

    return controls[:n_controls]

def calculate_event_impact(
    crime_df: pd.DataFrame,
    event_df: pd.DataFrame,
    crime_category: str = None,
    date_col: str = 'dispatch_date'
) -> dict:
    """Calculate difference-in-means for event vs control days.

    Returns dict with mean counts, difference, and confidence interval.
    """
    df = crime_df.copy()
    df[date_col] = pd.to_datetime(df[date_col]).dt.normalize()

    if crime_category:
        df = df[df['crime_category'] == crime_category]

    # Daily counts
    daily_counts = df.groupby(date_col).size()

    event_dates = set(event_df['date'])
    event_counts = daily_counts[daily_counts.index.isin(event_dates)]
    control_counts = daily_counts[~daily_counts.index.isin(event_dates)]

    event_mean = event_counts.mean()
    control_mean = control_counts.mean()
    diff = event_mean - control_mean

    # Bootstrap confidence interval
    from scipy import stats
    ci = stats.ttest_ind(event_counts, control_counts)

    return {
        'event_mean': event_mean,
        'control_mean': control_mean,
        'difference': diff,
        'pct_change': (diff / control_mean * 100) if control_mean > 0 else None,
        'p_value': ci.pvalue,
        'n_event_days': len(event_counts),
        'n_control_days': len(control_counts),
    }
```

### 6. Run Infrastructure Scripts

**6.1 Execute corridor download**
```bash
python scripts/download_corridors.py
```

**6.2 Execute event calendar creation**
```bash
python scripts/create_event_calendar.py
```

**6.3 Validate outputs**
- `data/boundaries/corridors.geojson` exists with highway and SEPTA features
- `data/external/event_calendar.parquet` exists with events from 2015-2025
- `config/phase3_config.yaml` is valid YAML

## Validation Criteria

- [ ] `config/phase3_config.yaml` exists and is valid YAML
- [ ] `analysis/phase3_config_loader.py` loads config without errors
- [ ] `data/boundaries/corridors.geojson` contains highway and SEPTA geometries
- [ ] `data/external/event_calendar.parquet` contains holiday and sports events
- [ ] `analysis/event_utils.py` functions execute without errors
- [ ] All scripts run successfully from repo root

## Dependencies

- geopandas (for corridor geometries)
- pandas (for event calendar)
- requests (for OSM API, optional)
- PyYAML (for config loading)

## Estimated Time

- Config file: 15 min
- Config loader: 10 min
- Corridor download script: 25 min
- Event calendar script: 20 min
- Event utilities: 15 min
- Testing & validation: 5 min
- **Total: ~90 min**

---
*Plan created: 2026-02-03*
