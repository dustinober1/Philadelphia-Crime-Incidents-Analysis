---
phase: 04-dashboard-foundation
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - dashboard/components/__init__.py
  - dashboard/components/cache.py
autonomous: true

must_haves:
  truths:
    - "Data loading function loads full Parquet dataset with @st.cache_data decorator"
    - "Cached data avoids reloading 3.5M rows on each filter change"
    - "Cache configured with ttl=3600 and max_entries=10 for memory management"
    - "Loading includes coordinate validation and temporal feature extraction"
  artifacts:
    - path: "dashboard/components/__init__.py"
      provides: "Components package initialization"
      min_lines: 5
    - path: "dashboard/components/cache.py"
      provides: "Data loading with Streamlit caching"
      min_lines: 80
      exports: ["load_crime_data", "get_data_summary"]
  key_links:
    - from: "dashboard/components/cache.py"
      to: "analysis/utils.py"
      via: "Use load_data(), validate_coordinates(), extract_temporal_features()"
      pattern: "from analysis\\.utils import"
    - from: "dashboard/components/cache.py"
      to: "analysis/config.py"
      via: "Import CRIME_DATA_PATH for data path"
      pattern: "from analysis\\.config import"
    - from: "dashboard/components/cache.py"
      to: "data/crime_incidents_combined.parquet"
      via: "Load full Parquet dataset with pd.read_parquet"
      pattern: "pd\\.read_parquet"
---

<objective>
Implement data loading component with aggressive Streamlit caching for the 3.5M-row crime dataset.

Purpose: Enable fast dashboard startup (<5 seconds) by caching the full dataset in memory. The cache persists across user interactions, avoiding expensive Parquet I/O on each filter change. This is critical for the 5-second load time success criterion.

Output: Reusable data loading functions with @st.cache_data decorators that return validated, feature-enriched DataFrames.
</objective>

<execution_context>
@/Users/dustinober/.claude/get-shit-done/workflows/execute-plan.md
@/Users/dustinober/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/REQUIREMENTS.md
@.planning/phases/04-dashboard-foundation/04-CONTEXT.md
@.planning/phases/04-dashboard-foundation/04-RESEARCH.md

@analysis/config.py
@analysis/utils.py
@analysis/temporal_analysis.py
@analysis/categorical_analysis.py
</context>

<tasks>

<task type="auto">
  <name>Create cached data loading component</name>
  <files>dashboard/components/__init__.py, dashboard/components/cache.py</files>
  <action>
Create the dashboard components directory and data loading module:

1. **CREATE dashboard/components/__init__.py**:
```python
"""
Dashboard components.

Reusable components for data loading, filtering, and visualization.
"""

from dashboard.components.cache import load_crime_data, get_data_summary

__all__ = ["load_crime_data", "get_data_summary"]
```

2. **CREATE dashboard/components/cache.py**:
```python
"""
Data loading with Streamlit caching for the Philadelphia Crime Incidents dashboard.

Implements aggressive caching strategy to handle 3.5M-row Parquet dataset
with sub-5-second load times.
"""

import streamlit as st
import pandas as pd
from pathlib import Path

from analysis.config import CRIME_DATA_PATH
from analysis.utils import load_data, validate_coordinates, extract_temporal_features
from dashboard.config import CACHE_CONFIG


@st.cache_data(
    ttl=CACHE_CONFIG["data_ttl"],
    max_entries=CACHE_CONFIG["data_max_entries"],
    show_spinner="Loading crime data (3.5M rows, first load takes ~10s)...",
)
def load_crime_data(include_2026: bool = False) -> pd.DataFrame:
    """
    Load the full crime incidents dataset with caching.

    This function is cached by Streamlit. First call loads Parquet from disk
    (~10 seconds), subsequent calls return cached data instantly.

    Args:
        include_2026: If False, exclude 2026 data (incomplete year).
                      Default False to match analysis standards.

    Returns:
        DataFrame with validated coordinates and temporal features extracted.

    Example:
        >>> df = load_crime_data()
        >>> len(df)
        3490000+
    """
    # Load raw data using existing utility
    df = load_data(clean=False)

    # Exclude 2026 if requested (incomplete year)
    if not include_2026 and "dispatch_date" in df.columns:
        df = df[df["dispatch_date"].dt.year < 2026].copy()

    # Validate coordinates (adds valid_coord flag)
    df = validate_coordinates(df)

    # Extract temporal features (adds year, month, day, hour, etc.)
    df = extract_temporal_features(df)

    # Add crime category classification
    from analysis.utils import classify_crime_category
    df = classify_crime_category(df)

    return df


@st.cache_data(
    ttl=CACHE_CONFIG["filter_ttl"],
    max_entries=CACHE_CONFIG["filter_max_entries"],
    show_spinner="Applying filters...",
)
def apply_filters(
    df: pd.DataFrame,
    start_date: str | None = None,
    end_date: str | None = None,
    districts: list[int] | None = None,
    crime_categories: list[str] | None = None,
) -> pd.DataFrame:
    """
    Apply filters to the crime dataset with caching.

    Each unique filter combination creates a cache entry. Results are cached
    to avoid recomputing the same filtered view.

    Args:
        df: Full crime dataset (from load_crime_data).
        start_date: Start date filter (YYYY-MM-DD format or None for no filter).
        end_date: End date filter (YYYY-MM-DD format or None for no filter).
        districts: List of police districts to include (None for all).
        crime_categories: List of crime categories to include (None for all).

    Returns:
        Filtered DataFrame.

    Example:
        >>> df = load_crime_data()
        >>> filtered = apply_filters(df, start_date="2020-01-01", districts=[1, 2, 3])
        >>> len(filtered)
        # Returns count for districts 1-3 from 2020 onwards
    """
    result = df.copy()

    # Date range filter
    if start_date is not None and "dispatch_date" in result.columns:
        start_dt = pd.to_datetime(start_date)
        result = result[result["dispatch_date"] >= start_dt].copy()

    if end_date is not None and "dispatch_date" in result.columns:
        end_dt = pd.to_datetime(end_date)
        result = result[result["dispatch_date"] <= end_dt].copy()

    # District filter
    if districts is not None and "dc_dist" in result.columns:
        # Handle district values that may be strings or floats
        result = result[result["dc_dist"].isin(districts)].copy()

    # Crime category filter
    if crime_categories is not None and "crime_category" in result.columns:
        result = result[result["crime_category"].isin(crime_categories)].copy()

    return result


def get_data_summary(df: pd.DataFrame) -> dict:
    """
    Generate summary statistics for the dataset.

    Args:
        df: Crime dataset (full or filtered).

    Returns:
        Dict with summary statistics: total_records, date_range, districts,
        crime_types, coord_coverage.
    """
    summary = {
        "total_records": len(df),
    }

    # Date range
    if "dispatch_date" in df.columns:
        summary["date_range"] = (
            df["dispatch_date"].min().strftime("%Y-%m-%d"),
            df["dispatch_date"].max().strftime("%Y-%m-%d"),
        )
        summary["years"] = df["dispatch_date"].dt.year.nunique()

    # Districts
    if "dc_dist" in df.columns:
        summary["districts"] = df["dc_dist"].nunique()

    # Crime categories
    if "crime_category" in df.columns:
        summary["crime_categories"] = df["crime_category"].value_counts().to_dict()

    # Coordinate coverage
    if "valid_coord" in df.columns:
        coord_valid = df["valid_coord"].sum()
        coord_pct = (coord_valid / len(df) * 100) if len(df) > 0 else 0
        summary["coord_coverage"] = {
            "valid": int(coord_valid),
            "total": len(df),
            "percentage": round(coord_pct, 2),
        }

    return summary


@st.cache_data(show_spinner="Loading pre-computed reports...")
def load_cached_report(report_path: str) -> str | None:
    """
    Load a pre-generated markdown report for embedding.

    Used to embed existing analysis reports without re-running expensive
    computations.

    Args:
        report_path: Path to the markdown report file.

    Returns:
        Report content as string, or None if file not found.
    """
    path = Path(report_path)
    if not path.exists():
        return None

    return path.read_text()
```

3. **VERIFY the cache module works**:
```bash
cd "/Users/dustinober/Projects/Crime Incidents Philadelphia"
python -c "
import streamlit as st
# Mock streamlit for testing
class MockSt:
    class cache_data:
        def __init__(self, *args, **kwargs):
            pass
        def __call__(self, func):
            return func
st.cache_data = MockSt.cache_data
st.spinner = lambda x: lambda f: f

from dashboard.components.cache import load_crime_data, get_data_summary
df = load_crime_data()
summary = get_data_summary(df)
print('Records:', summary['total_records'])
print('Years:', summary['years'])
print('Coord coverage:', summary['coord_coverage']['percentage'], '%')
"
```

Key implementation notes:
- Use @st.cache_data with ttl=3600 (1 hour) and max_entries=10 for data loading
- Use @st.cache_data with ttl=1800 (30 min) and max_entries=50 for filter results
- Always apply validate_coordinates() and extract_temporal_features() after loading
- Exclude 2026 by default (incomplete year), allow optional inclusion
- Filter function caches by filter parameters - each unique combination gets its own cache entry
- Follow RESEARCH.md Pattern 1 for data loading with caching
- Use existing analysis/utils.py functions - no duplicated logic
</action>
  <verify>
python -c "
import streamlit as st
class MockCache:
    def __init__(self, *args, **kwargs): pass
    def __call__(self, func): return func
st.cache_data = MockCache
from dashboard.components.cache import load_crime_data, apply_filters, get_data_summary
df = load_crime_data()
print('Loaded:', len(df), 'records')
summary = get_data_summary(df)
print('Summary:', summary['total_records'], 'records,', summary['years'], 'years')
filtered = apply_filters(df, start_date='2020-01-01', end_date='2020-12-31')
print('Filtered 2020:', len(filtered), 'records')
"
</verify>
  <done>
load_crime_data() returns DataFrame with:
- valid_coord column from validate_coordinates()
- year, month, day, hour, etc. from extract_temporal_features()
- crime_category column from classify_crime_category()
- 2026 excluded by default

apply_filters() correctly filters by date range, districts, and crime categories

get_data_summary() returns dict with total_records, date_range, years, districts, coord_coverage

Functions use @st.cache_data decorator with appropriate ttl and max_entries
</done>
</task>

</tasks>

<verification>
After completion:
1. Verify module imports: `python -c "from dashboard.components.cache import load_crime_data, apply_filters, get_data_summary"`
2. Verify data loading: `python -c "from dashboard.components.cache import load_crime_data; df = load_crime_data(); print(len(df))"` should return ~3.4M
3. Verify columns exist: `python -c "from dashboard.components.cache import load_crime_data; df = load_crime_data(); print(df.columns.tolist())"` should include valid_coord, year, month, crime_category
4. Verify filtering works: Test apply_filters() with date range and district filters
5. Verify cache decorator is applied: Inspect source for @st.cache_data decorator
</verification>

<success_criteria>
Data loading completes in <15 seconds on first load, <1 second on cache hit. DataFrame includes validated coordinates and temporal features. Filtering produces correct subsets. Summary function returns accurate statistics. Cache configuration matches CACHE_CONFIG values.
</success_criteria>

<output>
After completion, create `.planning/phases/04-dashboard-foundation/04-02-SUMMARY.md`
</output>
