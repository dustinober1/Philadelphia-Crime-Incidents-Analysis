---
phase: 05-foundation-architecture
plan: 07
type: execute
wave: 4
depends_on: ["05-05"]
files_modified:
  - tests/test_data_loading.py
  - tests/test_data_validation.py
  - tests/test_data_preprocessing.py
autonomous: true
gap_closure: true

must_haves:
  truths:
    - "Developer can run 'pytest tests/test_data_loading.py' and all tests pass"
    - "Developer can run 'pytest tests/test_data_validation.py' and all tests pass"
    - "Developer can run 'pytest tests/test_data_preprocessing.py' and all tests pass"
    - "Developer can run 'pytest --cov analysis.data' and see 90%+ coverage for data layer"
    - "Developer can run caching test and see 2nd load is 10-20x faster than 1st"
  artifacts:
    - path: "tests/test_data_loading.py"
      provides: "Tests for load_crime_data, load_boundaries, load_external_data"
      min_lines: 80
      test_functions: "test_load_crime_data, test_cache_performance, test_load_boundaries"
    - path: "tests/test_data_validation.py"
      provides: "Tests for validate_crime_data, validate_coordinates, CrimeIncidentValidator"
      min_lines: 80
      test_functions: "test_validate_crime_data, test_validate_coordinates, test_crime_incident_validator"
    - path: "tests/test_data_preprocessing.py"
      provides: "Tests for filter_by_date_range, aggregate_by_period, add_temporal_features"
      min_lines: 80
      test_functions: "test_filter_by_date_range, test_aggregate_by_period, test_add_temporal_features"
  key_links:
    - from: "tests/test_data_loading.py"
      to: "analysis/data/loading.py"
      via: "pytest imports and tests load functions"
      pattern: "from analysis.data.loading import"
    - from: "tests/test_data_validation.py"
      to: "analysis/data/validation.py"
      via: "pytest imports and tests validation functions"
      pattern: "from analysis.data.validation import"
    - from: "tests/test_data_preprocessing.py"
      to: "analysis/data/preprocessing.py"
      via: "pytest imports and tests preprocessing functions"
      pattern: "from analysis.data.preprocessing import"
---

<objective>
Create tests for data layer modules (loading, validation, preprocessing) to achieve 90%+ test coverage and verify caching performance.

Purpose: The verification report identified that no tests exist for data layer modules and that caching behavior is not verified. This plan creates comprehensive tests for loading (with cache performance verification), validation, and preprocessing utilities.

Output: tests/test_data_loading.py, tests/test_data_validation.py, tests/test_data_preprocessing.py with comprehensive test coverage (>90%) for the data layer, including cache performance verification.
</objective>

<execution_context>
@/Users/dustinober/.claude/get-shit-done/workflows/execute-plan.md
@/Users/dustinober/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-foundation-architecture/05-VERIFICATION.md
@.planning/phases/05-foundation-architecture/05-02-SUMMARY.md

@analysis/data/loading.py
@analysis/data/validation.py
@analysis/data/preprocessing.py
@analysis/data/cache.py
@tests/test_phase2_spatial.py
@pyproject.toml
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create tests/test_data_loading.py with cache performance verification</name>
  <files>tests/test_data_loading.py</files>
  <action>
    Create tests/test_data_loading.py with comprehensive tests for loading functions and cache performance:

    1. Import required modules:
       ```python
       import pytest
       import time
       import pandas as pd
       from analysis.data import load_crime_data, load_boundaries, load_external_data, clear_cache
       ```

    2. Test load_crime_data basic functionality:
       - Test load_crime_data() returns DataFrame
       - Test load_crime_data(clean=True) has valid coordinates
       - Test load_crime_data(clean=False) includes all rows
       - Verify DataFrame has expected columns (objectid, dispatch_date, ucr_general, lat, lng)
       - Verify DataFrame is not empty

    3. Test cache performance (critical for Gap 4):
       - Test 1st load time vs 2nd load time
       - Clear cache before test: clear_cache()
       - Measure 1st load: start = time.time(); df1 = load_crime_data(); first_load = time.time() - start
       - Measure 2nd load: start = time.time(); df2 = load_crime_data(); second_load = time.time() - start
       - Verify speedup: assert first_load / second_load >= 5.0 (at least 5x faster, 10-20x expected)
       - Verify DataFrames are equal: pd.testing.assert_frame_equal(df1, df2)

    4. Test load_boundaries:
       - Test load_boundaries("police_districts") returns GeoJSON bytes or dict
       - Test load_boundaries("census_tracts") returns GeoJSON bytes or dict
       - Test invalid name raises ValueError or returns None

    5. Test load_external_data:
       - Test load_external_data() returns dict with expected keys
       - Verify weather data structure if present

    6. Test cache directory:
       - Verify .cache/joblib/ directory exists after first load
       - Verify clear_cache() removes cached files

    7. Mark slow tests with @pytest.mark.slow decorator

    Reference from 05-02-SUMMARY: Data loading uses joblib.Memory for caching. @memory.cache decorator provides 20x speedup on repeated loads. Cache directory is .cache/joblib/.

    Use pytest fixtures for sample data. Add docstrings to test functions.
  </action>
  <verify>
    Run: `pytest tests/test_data_loading.py -v --cov analysis/data/loading --cov-report=term-missing`. Expected: All tests pass, coverage >=90%, cache speedup test passes (5x+ faster).
  </verify>
  <done>
    - tests/test_data_loading.py created with >=80 lines
    - All tests pass
    - Coverage for loading.py >=90%
    - Cache performance test verifies 5x+ speedup on 2nd load
  </done>
</task>

<task type="auto">
  <name>Task 2: Create tests/test_data_validation.py with Pydantic validation</name>
  <files>tests/test_data_validation.py</files>
  <action>
    Create tests/test_data_validation.py with comprehensive tests for validation functions:

    1. Import required modules:
       ```python
       import pytest
       import pandas as pd
       from datetime import datetime
       from analysis.data.validation import validate_crime_data, validate_coordinates, CrimeIncidentValidator
       from pydantic import ValidationError
       ```

    2. Test CrimeIncidentValidator with valid data:
       - Test valid incident (all required fields, valid coordinates, valid UCR)
       - Test valid UCR codes in expanded format (100-9999)
       - Test valid PSA as letter code ("A", "E", "D")
       - Test valid coordinates within Philly bounds
       - Test valid datetime string

    3. Test CrimeIncidentValidator with invalid data:
       - Test invalid UCR code (outside 100-9999)
       - Test invalid coordinates (outside Philly bounds)
       - - Lat < 39.85 or > 40.15 should fail
       - - Lon < -75.30 or > -74.95 should fail
       - Test invalid datetime string
       - Test missing required fields

    4. Test validate_coordinates:
       - Test valid Philly coordinates pass
       - Test invalid coordinates return errors
       - Test boundary conditions (edge of Philly bounds)
       - Test None/NaN handling

    5. Test validate_crime_data:
       - Test with valid sample DataFrame (10 rows)
       - Test with DataFrame containing invalid rows (1-2 invalid)
       - Verify error_count matches expected
       - Verify error_details returned for invalid rows
       - Test validation with sample_size parameter

    6. Test coordinate bounds constants:
       - Import and test PHILLY_LON_MIN, PHILLY_LON_MAX, PHILLY_LAT_MIN, PHILLY_LAT_MAX from analysis.config

    Reference from 05-02-SUMMARY: Pydantic validation with Philly coordinate bounds (Lat: 39.85-40.15, Lon: -75.30 to -74.95). UCR codes validated in expanded format (100-9999). PSA validated as letter codes.

    Use pytest fixtures and parametrize for edge cases. Add docstrings to test functions.
  </action>
  <verify>
    Run: `pytest tests/test_data_validation.py -v --cov analysis/data/validation --cov-report=term-missing`. Expected: All tests pass, coverage >=90%.
  </verify>
  <done>
    - tests/test_data_validation.py created with >=80 lines
    - All tests pass
    - Coverage for validation.py >=90%
    - Pydantic validation tested with valid and invalid data
  </done>
</task>

<task type="auto">
  <name>Task 3: Create tests/test_data_preprocessing.py with date aggregation</name>
  <files>tests/test_data_preprocessing.py</files>
  <action>
    Create tests/test_data_preprocessing.py with comprehensive tests for preprocessing functions:

    1. Import required modules:
       ```python
       import pytest
       import pandas as pd
       from analysis.data.preprocessing import filter_by_date_range, aggregate_by_period, add_temporal_features
       ```

    2. Test filter_by_date_range:
       - Test filtering DataFrame by date range
       - Test start_date only (filter from date onwards)
       - Test end_date only (filter up to date)
       - Test both start_date and end_date
       - Test with date column as string vs datetime
       - Test invalid date handling

    3. Test aggregate_by_period:
       - Test daily aggregation (period='D')
       - Test weekly aggregation (period='W')
       - Test monthly aggregation (period='ME')
       - Test yearly aggregation (period='YE')
       - Test with count_col parameter (default 'objectid')
       - Verify index is DatetimeIndex or PeriodIndex
       - Verify column is named correctly ('count' or custom)

    4. Test add_temporal_features:
       - Test adding temporal features to DataFrame
       - Verify 'year', 'month', 'day', 'day_of_week', 'hour', 'is_weekend' columns added
       - Test with datetime column
       - Test with string datetime column that needs parsing

    5. Create sample DataFrame fixtures:
       - Create sample crime data with dispatch_date, objectid, ucr_general
       - Create sample data with invalid dates for error handling

    Reference from 05-02-SUMMARY: Preprocessing uses 'ME' for monthly aggregation (pandas 2.2+). Default count_col is 'objectid' (matches data schema). Temporal features use extract_temporal_features from utils.

    Use pytest fixtures for sample data. Add docstrings to test functions.
  </action>
  <verify>
    Run: `pytest tests/test_data_preprocessing.py -v --cov analysis/data/preprocessing --cov-report=term-missing`. Expected: All tests pass, coverage >=90%.
  </verify>
  <done>
    - tests/test_data_preprocessing.py created with >=80 lines
    - All tests pass
    - Coverage for preprocessing.py >=90%
    - Date filtering, aggregation, and temporal features tested
  </done>
</task>

</tasks>

<verification>
After all tasks complete:

1. Verify all data layer tests pass:
   ```bash
   pytest tests/test_data_loading.py tests/test_data_validation.py tests/test_data_preprocessing.py -v
   ```
   Expected: All tests pass (100% pass rate)

2. Verify coverage meets 90% target for data layer:
   ```bash
   pytest tests/test_data_loading.py tests/test_data_validation.py tests/test_data_preprocessing.py --cov analysis.data --cov-report=term-missing
   ```
   Expected: Coverage >=90% for loading.py, validation.py, preprocessing.py

3. Verify cache performance specifically:
   ```bash
   pytest tests/test_data_loading.py -k cache -v
   ```
   Expected: Cache performance test passes (2nd load is 5x+ faster than 1st)

4. Run all tests to ensure no regressions:
   ```bash
   pytest tests/ -v
   ```
   Expected: All tests pass (including test_classification.py, test_temporal.py, test_phase2_spatial.py)
</verification>

<success_criteria>
1. tests/test_data_loading.py created with >=80 lines
2. tests/test_data_validation.py created with >=80 lines
3. tests/test_data_preprocessing.py created with >=80 lines
4. All tests pass (100% pass rate)
5. Coverage >=90% for loading.py, validation.py, preprocessing.py
6. Cache performance test verifies 5x+ speedup (10-20x expected)
7. No regressions in existing tests
</success_criteria>

<output>
After completion, create `.planning/phases/05-foundation-architecture/05-07-SUMMARY.md`
</output>
