---
phase: 05-foundation-architecture
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - analysis/data/__init__.py
  - analysis/data/loading.py
  - analysis/data/validation.py
  - analysis/data/preprocessing.py
  - analysis/data/cache.py
  - analysis/config.py
  - .cache/joblib/.gitkeep
autonomous: true

must_haves:
  truths:
    - "Developer can import load_crime_data, validate_crime_data from analysis.data"
    - "Developer can clear data cache with clear_cache() function"
    - "Data is cached after first load, subsequent loads are faster"
    - "Invalid data (bad coordinates, missing dates) is caught by validation"
  artifacts:
    - path: "analysis/data/loading.py"
      provides: "Data loading functions with caching"
      exports: ["load_crime_data", "load_boundaries", "load_external_data"]
      min_lines: 80
    - path: "analysis/data/validation.py"
      provides: "Pydantic-based data validation"
      exports: ["validate_crime_data", "validate_coordinates", "CrimeIncidentValidator"]
      min_lines: 60
    - path: "analysis/data/cache.py"
      provides: "Caching layer using joblib.Memory"
      exports: ["memory", "clear_cache"]
      min_lines: 20
    - path: "analysis/data/preprocessing.py"
      provides: "Data transformation utilities"
      exports: ["filter_by_date_range", "aggregate_by_period"]
      min_lines: 50
  key_links:
    - from: "analysis/data/loading.py"
      to: "analysis/data/cache.py"
      via: "from .cache import memory"
      pattern: "@memory\\.cache"
    - from: "analysis/data/loading.py"
      to: "analysis/data/validation.py"
      via: "from .validation import validate_crime_data"
      pattern: "validate_crime_data"
    - from: "analysis/data/__init__.py"
      to: "analysis/data/loading.py"
      via: "from .loading import load_crime_data"
      pattern: "load_crime_data"
---

<objective>
Implement a new data layer with loading, validation, preprocessing, and caching capabilities to replace the current analysis.utils.load_data() function.

Purpose: The current load_data() function in utils.py is a simple wrapper without validation or caching. The new data layer provides type-safe data loading with Pydantic validation, joblib caching for performance, and preprocessing utilities for common data transformations.

Output: Complete data layer module under analysis/data/ with loading, validation, preprocessing, and cache modules.
</objective>

<execution_context>
@/Users/dustinober/.claude/get-shit-done/workflows/execute-plan.md
@/Users/dustinober/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-foundation-architecture/05-RESEARCH.md

@analysis/utils.py
@analysis/config.py
@analysis/spatial_utils.py
@environment.yml
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create data loading module with caching</name>
  <files>analysis/data/loading.py, analysis/data/cache.py, .cache/joblib/.gitkeep</files>
  <action>
    Create the data loading infrastructure with joblib caching:

    1. Create `analysis/data/` directory
    2. Create `.cache/joblib/` directory with `.gitkeep` file
    3. Create `analysis/data/cache.py`:
       - Import joblib.Memory
       - Configure cache location at project root/.cache/joblib
       - Export `memory` instance and `clear_cache()` function
    4. Create `analysis/data/loading.py` with:
       - `load_crime_data(clean: bool = True) -> pd.DataFrame` - loads from parquet with caching
       - `load_boundaries(name: str) -> gpd.GeoDataFrame` - loads police districts or census tracts with caching
       - `load_external_data(name: str) -> pd.DataFrame` - loads weather/economic data with caching

    Caching implementation (using @memory.cache decorator):
    ```python
    from joblib import Memory
    from pathlib import Path
    import pandas as pd

    CACHE_DIR = Path(__file__).parent.parent.parent / ".cache" / "joblib"
    memory = Memory(location=CACHE_DIR, verbose=0)

    @memory.cache
    def load_crime_data_parquet(clean: bool = True) -> pd.DataFrame:
        \"\"\"Load crime data from parquet with caching.

        Args:
            clean: Whether to drop rows with missing dispatch_date.

        Returns:
            DataFrame with parsed dispatch_date column.

        Raises:
            FileNotFoundError: If crime data file doesn't exist.
        \"\"\"
        from analysis.config import CRIME_DATA_PATH

        if not CRIME_DATA_PATH.exists():
            raise FileNotFoundError(f"Crime data not found: {CRIME_DATA_PATH}")

        df = pd.read_parquet(CRIME_DATA_PATH)

        # Parse dispatch_date
        if "dispatch_date" in df.columns:
            if df["dispatch_date"].dtype.name == "category":
                df["dispatch_date"] = pd.to_datetime(df["dispatch_date"].astype(str), errors="coerce")
            elif not pd.api.types.is_datetime64_any_dtype(df["dispatch_date"]):
                df["dispatch_date"] = pd.to_datetime(df["dispatch_date"], errors="coerce")

        if clean and "dispatch_date" in df.columns:
            df = df.dropna(subset=["dispatch_date"])

        return df

    def load_crime_data(clean: bool = True) -> pd.DataFrame:
        \"\"\"Load crime data (wrapper for cached function).\"\"\"
        return load_crime_data_parquet(clean=clean)
    ```

    5. Add type hints to all functions
    6. Add Google-style docstrings
    7. Handle date parsing consistently (category -> datetime conversion)
  </action>
  <verify>
    Run: `python -c "from analysis.data import load_crime_data; df = load_crime_data(); print(f'Loaded {len(df)} rows')"` and verify it loads data successfully. Run a second time and verify it's faster (cached). Run: `python -c "from analysis.data import clear_cache; clear_cache(); print('Cache cleared')"` and verify cache directory is emptied.
  </verify>
  <done>
    - analysis/data/cache.py exists with memory and clear_cache
    - analysis/data/loading.py exists with load_crime_data, load_boundaries, load_external_data
    - Data loading functions use @memory.cache decorator
    - .cache/joblib/.gitkeep exists
    - Functions have type hints and docstrings
    - Second load is faster than first (cached)
  </done>
</task>

<task type="auto">
  <name>Task 2: Create data validation module with Pydantic</name>
  <files>analysis/data/validation.py</files>
  <action>
    Create Pydantic-based data validation:

    1. Create `analysis/data/validation.py`
    2. Import pydantic (v2) BaseModel, Field, field_validator
    3. Create `CrimeIncidentValidator` BaseModel with:
       - incident_id: str
       - dispatch_date: datetime (required)
       - ucr_general: int (ge=100, le=999)
       - text_general_code: str
       - point_x: float | None (ge=-75.3, le=-74.95)
       - point_y: float | None (ge=39.85, le=40.15)
       - dc_key: int | None
       - psa: int | None (ge=1, le=99)
    4. Add field_validator for ucr_general to validate range
    5. Add field_validator for coordinates to validate Philadelphia bounds
    6. Create `validate_crime_data(df: pd.DataFrame) -> pd.DataFrame` function
    7. Create `validate_coordinates(df: pd.DataFrame, x_col: str, y_col: str) -> pd.DataFrame` function

    Validation implementation:
    ```python
    from pydantic import BaseModel, Field, field_validator, ValidationError
    from datetime import datetime
    from typing import Any
    import pandas as pd

    class CrimeIncidentValidator(BaseModel):
        \"\"\"Pydantic model for validating crime incident data.

        Coordinates are validated against Philadelphia bounds:
        - Longitude: -75.3 to -74.95
        - Latitude: 39.85 to 40.15
        \"\"\"

        incident_id: str = Field(..., description="Unique incident identifier")
        dispatch_date: datetime = Field(..., description="Dispatch timestamp")
        ucr_general: int = Field(..., ge=100, le=999, description="UCR code (100-999)")
        text_general_code: str = Field(..., description="Crime type description")
        point_x: float | None = Field(None, ge=-75.3, le=-74.95, description="Longitude")
        point_y: float | None = Field(None, ge=39.85, le=40.15, description="Latitude")
        dc_key: int | None = Field(None, description="District code")
        psa: int | None = Field(None, ge=1, le=99, description="Police service area")

        @field_validator("ucr_general")
        @classmethod
        def validate_ucr_code(cls, v: int) -> int:
            \"\"\"Validate UCR code is in expected range.\"\"\"
            if not 100 <= v <= 999:
                raise ValueError(f"UCR code must be 100-999, got {v}")
            return v

        @field_validator("point_x", "point_y")
        @classmethod
        def validate_philly_coords(cls, v: float | None) -> float | None:
            \"\"\"Validate coordinates are within Philadelphia bounds.\"\"\"
            if v is not None:
                if not (-75.3 <= v <= -74.95 or 39.85 <= v <= 40.15):
                    raise ValueError(f"Coordinate outside Philadelphia bounds: {v}")
            return v

    def validate_crime_data(df: pd.DataFrame, sample_size: int = 1000) -> pd.DataFrame:
        \"\"\"Validate crime data using Pydantic model.

        Args:
            df: DataFrame to validate.
            sample_size: Number of rows to validate (full validation is slow).

        Returns:
            Validated DataFrame.

        Raises:
            ValueError: If validation fails with details of errors.
        \"\"\"
        # Sample validation for performance
        sample = df.sample(min(len(df), sample_size)) if len(df) > sample_size else df

        errors = []
        for idx, row in sample.iterrows():
            try:
                CrimeIncidentValidator(**row.to_dict())
            except ValidationError as e:
                errors.append((idx, str(e)))

        if errors:
            error_msg = "\\n".join(f"Row {idx}: {err}" for idx, err in errors[:5])
            if len(errors) > 5:
                error_msg += f"\\n... and {len(errors) - 5} more errors"
            raise ValueError(f"Data validation failed:\\n{error_msg}")

        return df
    ```

    Note: Use sampling for performance (validating millions of rows row-by-row is slow).
  </action>
  <verify>
    Run: `python -c "from analysis.data import validate_crime_data, load_crime_data; df = load_crime_data(); validate_crime_data(df); print('Validation passed')"` and verify validation passes. Test with invalid data: `python -c "import pandas as pd; from analysis.data.validation import validate_coordinates; df = pd.DataFrame({'point_x': [-999], 'point_y': [999]}); validate_coordinates(df, 'point_x', 'point_y')"` and verify it raises ValueError.
  </verify>
  <done>
    - analysis/data/validation.py exists with CrimeIncidentValidator
    - validate_crime_data function validates DataFrame using Pydantic
    - validate_coordinates function filters invalid coordinates
    - Validation uses sampling for performance
    - Functions have type hints and docstrings
    - Invalid data raises ValueError with helpful error message
  </done>
</task>

<task type="auto">
  <name>Task 3: Create preprocessing module and data __init__.py</name>
  <files>analysis/data/preprocessing.py, analysis/data/__init__.py</files>
  <action>
    Create preprocessing utilities and module exports:

    1. Create `analysis/data/preprocessing.py` with:
       - `filter_by_date_range(df, start=None, end=None, date_col="dispatch_date") -> pd.DataFrame`
       - `aggregate_by_period(df, period="M", count_col="incident_id") -> pd.DataFrame`
       - `add_temporal_features(df) -> pd.DataFrame` (wrapper for utils.temporal.extract_temporal_features)

    2. Create `analysis/data/__init__.py` that exports:
       - load_crime_data, load_boundaries, load_external_data (from loading)
       - validate_crime_data, validate_coordinates (from validation)
       - filter_by_date_range, aggregate_by_period (from preprocessing)
       - clear_cache (from cache)

    3. Add `__all__` list to define public API

    4. Add module docstring explaining data layer

    Preprocessing implementation:
    ```python
    from typing import Any
    import pandas as pd

    def filter_by_date_range(
        df: pd.DataFrame,
        start: str | None = None,
        end: str | None = None,
        date_col: str = "dispatch_date",
    ) -> pd.DataFrame:
        \"\"\"Filter DataFrame by date range.

        Args:
            df: Input DataFrame with datetime column.
            start: Start date (ISO format string, e.g., '2020-01-01').
            end: End date (ISO format string, e.g., '2023-12-31').
            date_col: Name of datetime column.

        Returns:
            Filtered DataFrame.

        Raises:
            ValueError: If date_col not found in DataFrame.
        \"\"\"
        result = df.copy()

        if date_col not in result.columns:
            raise ValueError(f"Column '{date_col}' not found in DataFrame")

        if start is not None:
            result = result[result[date_col] >= pd.to_datetime(start)]

        if end is not None:
            result = result[result[date_col] <= pd.to_datetime(end)]

        return result

    def aggregate_by_period(
        df: pd.DataFrame,
        period: str = "M",
        count_col: str = "incident_id",
        date_col: str = "dispatch_date",
    ) -> pd.DataFrame:
        \"\"\"Aggregate crime counts by time period.

        Args:
            df: Input DataFrame with datetime column.
            period: Pandas resample period ('D'=daily, 'W'=weekly, 'M'=monthly, 'Y'=yearly).
            count_col: Column to count (default: incident_id).
            date_col: Datetime column to group by.

        Returns:
            DataFrame with period index and count column.
        \"\"\"
        if date_col not in df.columns:
            raise ValueError(f"Column '{date_col}' not found in DataFrame")

        df_copy = df.set_index(date_col)
        counts = df_copy.resample(period)[count_col].count().reset_index()
        counts.columns = [date_col, "count"]
        return counts
    ```

    5. Update `analysis/config.py` to add coordinate bounds constants (if not present):
       - PHILLY_LON_MIN = -75.3
       - PHILLY_LON_MAX = -74.95
       - PHILLY_LAT_MIN = 39.85
       - PHILLY_LAT_MAX = 40.15
  </action>
  <verify>
    Run: `python -c "from analysis.data import load_crime_data, filter_by_date_range, aggregate_by_period; df = load_crime_data(); filtered = filter_by_date_range(df, '2020-01-01', '2020-12-31'); print(f'Filtered: {len(filtered)} rows')"` and verify filtering works. Run: `python -c "from analysis.data import load_crime_data, aggregate_by_period; df = load_crime_data(); agg = aggregate_by_period(df, 'M'); print(agg.head())"` and verify aggregation works.
  </verify>
  <done>
    - analysis/data/preprocessing.py exists with filter_by_date_range and aggregate_by_period
    - analysis/data/__init__.py exports all public functions
    - __all__ list defines public API
    - Module docstring explains data layer
    - analysis/config.py has PHILLY_LON_MIN/MAX and PHILLY_LAT_MIN/MAX constants
    - All functions have type hints and docstrings
  </done>
</task>

</tasks>

<verification>
After all tasks complete:

1. Verify data layer structure:
   ```bash
   ls -la analysis/data/
   # Should show: __init__.py, loading.py, validation.py, preprocessing.py, cache.py
   ```

2. Verify data loading and caching:
   ```bash
   python -c "from analysis.data import load_crime_data; import time; t1=time.time(); df=load_crime_data(); t2=time.time(); print(f'First load: {t2-t1:.2f}s')"
   python -c "from analysis.data import load_crime_data; import time; t1=time.time(); df=load_crime_data(); t2=time.time(); print(f'Second load (cached): {t2-t1:.2f}s')"
   # Second load should be significantly faster
   ```

3. Verify validation:
   ```bash
   python -c "from analysis.data import load_crime_data, validate_crime_data; df = load_crime_data(); validate_crime_data(df); print('Validation passed')"
   ```

4. Verify preprocessing:
   ```bash
   python -c "from analysis.data import load_crime_data, filter_by_date_range; df = load_crime_data(); filtered = filter_by_date_range(df, '2020-01-01', '2020-12-31'); print(f'Filtered: {len(filtered)} rows')"
   ```
</verification>

<success_criteria>
1. Developer can import from analysis.data to load, validate, and preprocess crime data
2. Data loading uses joblib caching for performance
3. Pydantic validation catches invalid data
4. Preprocessing utilities work correctly
</success_criteria>

<output>
After completion, create `.planning/phases/05-foundation-architecture/05-02-SUMMARY.md`
</output>
