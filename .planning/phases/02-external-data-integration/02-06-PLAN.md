---
phase: 02-external-data-integration
plan: 06
type: execute
wave: 3
depends_on: [02-02, 02-04]
files_modified:
  - analysis/correlation_analysis.py
autonomous: true

user_setup:
  - service: FRED API
    why: "Unemployment rate data for economic correlation"
    env_vars:
      - name: FRED_API_KEY
        source: "FRED Dashboard -> My Account -> API Keys"

must_haves:
  truths:
    - "User can import analyze_economic_crime_correlation from analysis.correlation_analysis"
    - "User can compute correlations between crime and unemployment rate"
    - "Analysis is performed at monthly resolution (unemployment data frequency)"
    - "Results include detrending, p-values, confidence intervals, and effect sizes"
  artifacts:
    - path: "analysis/correlation_analysis.py"
      provides: "Crime-economic correlation analysis with statistical rigor"
      contains: "analyze_economic_crime_correlation", "compute_district_level_correlation"
  key_links:
    - from: "analysis/correlation_analysis.py"
      to: "analysis/external_data.py"
      via: "from analysis.external_data import fetch_fred_data, align_temporal_data"
      pattern: "from analysis.external_data import"
    - from: "analysis/correlation_analysis.py"
      to: "analysis/stats_utils.py"
      via: "from analysis.stats_utils import correlation_test, apply_fdr_correction, bootstrap_ci"
      pattern: "from analysis.stats_utils import"
---

<objective>
Create the crime-economic correlation analysis module using FRED API unemployment data for Philadelphia County. Compute correlations at monthly resolution with detrending, bootstrap confidence intervals, and statistical significance testing.

Purpose: Enable CORR-02 requirement: "User can view correlation analysis between crime patterns and economic indicators by district/area."

Output: Extended `analysis/correlation_analysis.py` with economic correlation functions including unemployment-crime correlation, bootstrap CIs, and interpretation of economic indicators.
</objective>

<execution_context>
@/Users/dustinober/.claude/get-shit-done/workflows/execute-plan.md
@/Users/dustinober/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-external-data-integration/02-RESEARCH.md
@analysis/config.py
@analysis/external_data.py
@analysis/correlation_analysis.py
@analysis/stats_utils.py
@analysis/utils.py
@.planning/phases/02-external-data-integration/02-02-SUMMARY.md
@.planning/phases/02-external-data-integration/02-04-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Add economic correlation analysis to correlation_analysis.py</name>
  <files>analysis/correlation_analysis.py</files>
  <action>
Add the following functions to `analysis/correlation_analysis.py`:

```python
from analysis.external_data import fetch_fred_data, load_cached_fred
from analysis.utils import load_data, validate_coordinates, extract_temporal_features
from sklearn.preprocessing import StandardScaler
import warnings


def analyze_economic_crime_correlation(
    economic_vars: list = None,
    resolution: str = "monthly",
    detrend: bool = True,
    include_bootstrap_ci: bool = True,
) -> Dict:
    """
    Analyze correlation between crime patterns and economic indicators.

    Tests correlations between crime counts and economic variables including
    unemployment rate. Monthly resolution is used because FRED data is
    reported monthly.

    Args:
        economic_vars: List of economic variable names. Defaults to unemployment.
        resolution: Temporal resolution ('monthly' or 'annual').
        detrend: If True, detrends both crime and economic series.
        include_bootstrap_ci: If True, computes 99% bootstrap confidence intervals.

    Returns:
        Dictionary with:
            - correlations: DataFrame of economic variable, correlation, p_value
            - bootstrap_ci: DataFrame of bootstrap CIs for each correlation
            - raw_data: Dict of aligned crime and economic series
            - detrend_used: Whether detrending was applied
            - metadata: Analysis metadata

    Note:
        Economic variables available from FRED:
            - PAPHIL5URN: Philadelphia County unemployment rate (default)

        For district-level analysis, see compute_district_level_correlation().

    Example:
        >>> results = analyze_economic_crime_correlation()
        >>> print(results['correlations'])
    """
    # Set seed for reproducibility
    set_global_seed(STAT_CONFIG["random_seed"])

    # Default to unemployment rate
    if economic_vars is None:
        economic_vars = ['unemployment_rate']

    # Load data
    crime_df = load_data()
    crime_df = validate_coordinates(crime_df)
    crime_df = extract_temporal_features(crime_df)

    # Fetch economic data
    unemployment_df = fetch_fred_data(
        series_id="PAPHIL5URN",
        start_date=TEMPORAL_CONFIG[f"{resolution}_start"],
        end_date=TEMPORAL_CONFIG[f"{resolution}_end"],
    )

    # Align to monthly resolution
    aligned = align_temporal_data(
        crime_df=crime_df,
        unemployment_df=unemployment_df,
        resolution=resolution,
    )

    # Rename unemployment column
    if 'value' in aligned.columns:
        aligned = aligned.rename(columns={'value': 'unemployment_rate'})

    results = []
    ci_results = []

    for var in economic_vars:
        if var not in aligned.columns:
            warnings.warn(f"Economic variable '{var}' not found in aligned data.")
            continue

        crime = aligned['crime_count'].dropna()
        economic = aligned[var].dropna()

        # Align to common dates
        common_idx = crime.index.intersection(economic.index)
        crime_aligned = crime.loc[common_idx]
        economic_aligned = economic.loc[common_idx]

        # Store raw data
        raw_crime = crime_aligned.copy()
        raw_economic = economic_aligned.copy()

        # Detrend if requested
        if detrend:
            crime_aligned = detrend_series(crime_aligned, method='linear')
            economic_aligned = detrend_series(economic_aligned, method='linear')

        # Compute correlation (Spearman for robustness to non-normality)
        corr_result = correlation_test(
            economic_aligned.values,
            crime_aligned.values,
            method='spearman'
        )

        results.append({
            'variable': var,
            'correlation': corr_result['correlation'],
            'p_value': corr_result['p_value'],
            'test': corr_result['test_name'],
            'n': len(crime_aligned),
            'mean_crime': raw_crime.mean(),
            'std_crime': raw_crime.std(),
            'mean_economic': raw_economic.mean(),
            'std_economic': raw_economic.std(),
        })

        # Bootstrap confidence interval for correlation
        if include_bootstrap_ci:
            def corr_statistic(data):
                """Correlation for bootstrap."""
                x = data[:, 0]
                y = data[:, 1]
                from scipy.stats import spearmanr
                corr, _ = spearmanr(x, y)
                return corr if not np.isnan(corr) else 0.0

            # Stack data for bootstrap
            combined = np.column_stack([economic_aligned.values, crime_aligned.values])

            try:
                ci_result = bootstrap_ci(
                    combined,
                    corr_statistic,
                    confidence_level=STAT_CONFIG['confidence_level'],
                    n_resamples=STAT_CONFIG['bootstrap_n_resamples'],
                    random_state=STAT_CONFIG['bootstrap_random_state'],
                )

                ci_results.append({
                    'variable': var,
                    'ci_lower': ci_result[0],
                    'ci_upper': ci_result[1],
                    'point_estimate': ci_result[2],
                    'stderr': ci_result[3],
                })
            except Exception as e:
                warnings.warn(f"Bootstrap CI failed for {var}: {e}")

    # Create DataFrame and apply FDR correction
    corr_df = pd.DataFrame(results)
    if not corr_df.empty:
        p_values = corr_df['p_value'].values
        corr_df['p_value_fdr'] = apply_fdr_correction(p_values, method=STAT_CONFIG['fdr_method'])
        corr_df['is_significant'] = corr_df['p_value_fdr'] < STAT_CONFIG['alpha']
        corr_df['is_significant_raw'] = corr_df['p_value'] < STAT_CONFIG['alpha']

        # Effect size interpretation
        corr_df['effect_size'] = corr_df['correlation'].abs().apply(_interpret_correlation_effect)

    # Bootstrap CI DataFrame
    ci_df = pd.DataFrame(ci_results) if ci_results else pd.DataFrame()

    return {
        'correlations': corr_df,
        'bootstrap_ci': ci_df,
        'raw_data': {
            'crime': raw_crime if 'raw_crime' in locals() else None,
            'economic': raw_economic if 'raw_economic' in locals() else None,
        },
        'detrend_used': detrend,
        'resolution': resolution,
        'metadata': get_analysis_metadata(),
    }


def _interpret_correlation_effect(abs_r: float) -> str:
    """
    Interpret correlation coefficient as effect size.

    Guidelines for interpreting |r| (Cohen, 1988):
        - Small: 0.1
        - Medium: 0.3
        - Large: 0.5

    Args:
        abs_r: Absolute value of correlation coefficient.

    Returns:
        Interpretation string.
    """
    if abs_r < 0.1:
        return "negligible"
    elif abs_r < 0.3:
        return "small"
    elif abs_r < 0.5:
        return "medium"
    else:
        return "large"


def compute_district_level_correlation(
    districts: list = None,
    economic_var: str = 'unemployment_rate',
) -> pd.DataFrame:
    """
    Compute crime-economic correlation by police district.

    Note: This function is a placeholder for future implementation.
    Requires economic data at district/tract level, which needs:
    1. Census tract to police district crosswalk (OpenDataPhilly)
    2. Census API data aggregation to districts
    3. Alignment with crime data by district

    Args:
        districts: List of district numbers. If None, uses all districts.
        economic_var: Economic variable to correlate (future: poverty_rate, median_income).

    Returns:
        Currently returns None with message about data requirements.

    Todo:
        - Download OpenDataPhilly census-to-district crosswalk
        - Aggregate Census tract data to police districts
        - Implement district-level correlation analysis
    """
    warnings.warn(
        "District-level economic correlation requires Census tract to "
        "police district crosswalk. See Phase 2 research notes for "
        "OpenDataPhilly crosswalk dataset. "
        "Currently only city-level unemployment correlation is available."
    )
    return None


def compare_periods(
    crime_df: pd.DataFrame = None,
    economic_series: pd.Series = None,
    high_threshold: float = None,
    low_threshold: float = None,
) -> Dict:
    """
    Compare crime rates between high and low economic periods.

    Tests whether crime rates differ significantly between periods of
    high vs low unemployment (or other economic indicator).

    Args:
        crime_df: Crime DataFrame with dispatch_date column.
        economic_series: Economic indicator series (e.g., unemployment).
        high_threshold: Value above which is "high" period. If None, uses top quartile.
        low_threshold: Value below which is "low" period. If None, uses bottom quartile.

    Returns:
        Dictionary with:
            - high_crime_mean: Mean crime count during high economic indicator periods
            - low_crime_mean: Mean crime count during low economic indicator periods
            - difference: Absolute difference in means
            - test_result: Mann-Whitney U test results
            - is_significant: Whether difference is statistically significant

    Example:
        >>> result = compare_periods(economic_series=unemployment_series)
        >>> print(f'High unemployment crime: {result[\"high_crime_mean\"]:.2f}')
        >>> print(f'Low unemployment crime: {result[\"low_crime_mean\"]:.2f}')
    """
    # Set seed for reproducibility
    set_global_seed(STAT_CONFIG["random_seed"])

    if crime_df is None:
        crime_df = load_data()

    crime_df = extract_temporal_features(crime_df)

    # Aggregate to monthly
    from analysis.external_data import aggregate_crime_by_period
    crime_monthly = aggregate_crime_by_period(crime_df, period='M')

    # Align with economic series
    common_idx = crime_monthly.index.intersection(economic_series.index)
    crime_aligned = crime_monthly.loc[common_idx]['crime_count']
    economic_aligned = economic_series.loc[common_idx]

    # Determine thresholds
    if high_threshold is None:
        high_threshold = economic_aligned.quantile(0.75)
    if low_threshold is None:
        low_threshold = economic_aligned.quantile(0.25)

    # Split into high/low periods
    high_periods = economic_aligned >= high_threshold
    low_periods = economic_aligned <= low_threshold

    high_crime = crime_aligned[high_periods]
    low_crime = crime_aligned[low_periods]

    # Compute statistics
    from analysis.stats_utils import compare_two_samples, cohens_d
    from scipy.stats import mannwhitneyu

    test_result = compare_two_samples(
        high_crime.values,
        low_crime.values,
        alpha=STAT_CONFIG['alpha']
    )

    # Cohen's d
    d = cohens_d(high_crime.values, low_crime.values)

    return {
        'high_crime_mean': float(high_crime.mean()),
        'high_crime_std': float(high_crime.std()),
        'high_crime_n': len(high_crime),
        'low_crime_mean': float(low_crime.mean()),
        'low_crime_std': float(low_crime.std()),
        'low_crime_n': len(low_crime),
        'difference': float(high_crime.mean() - low_crime.mean()),
        'percent_diff': float((high_crime.mean() - low_crime.mean()) / low_crime.mean() * 100),
        'test_result': test_result,
        'cohens_d': d,
        'effect_size': _interpret_cohens_d(d),
        'thresholds': {
            'high': high_threshold,
            'low': low_threshold,
        },
        'metadata': get_analysis_metadata(),
    }


def _interpret_cohens_d(d: float) -> str:
    """Interpret Cohen's d effect size."""
    abs_d = abs(d)
    if abs_d < STAT_CONFIG['effect_size_small']:
        return "negligible"
    elif abs_d < STAT_CONFIG['effect_size_medium']:
        return "small"
    elif abs_d < STAT_CONFIG['effect_size_large']:
        return "medium"
    else:
        return "large"
```
  </action>
  <verify>
```bash
python -c "
from analysis.correlation_analysis import (
    analyze_economic_crime_correlation,
    compute_district_level_correlation,
    compare_periods
)
print('All functions imported successfully')

# Test compare_periods with mock data
import pandas as pd
import numpy as np

# Create mock economic series
dates = pd.date_range('2020-01-01', '2022-12-31', freq='M')
economic = pd.Series(np.random.randn(len(dates)) * 2 + 6, index=dates)  # Mean 6% unemployment

try:
    result = compare_periods(economic_series=economic)
    print('compare_periods works (will fail on actual data fetch)')
    print(f'Keys: {list(result.keys())}')
except Exception as e:
    if 'load_data' in str(e) or 'fetch' in str(e):
        print('Expected: Data not yet available')
    else:
        print(f'Error: {e}')
"
```
  </verify>
  <done>
analyze_economic_crime_correlation, compute_district_level_correlation, and compare_periods functions exist in correlation_analysis.py. Functions import correctly and have proper docstrings.
  </done>
</task>

</tasks>

<verification>
After completing all tasks:

1. Run the verify command from task 1 to confirm functions exist and import
2. Check that analyze_economic_crime_correlation includes unemployment rate as default
3. Verify bootstrap CI computation is included
4. Verify compare_periods function exists for high/low economic period comparison
5. Check that _interpret_correlation_effect and _interpret_cohens_d helpers exist
</verification>

<success_criteria>
1. analyze_economic_crime_correlation exists with unemployment_rate as default variable
2. Function supports monthly and annual resolution
3. Bootstrap CI computation is included
4. FDR correction is applied to p-values
5. Effect size interpretation is provided for correlations
6. compare_periods function exists for comparing high/low economic periods
7. compute_district_level_correlation placeholder exists with TODO for crosswalk data
8. Helper functions for effect size interpretation exist
</success_criteria>

<output>
After completion, create `.planning/phases/02-external-data-integration/02-06-SUMMARY.md` with:
- Economic variables supported (PAPHIL5URN unemployment)
- Resolution options (monthly, annual)
- Statistical methods used (Spearman correlation, bootstrap CI, Mann-Whitney U)
- Placeholder status for district-level analysis (needs crosswalk)
</output>
