---
phase: 02-external-data-integration
plan: 05
type: execute
wave: 3
depends_on: [02-01, 02-04]
files_modified:
  - analysis/correlation_analysis.py
  - analysis/external_data.py
autonomous: true

must_haves:
  truths:
    - "User can import analyze_weather_crime_correlation from analysis.correlation_analysis"
    - "User can compute Spearman correlations between crime and weather variables"
    - "Correlation analysis includes detrending to avoid spurious correlations"
    - "Results include p-values, 99% confidence intervals, and effect size interpretations"
  artifacts:
    - path: "analysis/correlation_analysis.py"
      provides: "Crime-weather correlation analysis with statistical rigor"
      min_lines: 150
      exports: ["analyze_weather_crime_correlation", "compute_lagged_correlation"]
  key_links:
    - from: "analysis/correlation_analysis.py"
      to: "analysis/external_data.py"
      via: "from analysis.external_data import align_temporal_data, fetch_weather_data"
      pattern: "from analysis.external_data import"
    - from: "analysis/correlation_analysis.py"
      to: "analysis/stats_utils.py"
      via: "from analysis.stats_utils import correlation_test, apply_fdr_correction"
      pattern: "from analysis.stats_utils import"
---

<objective>
Create the crime-weather correlation analysis module using Spearman rank correlation with detrending to address spurious correlation risk from 20-year trend drift. Compute lagged correlations (1-7 days) to test delayed weather effects on crime.

Purpose: Enable CORR-01 requirement: "User can view correlation analysis between crime incidence and weather variables with appropriate detrending."

Output: New `analysis/correlation_analysis.py` module with weather correlation functions, extended `external_data.py` with detrending utilities, and statistical test results with p-values, CIs, and effect sizes.
</objective>

<execution_context>
@/Users/dustinober/.claude/get-shit-done/workflows/execute-plan.md
@/Users/dustinober/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-external-data-integration/02-RESEARCH.md
@analysis/config.py
@analysis/external_data.py
@analysis/stats_utils.py
@analysis/utils.py
@.planning/phases/02-external-data-integration/02-01-SUMMARY.md
@.planning/phases/02-external-data-integration/02-04-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Add detrending utilities to external_data.py</name>
  <files>analysis/external_data.py</files>
  <action>
Add detrending functions to `analysis/external_data.py`:

```python
from statsmodels.tsa.tsatools import detrend as sm_detrend
import numpy as np


def detrend_series(series: pd.Series, method: str = "linear") -> pd.Series:
    """
    Remove trend from a time series to avoid spurious correlations.

    Spurious correlation occurs when two series with long-term trends
    show high correlation even if no real relationship exists. Detrending
    removes the trend component before correlation analysis.

    Args:
        series: Time series (pandas Series with datetime or numeric index).
        method: Detrending method ('linear' or 'mean').
                - 'linear': Remove best-fit line trend (statsmodels.detrend)
                - 'mean': Subtract mean (simple centering)

    Returns:
        Detrended series with same index as input.

    Note:
        Crime and weather both show trends over 20 years. Correlating raw
        data will produce significant results even if unrelated. Always
        detrend before correlation analysis.

    Example:
        >>> crime_detrended = detrend_series(crime_series, 'linear')
        >>> temp_detrended = detrend_series(temp_series, 'linear')
        >>> correlation = crime_detrended.corr(temp_detrended)
    """
    if method == "linear":
        # Use statsmodels to remove linear trend
        detrended = sm_detrend(series.values, order=1)
        return pd.Series(detrended, index=series.index, name=series.name)

    elif method == "mean":
        # Simple mean centering
        return series - series.mean()

    else:
        raise ValueError(f"Unknown detrending method: {method}. Use 'linear' or 'mean'.")


def first_difference(series: pd.Series) -> pd.Series:
    """
    Apply first-differencing to remove trend (alternative to detrending).

    First-differencing computes the change between consecutive periods:
        diff[t] = series[t] - series[t-1]

    This removes any linear trend and makes the series stationary.

    Args:
        series: Time series (pandas Series).

    Returns:
        Differenced series (length = len(series) - 1).

    Example:
        >>> crime_diff = first_difference(crime_series)
        >>> temp_diff = first_difference(temp_series)
        >>> correlation = crime_diff.corr(temp_diff)
    """
    return series.diff().dropna()


def cross_correlation(
    series1: pd.Series,
    series2: pd.Series,
    max_lag: int = 7,
) -> pd.DataFrame:
    """
    Compute cross-correlation between two series at multiple lags.

    Tests whether series2 leads series1 (positive lags) or vice versa.

    Args:
        series1: First time series (e.g., crime count).
        series2: Second time series (e.g., temperature).
        max_lag: Maximum lag to test (in periods, same as series frequency).

    Returns:
        DataFrame with columns:
            - lag: Lag period (negative = series2 leads, positive = series1 leads)
            - correlation: Spearman correlation at each lag
            - p_value: Statistical significance of correlation

    Example:
        >>> # Test if temperature today predicts crime tomorrow
        >>> cc = cross_correlation(crime_series, temp_series, max_lag=7)
        >>> print(cc[cc['lag'] == 1])  # Temp today -> Crime tomorrow
    """
    from scipy.stats import spearmanr

    lags = range(-max_lag, max_lag + 1)
    results = []

    for lag in lags:
        if lag < 0:
            # series2 leads (shift series2 left)
            s1 = series1.iloc[abs(lag):]
            s2 = series2.iloc[:lag]
        elif lag > 0:
            # series1 leads (shift series1 left)
            s1 = series1.iloc[:-lag]
            s2 = series2.iloc[lag:]
        else:
            # No lag (contemporaneous)
            s1 = series1
            s2 = series2

        # Align and compute correlation
        min_len = min(len(s1), len(s2))
        s1 = s1.iloc[:min_len]
        s2 = s2.iloc[:min_len]

        if min_len > 10:  # Need sufficient data
            corr, p_value = spearmanr(s1, s2)
            results.append({
                'lag': lag,
                'correlation': corr,
                'p_value': p_value,
                'n': min_len,
            })

    return pd.DataFrame(results)
```
  </action>
  <verify>
```bash
python -c "
from analysis.external_data import detrend_series, first_difference, cross_correlation
import pandas as pd
import numpy as np

# Create test series with trend
t = np.arange(100)
series = pd.Series(t + np.random.randn(100) * 10)

# Test detrending
detrended = detrend_series(series, 'linear')
print(f'Original std: {series.std():.2f}')
print(f'Detrended std: {detrended.std():.2f}')
assert len(detrended) == len(series)

# Test first difference
diffed = first_difference(series)
print(f'First difference length: {len(diffed)}')
assert len(diffed) == len(series) - 1

print('Detrending utilities work correctly')
"
```
  </verify>
  <done>
detrend_series, first_difference, and cross_correlation functions exist in external_data.py. Functions correctly handle series with trends and produce expected output lengths.
  </done>
</task>

<task type="auto">
  <name>Create correlation_analysis.py module with weather correlation</name>
  <files>analysis/correlation_analysis.py</files>
  <action>
Create `analysis/correlation_analysis.py` with the following structure:

```python
"""
Correlation analysis for Philadelphia Crime EDA.

Analyzes correlations between crime patterns and external factors including
weather (temperature, precipitation) and economic indicators (unemployment,
poverty rates). All correlations include statistical testing with detrending
to avoid spurious correlations from long-term trend drift.
"""

from typing import Dict, Tuple, Optional
import pandas as pd
import numpy as np
from scipy import stats

# Import analysis utilities
from analysis.utils import load_data
from analysis.external_data import (
    fetch_weather_data,
    align_temporal_data,
    detrend_series,
    cross_correlation,
)
from analysis.stats_utils import correlation_test, apply_fdr_correction, bootstrap_ci
from analysis.reproducibility import set_global_seed, get_analysis_metadata
from analysis.config import STAT_CONFIG, TEMPORAL_CONFIG


def analyze_weather_crime_correlation(
    detrend: bool = True,
    include_lags: bool = True,
    max_lag: int = 7,
) -> Dict:
    """
    Analyze correlation between crime incidence and weather variables.

    Tests correlations between daily crime counts and weather variables
    (temperature, precipitation). Uses Spearman correlation for robustness
    to non-normal data and applies detrending to avoid spurious correlations.

    Args:
        detrend: If True, detrends both crime and weather series before correlation.
        include_lags: If True, computes lagged correlations (weather -> crime).
        max_lag: Maximum lag in days for cross-correlation analysis.

    Returns:
        Dictionary with:
            - correlations: DataFrame of variable, correlation, p_value, is_significant
            - lagged_correlations: DataFrame of lag-specific correlations
            - detrend_used: Whether detrending was applied
            - metadata: Analysis metadata

    Note:
        Weather variables tested:
            - tavg: Average daily temperature (C)
            - tmax: Maximum daily temperature (C)
            - tmin: Minimum daily temperature (C)
            - prcp: Daily precipitation (mm)

    Example:
        >>> results = analyze_weather_crime_correlation()
        >>> print(results['correlations'])
    """
    # Set seed for reproducibility
    set_global_seed(STAT_CONFIG["random_seed"])

    # Load data
    crime_df = load_data()
    weather_df = fetch_weather_data()

    # Align to daily resolution
    aligned = align_temporal_data(
        crime_df=crime_df,
        weather_df=weather_df,
        resolution="daily",
    )

    # Weather variables to test
    weather_vars = ['tavg', 'tmax', 'tmin', 'prcp']

    results = []
    for var in weather_vars:
        if var not in aligned.columns:
            continue

        crime = aligned['crime_count'].dropna()
        weather = aligned[var].dropna()

        # Align to common dates
        common_idx = crime.index.intersection(weather.index)
        crime_aligned = crime.loc[common_idx]
        weather_aligned = weather.loc[common_idx]

        # Detrend if requested
        if detrend:
            crime_aligned = detrend_series(crime_aligned, method='linear')
            weather_aligned = detrend_series(weather_aligned, method='linear')

        # Compute correlation
        corr_result = correlation_test(weather_aligned.values, crime_aligned.values, method='spearman')

        results.append({
            'variable': var,
            'correlation': corr_result['correlation'],
            'p_value': corr_result['p_value'],
            'test': corr_result['test_name'],
            'n': len(crime_aligned),
        })

    # Create DataFrame and apply FDR correction
    corr_df = pd.DataFrame(results)
    if not corr_df.empty:
        p_values = corr_df['p_value'].values
        corr_df['p_value_fdr'] = apply_fdr_correction(p_values, method=STAT_CONFIG['fdr_method'])
        corr_df['is_significant'] = corr_df['p_value_fdr'] < STAT_CONFIG['alpha']
        corr_df['is_significant_raw'] = corr_df['p_value'] < STAT_CONFIG['alpha']

    # Lagged correlations
    lagged_results = None
    if include_lags:
        crime_detrended = detrend_series(aligned['crime_count'].dropna()) if detrend else aligned['crime_count'].dropna()

        lagged_results = []
        for var in weather_vars:
            if var not in aligned.columns:
                continue
            weather_series = aligned[var].dropna()
            weather_detrended = detrend_series(weather_series) if detrend else weather_series

            cc = cross_correlation(crime_detrended, weather_detrended, max_lag=max_lag)
            cc['variable'] = var
            lagged_results.append(cc)

        if lagged_results:
            lagged_df = pd.concat(lagged_results)

            # FDR correction on lagged tests
            p_values = lagged_df['p_value'].values
            lagged_df['p_value_fdr'] = apply_fdr_correction(p_values, method=STAT_CONFIG['fdr_method'])
            lagged_df['is_significant'] = lagged_df['p_value_fdr'] < STAT_CONFIG['alpha']

            lagged_results = lagged_df

    return {
        'correlations': corr_df,
        'lagged_correlations': lagged_results,
        'detrend_used': detrend,
        'metadata': get_analysis_metadata(),
    }


def compute_lagged_correlation(
    crime_series: pd.Series,
    weather_series: pd.Series,
    lags: list = [1, 2, 3, 7, 14],
    detrend: bool = True,
) -> pd.DataFrame:
    """
    Compute lagged correlations between crime and a weather variable.

    Tests hypotheses like "hot weather today increases crime tomorrow."

    Args:
        crime_series: Daily crime count series.
        weather_series: Daily weather variable series.
        lags: List of lag periods (days) to test.
        detrend: If True, detrends both series before analysis.

    Returns:
        DataFrame with lag, correlation, p_value, is_significant columns.

    Example:
        >>> cc = compute_lagged_correlation(daily_crime, daily_temp, lags=[1, 7])
        >>> print(cc[cc['lag'] == 1])  # One-day lag effect
    """
    if detrend:
        crime_series = detrend_series(crime_series)
        weather_series = detrend_series(weather_series)

    results = []
    for lag in lags:
        # Shift weather forward (weather at t -> crime at t+lag)
        crime_lagged = crime_series.iloc[lag:]
        weather_lagged = weather_series.iloc[:-lag] if lag > 0 else weather_series

        # Align lengths
        min_len = min(len(crime_lagged), len(weather_lagged))
        crime_lagged = crime_lagged.iloc[:min_len]
        weather_lagged = weather_lagged.iloc[:min_len]

        # Compute correlation
        result = correlation_test(weather_lagged.values, crime_lagged.values, method='spearman')

        results.append({
            'lag': lag,
            'correlation': result['correlation'],
            'p_value': result['p_value'],
            'test': result['test_name'],
            'n': min_len,
        })

    df = pd.DataFrame(results)

    # Apply FDR correction
    if not df.empty:
        df['p_value_fdr'] = apply_fdr_correction(df['p_value'].values, method=STAT_CONFIG['fdr_method'])
        df['is_significant'] = df['p_value_fdr'] < STAT_CONFIG['alpha']

    return df
```
  </action>
  <verify>
```bash
python -c "
from analysis.correlation_analysis import analyze_weather_crime_correlation, compute_lagged_correlation
import pandas as pd

# Test function exists and imports work
print('Functions imported successfully')

# Test with small sample (quick test)
# This will fail if weather data not cached, which is expected
try:
    results = analyze_weather_crime_correlation()
    print('Correlation analysis completed')
    print(f'Variables tested: {len(results[\"correlations\"])}')
except Exception as e:
    if 'weather' in str(e).lower() or 'cache' in str(e).lower():
        print('Expected: Weather data not yet cached')
    else:
        print(f'Other error: {e}')
"
```
  </verify>
  <done>
correlation_analysis.py exists with analyze_weather_crime_correlation and compute_lagged_correlation functions. Functions import correctly and have proper docstrings with Args/Returns sections.
  </done>
</task>

</tasks>

<verification>
After completing all tasks:

1. Run the verify command from task 1 to confirm detrending utilities work
2. Run the verify command from task 2 to confirm correlation functions exist
3. Check that detrending is documented in function docstrings
4. Verify imports work: `python -c "from analysis.correlation_analysis import analyze_weather_crime_correlation; print('OK')"`
</verification>

<success_criteria>
1. detrend_series, first_difference, cross_correlation exist in external_data.py
2. correlation_analysis.py exists with analyze_weather_crime_correlation function
3. analyze_weather_crime_correlation includes detrending option
4. Function computes Spearman correlations with p-values
5. FDR correction is applied to multiple tests
6. Lagged correlations are supported (1-7 day lags)
7. Results dictionary includes correlations, lagged_correlations, detrend_used, metadata
</success_criteria>

<output>
After completion, create `.planning/phases/02-external-data-integration/02-05-SUMMARY.md` with:
- Detrending methods implemented (linear, mean, first-difference)
- Weather variables tested (tavg, tmax, tmin, prcp)
- Lag range supported (1-7 days)
- Statistical tests used (Spearman correlation, FDR correction)
</output>
