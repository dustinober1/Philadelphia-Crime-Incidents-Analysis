---
phase: 02-external-data-integration
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - .venv/pyvenv.cfg
  - analysis/external_data.py
  - analysis/config.py
  - .gitignore
  - .env.example
autonomous: true

must_haves:
  truths:
    - "User can import fetch_weather_data from analysis.external_data"
    - "User can fetch Philadelphia daily weather data for 2006-2026"
    - "User can cache weather data locally to avoid repeated API calls"
    - "Weather data includes temperature (avg, min, max) and precipitation"
  artifacts:
    - path: "analysis/external_data.py"
      provides: "Weather data fetching and caching utilities"
      min_lines: 100
      exports: ["fetch_weather_data", "load_cached_weather"]
    - path: "data/external/weather_philly_2006_2026.parquet"
      provides: "Cached Philadelphia weather dataset"
      optional: true
    - path: ".env.example"
      provides: "Environment variable template"
      contains: "API_KEY"
  key_links:
    - from: "analysis/correlation_analysis.py"
      to: "analysis/external_data.py"
      via: "from analysis.external_data import fetch_weather_data"
      pattern: "from analysis.external_data import"
    - from: "analysis/config.py"
      to: "data/external/"
      via: "EXTERNAL_DATA_DIR constant"
      pattern: "EXTERNAL_DATA_DIR"
---

<objective>
Create the weather data ingestion module using the Meteostat library to fetch Philadelphia daily weather observations (temperature, precipitation) spanning 2006-2026. Implement local caching to avoid repeated API calls and configure the external data directory structure.

Purpose: Provide reliable, cached access to historical weather data for correlation with crime patterns. Weather data is needed for CORR-01 (crime-weather correlation analysis).

Output: New `analysis/external_data.py` module with weather fetching functions, cached weather dataset in `data/external/`, updated `analysis/config.py` with external data paths, and `.env.example` template for API keys.
</objective>

<execution_context>
@/Users/dustinober/.claude/get-shit-done/workflows/execute-plan.md
@/Users/dustinober/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-external-data-integration/02-RESEARCH.md
@analysis/config.py
@analysis/utils.py
</context>

<tasks>

<task type="auto">
  <name>Install meteostat library for weather data</name>
  <files>.venv/pyvenv.cfg</files>
  <action>
Install the meteostat package for fetching historical weather data:

```bash
source .venv/bin/activate
pip install meteostat
```

Meteostat provides daily weather observations (temperature, precipitation, snow, wind) for global locations including Philadelphia. No API key required for basic usage.
  </action>
  <verify>
```bash
source .venv/bin/activate
python -c "
import meteostat
print(f'Meteostat version: {meteostat.__version__}')
from meteostat import Daily, Point
print('Meteostat imported successfully')
"
```
  </verify>
  <done>
meteostat is installed in .venv and imports without errors.
  </done>
</task>

<task type="auto">
  <name>Create external_data.py module with weather fetching functions</name>
  <files>analysis/external_data.py</files>
  <action>
Create `analysis/external_data.py` with the following structure:

```python
"""
External data fetching utilities for Philadelphia Crime EDA.

Provides functions to fetch and cache external data sources including
weather (Meteostat), economic indicators (FRED, Census), and policing data.
"""

from pathlib import Path
from datetime import datetime
import pandas as pd
from meteostat import Daily, Point

# Import configuration
from analysis.config import EXTERNAL_DATA_DIR, PHILADELPHIA_CENTER


def fetch_weather_data(
    start_date: str = "2006-01-01",
    end_date: str = "2026-01-31",
    cache_path: Path = None,
    force_refresh: bool = False,
) -> pd.DataFrame:
    """
    Fetch daily weather data for Philadelphia using Meteostat API.

    Retrieves temperature (tavg, tmin, tmax) and precipitation (prcp) data
    for the specified date range. Uses local cache to avoid repeated API calls.

    Args:
        start_date: Start date in YYYY-MM-DD format. Default is "2006-01-01".
        end_date: End date in YYYY-MM-DD format. Default is "2026-01-31".
        cache_path: Path to cache file. If None, uses EXTERNAL_DATA_DIR.
        force_refresh: If True, bypass cache and fetch from API.

    Returns:
        DataFrame with columns:
            - date: Date index
            - tavg: Average temperature (C)
            - tmin: Minimum temperature (C)
            - tmax: Maximum temperature (C)
            - prcp: Precipitation (mm)
            - snow: Snow depth (mm)
            - wdir: Wind direction (degrees)
            - wspd: Wind speed (km/h)
            - wpgt: Wind peak gust (km/h)
            - pres: Sea-level air pressure (hPa)
            - tsun: Sunshine duration (minutes)

    Raises:
        ValueError: If dates are invalid or API returns no data.

    Example:
        >>> df = fetch_weather_data("2020-01-01", "2020-12-31")
        >>> print(df[['tavg', 'prcp']].head())
    """
    if cache_path is None:
        cache_path = EXTERNAL_DATA_DIR / "weather_philly_2006_2026.parquet"

    # Check cache first
    if cache_path.exists() and not force_refresh:
        cached_data = pd.read_parquet(cache_path)
        # Ensure requested range is covered
        cached_data['date'] = pd.to_datetime(cached_data.index)
        cached_start = cached_data['date'].min()
        cached_end = cached_data['date'].max()
        if pd.to_datetime(start_date) >= cached_start and pd.to_datetime(end_date) <= cached_end:
            return cached_data.loc[start_date:end_date]

    # Philadelphia coordinates: lat, lon, elevation (meters)
    philly = Point(
        PHILADELPHIA_CENTER["lat"],
        PHILADELPHIA_CENTER["lon"],
        6  # Approximate elevation in meters
    )

    start = datetime.strptime(start_date, "%Y-%m-%d")
    end = datetime.strptime(end_date, "%Y-%m-%d")

    # Meteostat has a 10-year limit per request, so split into chunks
    weather_chunks = []
    current_start = start

    while current_start <= end:
        # End of this chunk (max 10 years from current_start)
        chunk_end = min(
            datetime(current_start.year + 10, 12, 31),
            end
        )

        data = Daily(philly, current_start, chunk_end)
        df = data.fetch()
        weather_chunks.append(df)

        # Move to next chunk
        current_start = datetime(chunk_end.year + 1, 1, 1)

    # Combine chunks
    if weather_chunks:
        result = pd.concat(weather_chunks)
    else:
        raise ValueError("No weather data returned from Meteostat API")

    # Ensure cache directory exists
    cache_path.parent.mkdir(parents=True, exist_ok=True)

    # Save to cache
    result.to_parquet(cache_path)

    return result.loc[start_date:end_date]


def load_cached_weather(
    cache_path: Path = None,
    start_date: str = None,
    end_date: str = None,
) -> pd.DataFrame:
    """
    Load cached weather data from local parquet file.

    Args:
        cache_path: Path to cache file. If None, uses default.
        start_date: Optional start date filter (YYYY-MM-DD).
        end_date: Optional end date filter (YYYY-MM-DD).

    Returns:
        DataFrame with weather data. Returns None if cache doesn't exist.

    Example:
        >>> df = load_cached_weather()
        >>> if df is not None:
        ...     print(df.describe())
    """
    if cache_path is None:
        cache_path = EXTERNAL_DATA_DIR / "weather_philly_2006_2026.parquet"

    if not cache_path.exists():
        return None

    df = pd.read_parquet(cache_path)

    if start_date:
        df = df.loc[start_date:]
    if end_date:
        df = df.loc[:end_date]

    return df
```
  </action>
  <verify>
```bash
python -c "
from analysis.external_data import fetch_weather_data, load_cached_weather
import pandas as pd

# Test fetch with small date range
df = fetch_weather_data('2020-01-01', '2020-01-31')
print(f'Fetched {len(df)} days of weather data')
print(f'Columns: {list(df.columns)}')
print('Sample data:')
print(df[['tavg', 'tmin', 'tmax', 'prcp']].head())

# Test cache load
df2 = load_cached_weather()
if df2 is not None:
    print(f'Cache has {len(df2)} total days')
else:
    print('No cache yet')
"
```
  </verify>
  <done>
fetch_weather_data and load_cached_weather functions exist, fetch returns DataFrame with tavg/tmin/tmax/prcp columns, and 31 days of test data are successfully retrieved.
  </done>
</task>

<task type="auto">
  <name>Add external data paths to config.py and create .env.example</name>
  <files>analysis/config.py</files>
  <action>
Add external data configuration to `analysis/config.py`:

```python
# After DATA_DIR section, add:

# External data paths
EXTERNAL_DATA_DIR = DATA_DIR / "external"
EXTERNAL_CACHE_DIR = EXTERNAL_DATA_DIR / ".cache"
```

Create `.env.example` file at project root:

```bash
# External Data API Keys
# Copy this file to .env and add your actual API keys

# FRED API Key for economic data
# Get free key at: https://fred.stlouisfed.org/docs/api/api_key.html
FRED_API_KEY=your_fred_api_key_here

# U.S. Census API Key for economic data
# Get free key at: https://api.census.gov/data/key_signup.html
CENSUS_API_KEY=your_census_api_key_here
```

Update `.gitignore` to add:
```
.env
data/external/.cache/
data/external/*.parquet
```
  </action>
  <verify>
```bash
python -c "
from analysis.config import EXTERNAL_DATA_DIR, EXTERNAL_CACHE_DIR
print(f'External data dir: {EXTERNAL_DATA_DIR}')
print(f'Cache dir: {EXTERNAL_CACHE_DIR}')

# Test .env.example exists
from pathlib import Path
env_example = Path('.env.example')
assert env_example.exists(), '.env.example not found'
print('.env.example exists')

# Check gitignore has entries
gitignore = Path('.gitignore').read_text()
assert '.env' in gitignore, '.env not in gitignore'
assert 'data/external/.cache/' in gitignore, 'cache not in gitignore'
print('gitignore updated correctly')
"
```
  </verify>
  <done>
EXTERNAL_DATA_DIR and EXTERNAL_CACHE_DIR constants exist in config.py, .env.example file exists at project root with FRED_API_KEY and CENSUS_API_KEY placeholders, and .gitignore excludes .env and data/external cache.
  </done>
</task>

</tasks>

<verification>
After completing all tasks:

1. Run the verify command from task 2 to confirm weather fetching works
2. Check that meteostat is installed: `python -c "import meteostat; print(meteostat.__version__)"`
3. Verify config constants: `python -c "from analysis.config import EXTERNAL_DATA_DIR; print(EXTERNAL_DATA_DIR)"`
4. Confirm .env.example exists and contains API key placeholders
5. Verify .gitignore excludes .env and data/external cache
6. Check that data/external/weather_philly_2006_2026.parquet was created
</verification>

<success_criteria>
1. meteostat is installed and importable
2. analysis/external_data.py exists with fetch_weather_data and load_cached_weather functions
3. fetch_weather_data returns DataFrame with tavg, tmin, tmax, prcp columns
4. Weather data is cached to data/external/weather_philly_2006_2026.parquet
5. config.py has EXTERNAL_DATA_DIR and EXTERNAL_CACHE_DIR constants
6. .env.example exists with API key templates
7. .gitignore excludes .env and data/external cache
</success_criteria>

<output>
After completion, create `.planning/phases/02-external-data-integration/02-01-SUMMARY.md` with:
- meteostat version installed
- Date range of weather data cached
- Columns available in weather dataset
- Any API issues or workarounds
</output>
