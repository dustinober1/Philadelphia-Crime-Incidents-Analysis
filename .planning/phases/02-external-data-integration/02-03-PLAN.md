---
phase: 02-external-data-integration
plan: 03
type: execute
wave: 2
depends_on: [02-01]
files_modified:
  - analysis/external_data.py
  - analysis/config.py
autonomous: true

must_haves:
  truths:
    - "User can import requests_cache enabled session from analysis.external_data"
    - "API responses are cached to SQLite backend with per-URL staleness"
    - "Weather data uses 7-day staleness (historical doesn't change)"
    - "FRED data uses 30-day staleness (monthly updates)"
    - "Census data uses 365-day staleness (retroactive doesn't change)"
  artifacts:
    - path: "analysis/external_data.py"
      provides: "Cached API session management"
      contains: "get_cached_session", "clear_cache"
    - path: "data/external/.cache/"
      provides: "SQLite cache database for API responses"
      optional: true
    - path: "analysis/config.py"
      contains: "CACHE_CONFIG"
  key_links:
    - from: "analysis/external_data.py"
      to: "data/external/.cache/"
      via: "requests_cache.CachedSession"
      pattern: "requests_cache"
---

<objective>
Implement the caching infrastructure layer using requests-cache to avoid API rate limits. Configure per-service staleness policies (weather: 7 days, FRED: 30 days, Census: 365 days) and provide cache management utilities.

Purpose: Prevent API rate limiting issues during development and ensure reproducible data access without repeated network calls. Caching is critical for CORR-01, CORR-02 success criteria (all external sources cached locally).

Output: Extended `analysis/external_data.py` with cached session management, CACHE_CONFIG in `analysis/config.py`, and SQLite cache database at `data/external/.cache/`.
</objective>

<execution_context>
@/Users/dustinober/.claude/get-shit-done/workflows/execute-plan.md
@/Users/dustinober/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-external-data-integration/02-RESEARCH.md
@analysis/config.py
@analysis/external_data.py
@.planning/phases/02-external-data-integration/02-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Add CACHE_CONFIG to config.py</name>
  <files>analysis/config.py</files>
  <action>
Add caching configuration to `analysis/config.py` after EXTERNAL_CACHE_DIR:

```python
# =============================================================================
# EXTERNAL DATA CACHING CONFIGURATION
# =============================================================================

# Cache staleness settings for different data sources
# Longer staleness = fewer API calls but potentially stale data
CACHE_CONFIG = {
    # Weather data: 7 days (historical data doesn't change often)
    "weather_staleness": 7,  # days

    # FRED economic data: 30 days (monthly updates, safe cache window)
    "fred_staleness": 30,  # days

    # Census ACS data: 365 days (retroactive data never changes)
    "census_staleness": 365,  # days

    # Cache backend type ('sqlite', 'memory', 'redis')
    "cache_backend": "sqlite",

    # Whether to use cache (set to False to force fresh API calls)
    "cache_enabled": True,
}
```

Add a helper function to get staleness as timedelta:

```python
from datetime import timedelta


def get_cache_staleness(source: str) -> timedelta:
    """
    Get cache staleness duration for a data source.

    Args:
        source: Data source name ('weather', 'fred', 'census').

    Returns:
        timedelta object for the staleness period.

    Raises:
        ValueError: If source name is not recognized.

    Example:
        >>> from analysis.config import get_cache_staleness
        >>> delta = get_cache_staleness('weather')
        >>> print(delta.days)
        7
    """
    key = f"{source.lower()}_staleness"
    if key not in CACHE_CONFIG:
        raise ValueError(f"Unknown cache source: {source}. Use 'weather', 'fred', or 'census'.")
    return timedelta(days=CACHE_CONFIG[key])
```
  </action>
  <verify>
```bash
python -c "
from analysis.config import CACHE_CONFIG, get_cache_staleness
from datetime import timedelta

print('CACHE_CONFIG keys:', list(CACHE_CONFIG.keys()))
print('Weather staleness:', CACHE_CONFIG['weather_staleness'], 'days')
print('FRED staleness:', CACHE_CONFIG['fred_staleness'], 'days')
print('Census staleness:', CACHE_CONFIG['census_staleness'], 'days')

# Test helper function
delta = get_cache_staleness('weather')
assert isinstance(delta, timedelta)
assert delta.days == 7
print('get_cache_staleness works correctly')
"
```
  </verify>
  <done>
CACHE_CONFIG exists in config.py with weather_staleness, fred_staleness, census_staleness, cache_backend, and cache_enabled keys. get_cache_staleness helper function exists and returns timedelta objects.
  </done>
</task>

<task type="auto">
  <name>Add cached session management to external_data.py</name>
  <files>analysis/external_data.py</files>
  <action>
Add caching infrastructure to `analysis/external_data.py`:

First, add imports:
```python
from datetime import timedelta
import requests_cache
from analysis.config import CACHE_CONFIG, get_cache_staleness, EXTERNAL_CACHE_DIR
```

Then add these functions after the imports (before fetch_weather_data):

```python
def _ensure_cache_dir() -> Path:
    """Ensure cache directory exists."""
    EXTERNAL_CACHE_DIR.mkdir(parents=True, exist_ok=True)
    return EXTERNAL_CACHE_DIR


def get_cached_session(source: str = "weather"):
    """
    Get a requests session with caching enabled.

    Uses requests-cache to cache API responses locally with per-source
    staleness settings. Avoids rate limits and speeds up repeated requests.

    Args:
        source: Data source name for staleness settings ('weather', 'fred', 'census').

    Returns:
        requests_cache.CachedSession configured for the data source.

    Example:
        >>> session = get_cached_session('weather')
        >>> response = session.get('https://example.com/api')
    """
    if not CACHE_CONFIG.get("cache_enabled", True):
        # Return uncached session if caching disabled
        import requests
        return requests.Session()

    _ensure_cache_dir()

    cache_path = EXTERNAL_CACHE_DIR / f"{source}_cache"
    staleness = get_cache_staleness(source)

    # Create cached session with SQLite backend
    session = requests_cache.CachedSession(
        cache_name=str(cache_path),
        expire_after=staleness,
        backend= CACHE_CONFIG.get("cache_backend", "sqlite"),
    )

    return session


def clear_cache(source: str = None) -> None:
    """
    Clear cached API responses.

    Args:
        source: Specific source to clear ('weather', 'fred', 'census').
                If None, clears all caches.

    Example:
        >>> clear_cache('weather')  # Clear only weather cache
        >>> clear_cache()  # Clear all caches
    """
    _ensure_cache_dir()

    if source:
        # Clear specific source cache
        cache_pattern = EXTERNAL_CACHE_DIR / f"{source}_cache.*"
        for cache_file in EXTERNAL_CACHE_DIR.glob(f"{source}_cache.*"):
            cache_file.unlink()
    else:
        # Clear all caches
        for cache_file in EXTERNAL_CACHE_DIR.glob("*_cache.*"):
            cache_file.unlink()


def get_cache_info() -> dict:
    """
    Get information about cached data.

    Returns:
        Dict with cache file sizes and counts for each data source.

    Example:
        >>> info = get_cache_info()
        >>> print(info['weather']['size_mb'])
    """
    _ensure_cache_dir()

    sources = ['weather', 'fred', 'census']
    info = {}

    for source in sources:
        cache_files = list(EXTERNAL_CACHE_DIR.glob(f"{source}_cache.*"))

        if cache_files:
            total_size = sum(f.stat().st_size for f in cache_files)
            info[source] = {
                'exists': True,
                'file_count': len(cache_files),
                'size_bytes': total_size,
                'size_mb': round(total_size / (1024 * 1024), 2),
                'files': [str(f.name) for f in cache_files],
            }
        else:
            info[source] = {'exists': False}

    # Also check parquet cache files
    parquet_files = list((EXTERNAL_DATA_DIR / "..").glob("*.parquet"))
    info['parquet'] = {
        'count': len(parquet_files),
        'files': [str(f.name) for f in parquet_files],
    }

    return info
```

Update fetch_weather_data to use cached session (modify the API call section):

```python
# In fetch_weather_data, replace the data.fetch() calls with:
session = get_cached_session('weather')
# Meteostat uses its own requests internally, so we'll cache at parquet level
# The requests_cache is for FRED/Census APIs specifically
```

Note: Meteostat library handles its own requests, so the parquet caching in fetch_weather_data is sufficient. The requests_cache is primarily for FRED and Census APIs.
  </action>
  <verify>
```bash
python -c "
from analysis.external_data import get_cached_session, clear_cache, get_cache_info
import tempfile

# Test get_cached_session
session = get_cached_session('weather')
print(f'Session type: {type(session).__name__}')

# Test get_cache_info
info = get_cache_info()
print(f'Cache info sources: {list(info.keys())}')
print(f'Weather cache exists: {info.get(\"weather\", {}).get(\"exists\", False)}')

print('Cache management functions work correctly')
"
```
  </verify>
  <done>
get_cached_session, clear_cache, and get_cache_info functions exist in external_data.py. get_cached_session returns a CachedSession, and get_cache_info returns dict with cache status for each source.
  </done>
</task>

</tasks>

<verification>
After completing all tasks:

1. Run the verify command from task 1 to confirm CACHE_CONFIG works
2. Run the verify command from task 2 to confirm cache functions work
3. Verify cache directory is created: `ls -la data/external/.cache/`
4. Test cache info: `python -c "from analysis.external_data import get_cache_info; import pprint; pprint.pprint(get_cache_info())"`
</verification>

<success_criteria>
1. CACHE_CONFIG exists in config.py with 5 keys (weather_staleness, fred_staleness, census_staleness, cache_backend, cache_enabled)
2. get_cache_staleness helper function exists and returns timedelta
3. get_cached_session function exists and returns CachedSession
4. clear_cache and get_cache_info functions exist
5. data/external/.cache/ directory is created when functions are called
</success_criteria>

<output>
After completion, create `.planning/phases/02-external-data-integration/02-03-SUMMARY.md` with:
- CACHE_CONFIG values (staleness in days for each source)
- Cache backend configured (SQLite)
- Cache directory location
</output>
