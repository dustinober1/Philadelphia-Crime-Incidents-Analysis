---
phase: 12-api-&-cli-testing
plan: 07
type: execute
wave: 2
depends_on: ["12-01", "12-02", "12-03", "12-04"]
files_modified:
  - tests/test_api_endpoints.py
autonomous: true

must_haves:
  truths:
    - "API validation errors return 422 status with correct error structure"
    - "HTTP exceptions return appropriate status codes (401, 404, 429)"
    - "Unhandled server errors return 500 with generic error message"
    - "CORS middleware allows requests from allowed origins"
    - "Request logging middleware logs requests and adds X-Request-ID header"
  artifacts:
    - path: "tests/test_api_endpoints.py"
      provides: "Comprehensive error handling tests for API"
      min_lines: 250
  key_links:
    - from: "tests/test_api_endpoints.py"
      to: "api/main.py"
      via: "TestClient error response validation"
      pattern: "assert response\.status_code == [45]"
---

<objective>
Write comprehensive error handling tests for the FastAPI application including validation errors, HTTP exceptions, and middleware behavior.

Purpose: Ensure API error handling is thoroughly tested including 422 validation errors, 401/404 HTTP exceptions, 500 server errors, rate limiting, CORS behavior, and request logging.

Output: Extended test_api_endpoints.py with comprehensive error handling tests.
</objective>

<execution_context>
@/Users/dustinober/.claude/get-shit-done/workflows/execute-plan.md
@/Users/dustinober/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/phases/12-api-&-cli-testing/12-RESEARCH.md
@tests/test_api_endpoints.py
@api/main.py
@api/routers/questions.py
@api/routers/metadata.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add validation error (422) tests</name>
  <files>tests/test_api_endpoints.py</files>
  <action>
Add validation error tests to test_api_endpoints.py:

1. **test_validation_error_invalid_query_param_type()**
   - Call /api/v1/trends/annual?category=InvalidCategory
   - Verify 422 status code
   - Verify error response has "detail" key
   - Verify error message indicates validation failure

2. **test_validation_error_invalid_year_format()**
   - Call /api/v1/trends/monthly?start_year=abc
   - Verify 422 status code
   - Verify error indicates integer validation failed

3. **test_validation_error_missing_required_body()**
   - POST to /api/v1/questions without body
   - Verify 422 status code
   - Verify error lists missing required fields

4. **test_questions_status_validation()**
   - Call /api/v1/questions?status=invalid
   - Verify 422 status code
   - Verify error message "status must be answered or pending"

Extend existing test_error_payload_shape_for_http_errors with more cases.
  </action>
  <verify>Run pytest tests/test_api_endpoints.py -v -k "validation" and verify all tests pass</verify>
  <done>Validation error handling is tested for query params, body validation, and enum values</done>
</task>

<task type="auto">
  <name>Task 2: Add HTTP exception (401, 404, 429) tests</name>
  <files>tests/test_api_endpoints.py</files>
  <action>
Add HTTP exception tests to test_api_endpoints.py:

1. **test_401_unauthorized_missing_token()**
   - Call /api/v1/questions?status=pending without auth header
   - Verify 401 status code
   - Verify "Missing admin token" in error message

2. **test_401_invalid_admin_token()**
   - Call /api/v1/questions?status=pending with invalid Bearer token
   - Verify 401 status code
   - Verify "Invalid admin token" in error message

3. **test_401_invalid_admin_password()**
   - POST to /api/v1/questions/admin/session with wrong password
   - Verify 401 status code
   - Verify "Invalid credentials" in error message

4. **test_404_question_not_found()**
   - PATCH to /api/v1/questions/{nonexistent_id} with valid auth
   - Verify 404 status code
   - Verify "Question not found" in error message

5. **test_429_rate_limit_exceeded()**
   - Submit more than 5 questions from same IP within hour
   - Use loop or multiple requests
   - Verify 429 status code
   - Verify "Rate limit exceeded" in error message
   - Reset _RATE_LIMIT after test

Use _reset_questions_state() helper for cleanup.
  </action>
  <verify>Run pytest tests/test_api_endpoints.py -v -k "http_error or 401 or 404 or 429" and verify all tests pass</verify>
  <done>HTTP exception handling is tested for 401, 404, and 429 status codes</done>
</task>

<task type="auto">
  <name>Task 3: Add server error (500) and exception handler tests</name>
  <files>tests/test_api_endpoints.py</files>
  <action>
Add server error and exception handler tests:

1. **test_500_internal_server_error_on_missing_data()**
   - Use monkeypatch to clear _DATA_CACHE
   - Call any endpoint that requires data (e.g., /api/v1/trends/annual)
   - Verify 500 status code
   - Verify "internal_error" in response
   - Verify "An unexpected server error occurred" in message

2. **test_500_error_payload_structure()**
   - Trigger a 500 error (via missing data)
   - Verify error response has "error": "internal_error"
   - Verify error response has "message" key
   - Verify error structure matches main.py exception handler

3. **test_http_exception_handler_structure()**
   - Trigger a 422 error
   - Verify error response has "error": "http_error"
   - Verify error response has "message" key

4. **test_validation_exception_handler_structure()**
   - Trigger a validation error
   - Verify error response has "error": "validation_error"
   - Verify error response has "details" key with validation errors

Pattern: Use monkeypatch.setattr("api.services.data_loader._DATA_CACHE", {}).
  </action>
  <verify>Run pytest tests/test_api_endpoints.py -v -k "500 or exception_handler" and verify all tests pass</verify>
  <done>Server error handling and exception handler structure are tested</done>
</task>

<task type="auto">
  <name>Task 4: Add middleware tests (CORS, request logging)</name>
  <files>tests/test_api_endpoints.py</files>
  <action>
Add middleware behavior tests:

1. **test_cors_headers_present()**
   - Make request with Origin header
   - Verify response has CORS headers (Access-Control-Allow-Origin, etc.)
   - Verify allowed origins include localhost:3000

2. **test_cors_preflight_request()**
   - Make OPTIONS request with Origin and Access-Control-Request-Method headers
   - Verify 204 or 200 status
   - Verify CORS headers are present

3. **test_request_id_header_added()**
   - Make any request
   - Verify "X-Request-ID" header is present
   - Verify header is a 12-character hex string

4. **test_request_id_format()**
   - Make multiple requests
   - Verify each request has unique X-Request-ID
   - Verify format is consistent (12 hex characters)

5. **test_health_endpoint_includes_contract_status()**
   - Call /api/health
   - Verify response has "ok" key
   - Verify response has "loaded_keys" list
   - Verify response has "data_dir" and "missing_exports"

Add these tests to test_api_endpoints.py.
Note: CORS behavior depends on TestClient implementation, may need to use real HTTP client for full testing.
  </action>
  <verify>Run pytest tests/test_api_endpoints.py -v -k "cors or middleware or request_id" and verify all tests pass</verify>
  <done>CORS and request logging middleware behavior is tested</done>
</task>

<task type="auto">
  <name>Task 5: Add questions router edge case tests</name>
  <files>tests/test_api_endpoints.py</files>
  <action>
Add edge case tests for questions router:

1. **test_question_text_too_long()**
   - Submit question with >1000 characters
   - Verify 422 status
   - Verify "question_text must be 1-1000 characters" in error

2. **test_question_text_contains_url()**
   - Submit question with URL (e.g., "Check http://example.com")
   - Verify 422 status
   - Verify "URLs are not allowed" in error

3. **test_question_text_all_caps_spam()**
   - Submit question with >20 char all-caps text
   - Verify 422 status
   - Verify "Likely spam content" in error

4. **test_honeypot_field_success()**
   - Submit question with honeypot field filled
   - Verify 200 status
   - Verify question is NOT actually stored (returns {"ok": True} without id)

5. **test_delete_question_removes_from_storage()**
   - Create question, get ID
   - DELETE with admin auth
   - Verify question no longer in _IN_MEMORY or Firestore mock

Use _reset_questions_state() for cleanup.
  </action>
  <verify>Run pytest tests/test_api_endpoints.py -v -k "question" and verify edge case tests pass</verify>
  <done>Questions router edge cases are tested (spam detection, honeypot, validation)</done>
</task>

</tasks>

<verification>
After completion, verify:
1. Run pytest tests/test_api_endpoints.py -v -k "error" and count passing tests
2. All error status codes are tested (400, 401, 404, 422, 429, 500)
3. Exception handler structure is validated for all error types
4. Middleware behavior is tested (CORS, request logging, X-Request-ID)
5. At least 15 new error-focused tests pass
</verification>

<success_criteria>
1. API error handling is comprehensively tested
2. All exception handlers in api/main.py have tests
3. Validation errors return 422 with correct structure
4. HTTP exceptions (401, 404, 429) return correct status codes
5. Server errors return 500 with generic message (no sensitive data leaked)
6. Middleware behavior is validated (CORS headers, request IDs)
7. Questions router edge cases are covered
</success_criteria>

<output>
After completion, create .planning/phases/12-api-&-cli-testing/12-07-SUMMARY.md with:
- Number of error handling tests added
- Coverage percentage for api/main.py
- Any security issues discovered (e.g., information leakage in errors)
</output>
