---
phase: 12-api-&-cli-testing
plan: 04
type: execute
wave: 1
depends_on: []
files_modified:
  - tests/test_api_endpoints.py
autonomous: true

must_haves:
  truths:
    - "Both forecasting API endpoints return 200 status code"
    - "Forecasting endpoints return correctly structured JSON data"
    - "Time series forecast has expected fields (dates, predictions, confidence intervals)"
    - "Classification features have expected structure"
    - "Error handling for missing forecast data is tested"
  artifacts:
    - path: "tests/test_api_endpoints.py"
      provides: "Test suite for forecasting API endpoints"
      min_lines: 100
  key_links:
    - from: "tests/test_api_endpoints.py"
      to: "api/routers/forecasting.py"
      via: "TestClient"
      pattern: "client\.get\('/api/v1/forecasting"
---

<objective>
Write integration tests for both forecasting endpoints using FastAPI TestClient.

Purpose: Ensure both forecasting endpoints (/time-series, /classification) are properly tested with request/response contract validation.

Output: Extended test_api_endpoints.py with comprehensive tests for forecasting endpoints.
</objective>

<execution_context>
@/Users/dustinober/.claude/get-shit-done/workflows/execute-plan.md
@/Users/dustinober/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/phases/12-api-&-cli-testing/12-RESEARCH.md
@tests/test_api_endpoints.py
@api/routers/forecasting.py
@api/services/data_loader.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add tests for both forecasting endpoints</name>
  <files>tests/test_api_endpoints.py</files>
  <action>
Add integration tests for the 2 forecasting endpoints in test_api_endpoints.py:

1. **test_forecasting_time_series()** - Test GET /api/v1/forecasting/time-series
   - Verify response status is 200
   - Verify response is a dict (not list)
   - Verify expected keys exist (forecast, model_metadata, confidence_intervals)
   - Verify forecast data has date/value structure

2. **test_forecasting_classification()** - Test GET /api/v1/forecasting/classification
   - Verify response status is 200
   - Verify response is a list
   - Verify each row has classification features
   - Verify feature importance or probability fields exist

Use the existing TestClient pattern. Load real data using setup_module().
Focus on data structure validation, not prediction accuracy.
  </action>
  <verify>Run pytest tests/test_api_endpoints.py -v -k "forecasting" and verify all new tests pass</verify>
  <done>Both forecasting endpoints have passing tests with structure validation</done>
</task>

<task type="auto">
  <name>Task 2: Add forecasting endpoint data validation tests</name>
  <files>tests/test_api_endpoints.py</files>
  <action>
Add detailed data validation tests for forecasting endpoints:

1. **test_forecasting_time_series_structure()**
   - Verify forecast has future dates beyond training data
   - Verify confidence intervals are present (upper/lower bounds)
   - Verify model metadata includes model type, training period
   - Verify prediction values are numeric

2. **test_forecasting_classification_features()**
   - Verify classification features include UCR codes
   - Verify feature importance scores are present
   - Verify probability distributions for classes exist
   - Verify temporal coverage of predictions

Add these tests to test_api_endpoints.py.
Use isinstance checks and key validation for structure tests.
  </action>
  <verify>Run pytest tests/test_api_endpoints.py -v -k "forecasting" and verify data validation tests pass</verify>
  <done>Forecasting data is validated for structure, completeness, and expected fields</done>
</task>

<task type="auto">
  <name>Task 3: Add forecasting endpoint error handling tests</name>
  <files>tests/test_api_endpoints.py</files>
  <action>
Add error handling tests for forecasting endpoints:

1. **test_forecasting_missing_data()**
   - Use monkeypatch to remove forecast data from _DATA_CACHE
   - Call /api/v1/forecasting/time-series
   - Verify 500 status is returned
   - Verify error response structure

2. **test_forecasting_malformed_data()**
   - Use monkeypatch to set forecast data to malformed dict (missing required keys)
   - Verify endpoint still returns 200 (data loader returns cached data as-is)
   - This tests that endpoint passes through cached data

Pattern: Use monkeypatch.setattr("api.services.data_loader._DATA_CACHE", mock_cache).
  </action>
  <verify>Run pytest tests/test_api_endpoints.py -v -k "forecasting" and verify error handling tests pass</verify>
  <done>Error paths for forecasting endpoints are tested</done>
</task>

</tasks>

<verification>
After completion, verify:
1. Run pytest tests/test_api_endpoints.py -v -k "forecasting" and count passing tests
2. Both forecasting endpoints (/time-series, /classification) have tests
3. Data validation includes forecast structure, confidence intervals, and features
4. At least 4 new forecasting-focused tests pass
5. Coverage includes happy path, data validation, and error handling
</verification>

<success_criteria>
1. Both forecasting API endpoints have integration tests
2. Tests validate response structure and expected forecast fields
3. Forecast data structure is validated (dates, predictions, confidence intervals)
4. Error handling is tested for missing forecast data
5. All tests follow existing TestClient patterns
</success_criteria>

<output>
After completion, create .planning/phases/12-api-&-cli-testing/12-04-SUMMARY.md with:
- Number of tests added for forecasting endpoints
- Coverage percentage for api/routers/forecasting.py
- Forecast data structure validation results
</output>
