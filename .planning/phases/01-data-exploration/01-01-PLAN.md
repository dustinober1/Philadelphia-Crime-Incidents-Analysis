---
phase: 01-data-exploration
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - requirements.txt
  - src/utils/config.py
  - src/data/loader.py
  - src/data/__init__.py
autonomous: true
must_haves:
  truths:
    - "Project dependencies include pyarrow"
    - "Config loader correctly resolves absolute paths"
    - "Data loader returns a pandas DataFrame backed by pyarrow"
  artifacts:
    - path: "src/data/loader.py"
      provides: "load_crime_data function"
      exports: ["load_crime_data"]
    - path: "src/utils/config.py"
      provides: "Config access"
      exports: ["load_config", "PROJECT_ROOT"]
  key_links:
    - from: "src/data/loader.py"
      to: "data/processed/crime_incidents_combined.parquet"
      via: "pd.read_parquet with pyarrow engine"
---

<objective>
Establish the data loading infrastructure and dependency management.

Purpose: Enable efficient loading of the large parquet dataset using PyArrow backend as recommended in research.
Output: Utilities to read config and load data.
</objective>

<execution_context>
@~/.config/opencode/get-shit-done/workflows/execute-plan.md
@~/.config/opencode/get-shit-done/templates/summary.md
</execution_context>

<context>
@config.ini
@requirements.txt
</context>

<tasks>

<task type="auto">
  <name>Task 1: Update Dependencies</name>
  <files>requirements.txt</files>
  <action>
    Add `pyarrow` to requirements.txt to support fast Parquet I/O and nullable types.
    Ensure pandas is present (it is, but keep it).
  </action>
  <verify>grep "pyarrow" requirements.txt</verify>
  <done>pyarrow is listed in requirements.txt</done>
</task>

<task type="auto">
  <name>Task 2: Create Configuration Utility</name>
  <files>src/utils/config.py</files>
  <action>
    Create a utility to load `config.ini` safely.
    - Define `PROJECT_ROOT` using `Path(__file__)` parents.
    - Implement `load_config()` that returns a ConfigParser object.
    - Ensure paths are resolved relative to PROJECT_ROOT, not CWD.
  </action>
  <verify>python -c "from src.utils.config import load_config, PROJECT_ROOT; print(PROJECT_ROOT); print(load_config().sections())"</verify>
  <done>Script prints valid root path and config sections</done>
</task>

<task type="auto">
  <name>Task 3: Create Data Loader</name>
  <files>src/data/loader.py, src/data/__init__.py</files>
  <action>
    Create a data loading module.
    - Import `load_config` and `PROJECT_ROOT`.
    - Implement `load_crime_data(filename="crime_incidents_combined.parquet") -> pd.DataFrame`.
    - Construct full path using `config['data_paths']['processed_data_path']`.
    - Use `pd.read_parquet(..., engine='pyarrow', dtype_backend='pyarrow')` for memory efficiency.
    - Add basic error handling (FileNotFound).
  </action>
  <verify>python -c "from src.data.loader import load_crime_data; df = load_crime_data(); print(df.shape); print(df.dtypes)"</verify>
  <done>Prints dataframe shape and pyarrow-backed dtypes</done>
</task>

</tasks>

<verification>
Run the inline python verification commands to ensure loading works.
</verification>

<success_criteria>
- requirements.txt contains pyarrow
- src/utils/config.py correctly finds config.ini
- src/data/loader.py loads the parquet file without error
- Dataframe dtypes show usage of pyarrow backend (e.g., string[pyarrow])
</success_criteria>

<output>
After completion, create `.planning/phases/01-data-exploration/01-01-SUMMARY.md`
</output>
