---
phase: 01-statistical-rigor
plan: 04
type: execute
wave: 3
depends_on: [01-02, 01-03]
files_modified:
  - analysis/stats_utils.py
  - analysis/config.py
autonomous: true

must_haves:
  truths:
    - "User can view Cliff's delta effect sizes for non-parametric comparisons"
    - "User can view odds ratios for proportion comparisons"
    - "User can view FDR-adjusted p-values alongside raw p-values in all reports"
    - "User can view standardized coefficients for correlation analyses"
  artifacts:
    - path: "analysis/stats_utils.py"
      contains: "cliffs_delta", "odds_ratio", "standardized_coefficient"
      exports: ["cliffs_delta", "odds_ratio", "standardized_coefficient"]
    - path: "analysis/config.py"
      contains: "CLIFFS_DELTA_THRESHOLDS"
  key_links:
    - from: "analysis/temporal_analysis.py"
      to: "analysis/stats_utils.py"
      via: "from analysis.stats_utils import cliffs_delta"
    - from: "analysis/categorical_analysis.py"
      to: "analysis/stats_utils.py"
      via: "from analysis.stats_utils import odds_ratio"
    - from: "analysis/cross_analysis.py"
      to: "analysis/stats_utils.py"
      via: "from analysis.stats_utils import standardized_coefficient"
---

<objective>
Add effect size calculations (Cliff's delta for non-parametric, odds ratios for proportions, standardized coefficients) to stats_utils.py and update config.py with effect size benchmarks. Ensure all modules report both raw and FDR-adjusted p-values, and include appropriate effect sizes for all statistical comparisons.

Purpose: Meet STAT-03 requirement for effect sizes (Cohen's d, odds ratios, standardized coefficients) and ensure STAT-04 (FDR correction) is consistently applied across all omnibus comparisons.

Output: Enhanced stats_utils.py with additional effect size functions, updated config.py with Cliff's delta thresholds, and updated analysis modules using appropriate effect sizes.
</objective>

<execution_context>
@/Users/dustinober/.claude/get-shit-done/workflows/execute-plan.md
@/Users/dustinober/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/phases/01-statistical-rigor/01-01-SUMMARY.md
@.planning/phases/01-statistical-rigor/01-02-SUMMARY.md
@.planning/phases/01-statistical-rigor/01-03-SUMMARY.md
@analysis/stats_utils.py
@analysis/config.py
@analysis/temporal_analysis.py
@analysis/categorical_analysis.py
@analysis/cross_analysis.py
</context>

<tasks>

<task type="auto">
  <name>Add Cliff's delta function to stats_utils.py</name>
  <files>analysis/stats_utils.py</files>
  <action>
Add **cliffs_delta(x, y)** function to `analysis/stats_utils.py`:

```python
def cliffs_delta(x: np.ndarray, y: np.ndarray) -> Tuple[float, str]:
    """
    Calculate Cliff's Delta non-parametric effect size.

    Cliff's Delta measures the ordinal dominance between two samples.
    More robust than Cohen's d for non-normal data.

    Interpretation (Romano et al., 2006):
    - negligible: |d| < 0.147
    - small: 0.147 <= |d| < 0.33
    - medium: 0.33 <= |d| < 0.474
    - large: |d| >= 0.474

    Args:
        x: First sample
        y: Second sample

    Returns:
        Tuple of (effect_size, interpretation)
    """
    # Convert to numpy arrays
    x = np.asarray(x)
    y = np.asarray(y)

    # Calculate Cliff's Delta
    # Delta = (P(x > y) - P(x < y)) where P is proportion
    n_x = len(x)
    n_y = len(y)

    # Count dominance
    greater = 0
    less = 0

    for xi in x:
        greater += np.sum(xi > y)
        less += np.sum(xi < y)

    # Cliff's Delta
    delta = (greater - less) / (n_x * n_y)

    # Interpretation
    abs_delta = abs(delta)
    if abs_delta < 0.147:
        interpretation = "negligible"
    elif abs_delta < 0.33:
        interpretation = "small"
    elif abs_delta < 0.474:
        interpretation = "medium"
    else:
        interpretation = "large"

    return delta, interpretation
```

Add docstring with formula and interpretation reference.
  </action>
  <verify>
```bash
python -c "
from analysis.stats_utils import cliffs_delta
import numpy as np

x = np.array([1, 2, 3, 4, 5])
y = np.array([2, 3, 4, 5, 6])  # Slight shift
delta, interp = cliffs_delta(x, y)
print(f'Cliff delta: {delta:.3f}, interpretation: {interp}')

# Test with larger difference
y2 = np.array([10, 11, 12, 13, 14])
delta2, interp2 = cliffs_delta(x, y2)
print(f'Cliff delta (large): {delta2:.3f}, interpretation: {interp2}')
"
```
  </verify>
  <done>
cliffs_delta function exists in stats_utils.py, returns both effect size value and interpretation string, and test cases show appropriate interpretations (small/large).
  </done>
</task>

<task type="auto">
  <name>Add odds_ratio and standardized_coefficient functions</name>
  <files>analysis/stats_utils.py</files>
  <action>
Add two more effect size functions to `analysis/stats_utils.py`:

1. **odds_ratio(counts1, counts2)** - Odds ratio for proportion comparison
```python
def odds_ratio(counts1: np.ndarray, counts2: np.ndarray, ci_level: float = 0.99) -> dict:
    """
    Calculate odds ratio with confidence interval for two proportions.

    Common in epidemiology and crime research for comparing proportions.

    Args:
        counts1: Array of [successes, failures] for group 1
        counts2: Array of [successes, failures] for group 2
        ci_level: Confidence level (default 0.99)

    Returns:
        Dict with or (odds ratio), ci_lower, ci_upper, p_value, is_significant
    """
    from scipy.stats import fisher_exact

    # 2x2 contingency table
    #         Group1   Group2
    # Success  a       b
    # Failure  c       d
    a, b = counts1[0], counts2[0]
    c, d = counts1[1], counts2[1]

    table = [[a, b], [c, d]]

    # Odds ratio
    or_val = (a * d) / (b * c) if (b * c) > 0 else float('inf')

    # Fisher's exact test for p-value
    _, p_value = fisher_exact(table, alternative='two-sided')

    # Log-based CI for odds ratio
    log_or = np.log(or_val) if or_val != float('inf') else np.log((a * d + 0.5) / (b * c + 0.5))
    se = np.sqrt(1/a + 1/b + 1/c + 1/d)

    from scipy.stats import norm
    z = norm.ppf(1 - (1 - ci_level) / 2)
    log_ci_low = log_or - z * se
    log_ci_high = log_or + z * se

    ci_lower = np.exp(log_ci_low)
    ci_upper = np.exp(log_ci_high)

    return {
        "or": or_val,
        "ci_lower": ci_lower,
        "ci_upper": ci_upper,
        "p_value": p_value,
        "is_significant": p_value < 0.01,
        "ci_level": ci_level
    }
```

2. **standardized_coefficient(x, y)** - Standardized regression coefficient
```python
def standardized_coefficient(x: np.ndarray, y: np.ndarray) -> dict:
    """
    Calculate standardized regression coefficient (beta) for correlation.

    Represents the change in Y (in SD units) for a 1 SD increase in X.

    Args:
        x: Predictor variable
        y: Outcome variable

    Returns:
        Dict with beta (standardized coefficient), ci_lower, ci_upper, p_value
    """
    # Z-score both variables
    x_z = (x - np.mean(x)) / np.std(x, ddof=1)
    y_z = (y - np.mean(y)) / np.std(y, ddof=1)

    # Simple regression of y_z on x_z
    from scipy.stats import linregress
    result = linregress(x_z, y_z)

    # Bootstrap CI for beta
    from scipy.stats import bootstrap
    import warnings

    def beta_stat(data):
        x_b, y_b = data
        x_z = (x_b - np.mean(x_b)) / np.std(x_b, ddof=1)
        y_z = (y_b - np.mean(y_b)) / np.std(y_b, ddof=1)
        # Slope is standardized coefficient
        with warnings.catch_warnings():
            warnings.simplefilter("ignore")
            result = linregress(x_z, y_z)
        return result.slope

    with warnings.catch_warnings():
        warnings.simplefilter("ignore")
        res = bootstrap((x, y), beta_stat, confidence_level=0.99,
                       n_resamples=9999, random_state=42)

    return {
        "beta": result.slope,
        "ci_lower": res.confidence_interval.low,
        "ci_upper": res.confidence_interval.high,
        "p_value": result.pvalue,
        "is_significant": result.pvalue < 0.01
    }
```

Add complete docstrings for both.
  </action>
  <verify>
```bash
python -c "
from analysis.stats_utils import odds_ratio, standardized_coefficient
import numpy as np

# Test odds ratio
# Summer: 1000 crimes out of 10000 days vs Winter: 800 crimes out of 10000 days
or_result = odds_ratio(np.array([1000, 9000]), np.array([800, 9200]))
print(f'Odds ratio: {or_result[\"or\"]:.3f}')
print(f'99% CI: [{or_result[\"ci_lower\"]:.3f}, {or_result[\"ci_upper\"]:.3f}]')
print(f'P-value: {or_result[\"p_value\"]:.4f}')

# Test standardized coefficient
x = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])
y = np.array([2, 4, 5, 4, 5, 7, 8, 6, 9, 11])
beta_result = standardized_coefficient(x, y)
print(f'Standardized beta: {beta_result[\"beta\"]:.3f}')
print(f'99% CI: [{beta_result[\"ci_lower\"]:.3f}, {beta_result[\"ci_upper\"]:.3f}]')
"
```
  </verify>
  <done>
odds_ratio and standardized_coefficient functions exist in stats_utils.py, odds_ratio returns OR with 99% CI and p-value, standardized_coefficient returns beta with 99% CI.
  </done>
</task>

<task type="auto">
  <name>Add Cliff's delta thresholds to config.py</name>
  <files>analysis/config.py</files>
  <action>
Add Cliff's Delta thresholds to STAT_CONFIG in `analysis/config.py`:

```python
STAT_CONFIG = {
    # ... existing keys ...

    # Effect size benchmarks (Cohen's d)
    "effect_size_small": 0.2,
    "effect_size_medium": 0.5,
    "effect_size_large": 0.8,

    # Cliff's Delta thresholds (non-parametric effect size)
    # Source: Romano et al. (2006)
    "cliffs_delta_negligible": 0.147,
    "cliffs_delta_small": 0.33,
    "cliffs_delta_medium": 0.474,
}
```

Also add a helper function in config.py or a comment showing how to interpret Cliff's delta:

```python
def interpret_cliffs_delta(delta: float) -> str:
    """Interpret Cliff's Delta effect size."""
    abs_d = abs(delta)
    if abs_d < STAT_CONFIG["cliffs_delta_negligible"]:
        return "negligible"
    elif abs_d < STAT_CONFIG["cliffs_delta_small"]:
        return "small"
    elif abs_d < STAT_CONFIG["cliffs_delta_medium"]:
        return "medium"
    else:
        return "large"
```
  </action>
  <verify>
```bash
python -c "
from analysis.config import STAT_CONFIG, interpret_cliffs_delta
print('Cliff delta thresholds:')
print(f'  Negligible: < {STAT_CONFIG[\"cliffs_delta_negligible\"]}')
print(f'  Small: < {STAT_CONFIG[\"cliffs_delta_small\"]}')
print(f'  Medium: < {STAT_CONFIG[\"cliffs_delta_medium\"]}')
print(f'  Large: >= {STAT_CONFIG[\"cliffs_delta_medium\"]}')
# Test interpretation
print(f'\\nInterpret 0.4: {interpret_cliffs_delta(0.4)}')
print(f'Interpret -0.6: {interpret_cliffs_delta(-0.6)}')
"
```
  </verify>
  <done>
config.py includes Cliff's delta thresholds in STAT_CONFIG and interpret_cliffs_delta helper function returns correct interpretations (negligible/small/medium/large).
  </done>
</task>

<task type="auto">
  <name>Verify FDR correction is applied in all modules</name>
  <files>analysis/temporal_analysis.py</files>
  <action>
Verify and update all analysis modules to report both raw and FDR-adjusted p-values:

1. Check each module for FDR usage:
   - temporal_analysis.py: FDR for year-over-year comparisons
   - summer_spike.py: FDR for yearly summer spike tests
   - covid_lockdown.py: FDR for monthly comparisons
   - spatial_analysis.py: FDR for district pairwise tests
   - categorical_analysis.py: FDR for crime type tests
   - cross_analysis.py: FDR for correlation tests
   - weighted_severity_analysis.py: FDR for district severity tests

2. Ensure report format includes both:
   ```markdown
   ### Comparison Results
   | Comparison | Raw p-value | FDR-adjusted p | Significant |
   |------------|-------------|----------------|-------------|
   | District A vs B | 0.0032 | 0.0124 | Yes |
   ```

3. Update any modules missing FDR reporting:
   - Add `from analysis.stats_utils import apply_fdr_correction` if missing
   - Store both raw and adjusted p-values in results dict
   - Include both in markdown report tables

4. Ensure effect sizes are paired with appropriate tests:
   - Parametric: Cohen's d
   - Non-parametric: Cliff's delta
   - Proportions: Odds ratio
   - Correlations: Standardized coefficient or r

Update modules as needed - focus on consistency of reporting format.
  </action>
  <verify>
```bash
python -c "
# Check modules import FDR correction
modules = [
    'analysis.temporal_analysis',
    'analysis.summer_spike',
    'analysis.covid_lockdown',
    'analysis.spatial_analysis',
    'analysis.categorical_analysis',
    'analysis.cross_analysis',
    'analysis.weighted_severity_analysis'
]

for mod in modules:
    try:
        m = __import__(mod, fromlist=[''])
        source = open(m.__file__).read()
        has_fdr = 'apply_fdr_correction' in source or 'false_discovery_control' in source
        has_effect_size = 'cohens_d' in source or 'cliffs_delta' in source or 'odds_ratio' in source
        print(f'{mod.split(\".\")[-1]}: FDR={has_fdr}, Effect size={has_effect_size}')
    except Exception as e:
        print(f'{mod}: Error - {e}')
"
```
  </verify>
  <done>
All 7 analysis modules include FDR correction (apply_fdr_correction or false_discovery_control), and all modules include appropriate effect size calculations (Cohen's d, Cliff's delta, or odds ratio).
  </done>
</task>

</tasks>

<verification>
After completing all tasks:

1. Run verify command from task 1 - cliffs_delta works correctly
2. Run verify command from task 2 - odds_ratio and standardized_coefficient work
3. Run verify command from task 3 - config has thresholds and interpret function
4. Run verify command from task 4 - all modules use FDR and effect sizes
5. Spot-check one module's report to see both raw and FDR-adjusted p-values
</verification>

<success_criteria>
1. stats_utils.py has cliffs_delta, odds_ratio, standardized_coefficient functions
2. config.py STAT_CONFIG includes Cliff's delta thresholds
3. interpret_cliffs_delta helper function exists in config.py
4. All 7 analysis modules apply FDR correction appropriately
5. All 7 analysis modules report appropriate effect sizes
6. Reports show both raw and FDR-adjusted p-values where applicable
7. All verify commands pass
</success_criteria>

<output>
After completion, create `.planning/phases/01-statistical-rigor/01-04-SUMMARY.md` with:
- New effect size functions added
- Cliff's delta thresholds configured
- FDR correction verification summary (which modules needed updates)
- Example effect sizes from test runs
- Any deviations from plan
</output>
