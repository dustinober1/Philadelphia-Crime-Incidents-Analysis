---
phase: 01-statistical-rigor
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - analysis/stats_utils.py
  - analysis/config.py
autonomous: true

must_haves:
  truths:
    - "User can import statistical test functions from analysis.stats_utils"
    - "User can run normality tests (Shapiro-Wilk, D'Agostino-Pearson) on any data array"
    - "User can compare two samples with automatic parametric/non-parametric selection"
    - "User can calculate Cohen's d effect size for two-group comparisons"
    - "User can calculate 99% bootstrap confidence intervals for any statistic"
    - "User can apply FDR (Benjamini-Hochberg) correction to arrays of p-values"
    - "User can perform Tukey HSD post-hoc tests for multi-group comparisons"
  artifacts:
    - path: "analysis/stats_utils.py"
      provides: "Centralized statistical testing functions"
      min_lines: 200
      exports: ["test_normality", "compare_two_samples", "cohens_d", "bootstrap_ci", "apply_fdr_correction", "tukey_hsd"]
    - path: "analysis/config.py"
      provides: "STAT_CONFIG constant for statistical parameters"
      contains: "STAT_CONFIG"
  key_links:
    - from: "analysis/temporal_analysis.py"
      to: "analysis/stats_utils.py"
      via: "from analysis.stats_utils import test_normality, mann_kendall_test, bootstrap_ci"
      pattern: "from analysis.stats_utils import"
    - from: "analysis/spatial_analysis.py"
      to: "analysis/stats_utils.py"
      via: "from analysis.stats_utils import apply_fdr_correction"
      pattern: "from analysis.stats_utils import"
---

<objective>
Create a centralized statistical testing utilities module that provides all statistical functions needed across the 11 analysis modules. This module will be the foundation for adding statistical rigor to temporal, spatial, and categorical analyses.

Purpose: Eliminate code duplication, ensure consistent statistical methodology across all analyses, and provide well-documented, reusable statistical functions.

Output: New `analysis/stats_utils.py` module with 7+ statistical functions, updated `analysis/config.py` with STAT_CONFIG, and all functions fully documented with docstrings and type hints.
</objective>

<execution_context>
@/Users/dustinober/.claude/get-shit-done/workflows/execute-plan.md
@/Users/dustinober/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-statistical-rigor/01-CONTEXT.md
@.planning/phases/01-statistical-rigor/01-RESEARCH.md
@analysis/config.py
@analysis/utils.py
@analysis/temporal_analysis.py
</context>

<tasks>

<task type="auto">
  <name>Create stats_utils.py module with core statistical functions</name>
  <files>analysis/stats_utils.py</files>
  <action>
Create `analysis/stats_utils.py` with the following functions:

1. **test_normality(data, alpha=0.05)** - Test normality using Shapiro-Wilk (n <= 5000) or D'Agostino-Pearson (n > 5000)
   - Returns: (is_normal: bool, statistic: float, p_value: float, test_name: str)
   - Use scipy.stats.shapiro for small samples, scipy.stats.normaltest for large

2. **compare_two_samples(x, y, alpha=0.05)** - Two-sample comparison with automatic test selection
   - Tests normality of both samples first
   - If both normal: scipy.stats.ttest_ind (Independent t-test)
   - If any non-normal: scipy.stats.mannwhitneyu (Mann-Whitney U)
   - Returns: dict with test_name, statistic, p_value, is_significant

3. **compare_multiple_samples(groups_dict, alpha=0.05)** - Multi-group comparison (ANOVA or Kruskal-Wallis)
   - Tests normality for all groups
   - If all normal: scipy.stats.f_oneway + scipy.stats.tukey_hsd
   - If any non-normal: scipy.stats.kruskal
   - Returns: dict with omnibus_test, statistic, p_value, post_hoc_results

4. **cohens_d(x, y)** - Cohen's d effect size for two samples
   - Implement manually: (mean_x - mean_y) / pooled_sd
   - Pooled SD: sqrt(((nx-1)*var_x + (ny-1)*var_y) / (nx+ny-2))
   - Returns: float (interpretation: small=0.2, medium=0.5, large=0.8)

5. **bootstrap_ci(data, statistic, confidence_level=0.99, n_resamples=9999, random_state=None)** - Bootstrap CI
   - Use scipy.stats.bootstrap
   - Returns: (lower_bound, upper_bound, point_estimate, standard_error)

6. **apply_fdr_correction(p_values, method='bh')** - False Discovery Rate correction
   - Use scipy.stats.false_discovery_control
   - Returns: numpy array of adjusted p-values

7. **tukey_hsd(*samples, confidence_level=0.99)** - Tukey HSD post-hoc test wrapper
   - Use scipy.stats.tukey_hsd
   - Returns: TukeyHSDResult with confidence intervals

8. **chi_square_test(contingency_table)** - Chi-square test of independence
   - Use scipy.stats.chi2_contingency
   - Returns: dict with statistic, p_value, dof, expected_freq

9. **correlation_test(x, y, method='auto')** - Correlation test with automatic selection
   - Test normality first
   - If both normal: scipy.stats.pearsonr
   - If any non-normal: scipy.stats.spearmanr
   - Returns: dict with correlation, p_value, test_name

Add type hints for all function signatures. Use `from typing import Tuple, Dict, Literal, Optional, Callable`.

Each function must have:
- Docstring with Args, Returns, Raises sections
- Example usage in docstring
- References to scipy functions used
  </action>
  <verify>
```bash
# Test module loads and functions exist
python -c "
from analysis.stats_utils import (
    test_normality, compare_two_samples, compare_multiple_samples,
    cohens_d, bootstrap_ci, apply_fdr_correction, tukey_hsd,
    chi_square_test, correlation_test
)
print('All functions imported successfully')

# Quick smoke test
import numpy as np
x = np.random.normal(0, 1, 100)
y = np.random.normal(0.5, 1, 100)
result = compare_two_samples(x, y)
print(f'Test result: {result}')
print(f'Cohens d: {cohens_d(x, y):.3f}')
"
```
  </verify>
  <done>
All 9 functions are importable from analysis.stats_utils, each has complete docstrings with Args/Returns/Raises, and smoke test runs without errors.
  </done>
</task>

<task type="auto">
  <name>Add STAT_CONFIG to config.py with statistical parameters</name>
  <files>analysis/config.py</files>
  <action>
Add a STAT_CONFIG dictionary to `analysis/config.py` with the following keys:

```python
# =============================================================================
# STATISTICAL ANALYSIS CONFIGURATION
# =============================================================================

STAT_CONFIG = {
    # Confidence level for all intervals (99% per project decision)
    "confidence_level": 0.99,

    # Significance alpha for tests (matches 99% CI)
    "alpha": 0.01,

    # Bootstrap parameters
    "bootstrap_n_resamples": 9999,
    "bootstrap_random_state": 42,

    # Normality test threshold
    "normality_alpha": 0.05,

    # Effect size benchmarks (Cohen's d)
    "effect_size_small": 0.2,
    "effect_size_medium": 0.5,
    "effect_size_large": 0.8,

    # FDR correction method ('bh' = Benjamini-Hochberg, 'by' = Benjamini-Yekutieli)
    "fdr_method": "bh",

    # Random seed for reproducibility
    "random_seed": 42,
}
```

Add comment explaining that 99% CI was chosen for more conservative analysis appropriate to exploratory nature.

Update the docstring comment at the top of config.py to mention STAT_CONFIG.
  </action>
  <verify>
```bash
python -c "
from analysis.config import STAT_CONFIG
print('STAT_CONFIG keys:', list(STAT_CONFIG.keys()))
print('Confidence level:', STAT_CONFIG['confidence_level'])
assert STAT_CONFIG['confidence_level'] == 0.99
assert STAT_CONFIG['alpha'] == 0.01
print('STAT_CONFIG verified')
"
```
  </verify>
  <done>
STAT_CONFIG exists in config.py with all 9 keys, confidence_level is 0.99, alpha is 0.01, and python import test passes.
  </done>
</task>

<task type="auto">
  <name>Install pymannkendall for Mann-Kendall trend tests</name>
  <files>.venv/pyvenv.cfg</files>
  <action>
Install the pymannkendall package for Mann-Kendall trend testing (required for temporal trend analysis per research):

```bash
# Activate virtual environment and install
source .venv/bin/activate
pip install pymannkendall
```

Add mann_kendall_test function to stats_utils.py:
- Uses pymannkendall.original_test
- Returns: dict with trend ('increasing', 'decreasing', 'no trend'), tau, p_value, is_significant
- Handles input as 1D array of time-ordered values
  </action>
  <verify>
```bash
source .venv/bin/activate
python -c "
import pymannkendall as mk
from analysis.stats_utils import mann_kendall_test
import numpy as np
data = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])
result = mann_kendall_test(data)
print(f'Trend: {result[\"trend\"]}, p-value: {result[\"p_value\"]:.4f}')
"
```
  </verify>
  <done>
pymannkendall is installed in .venv, mann_kendall_test function exists in stats_utils.py, and test on increasing trend returns 'increasing'.
  </done>
</task>

</tasks>

<verification>
After completing all tasks:

1. Run the smoke test verify command from task 1
2. Run the STAT_CONFIG verify command from task 2
3. Run the pymannkendall verify command from task 3
4. Check that all functions have docstrings: `python -c "import analysis.stats_utils; help(analysis.stats_utils.test_normality)"`
5. Verify type hints exist: `python -c "import inspect; from analysis.stats_utils import test_normality; print(inspect.signature(test_normality))"`
</verification>

<success_criteria>
1. analysis/stats_utils.py exists with 10+ statistical functions
2. All functions have complete docstrings (Args, Returns, Raises)
3. All functions have type hints on parameters and return values
4. STAT_CONFIG exists in config.py with 9+ configuration keys
5. pymannkendall is installed and importable
6. mann_kendall_test function works correctly
7. All verify commands pass without errors
</success_criteria>

<output>
After completion, create `.planning/phases/01-statistical-rigor/01-01-SUMMARY.md` with:
- Functions implemented
- STAT_CONFIG values
- Installation summary (pymannkendall version)
- Any deviations from plan
</output>
