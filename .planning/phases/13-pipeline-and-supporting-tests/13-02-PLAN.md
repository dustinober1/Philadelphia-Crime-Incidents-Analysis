---
phase: 13-pipeline-and-supporting-tests
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - tests/test_pipeline_refresh.py
autonomous: true

must_haves:
  truths:
    - Pipeline refresh validates all required export files exist
    - Artifact validation checks JSON structure for required keys
    - Reproducibility verification compares two export runs
    - Refresh CLI command invokes validation correctly
  artifacts:
    - path: tests/test_pipeline_refresh.py
      provides: Pipeline refresh and validation tests
      min_lines: 150
    - path: pipeline/refresh_data.py
      provides: Refresh functions under test (existing)
  key_links:
    - from: tests/test_pipeline_refresh.py
      to: pipeline/refresh_data.py
      via: Direct function imports and tmp_path fixture
      pattern: "from pipeline.refresh_data import"
---

<objective>
Create comprehensive tests for pipeline refresh operations (pipeline/refresh_data.py) validating data integrity, artifact structure, and reproducibility of export outputs.

Purpose: Ensure refresh operations detect missing/corrupt exports and verify data consistency across runs.

Output: New test_pipeline_refresh.py with 10+ tests covering validation and reproducibility.
</objective>

<execution_context>
@/Users/dustinober/.claude/get-shit-done/workflows/execute-plan.md
@/Users/dustinober/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/phases/13-pipeline-and-supporting-tests/13-RESEARCH.md
@.planning/milestones/v1.3-ROADMAP.md
@.planning/REQUIREMENTS.md

# Prior Phase Decisions
- Use tmp_path for isolated test file creation
- Validate JSON structure using json.loads()
- Test error paths with pytest.raises()
</context>

<tasks>

<task type="auto">
  <name>Task 1: Test _validate_artifacts success cases</name>
  <files>tests/test_pipeline_refresh.py</files>
  <action>Create tests/test_pipeline_refresh.py and add success validation tests:
  1. test_validate_artifacts_passes_with_complete_exports - Create all required files in _REQUIRED_FILES, verify no exception raised
  2. test_validate_artifacts_passes_with_valid_metadata - Verify metadata.json with all required keys passes
  3. test_validate_artifacts_passes_with_valid_annual_trends - Verify annual_trends.json is non-empty list
  4. test_validate_artifacts_passes_with_valid_forecast - Verify forecast.json contains historical and forecast keys

Import _validate_artifacts and _REQUIRED_FILES from pipeline.refresh_data. Use tmp_path to create directory structure and populate with valid JSON content. Use pytest.raises(RuntimeError) negatively (should not raise).</action>
  <verify>pytest tests/test_pipeline_refresh.py -k "validate_artifacts" -v</verify>
  <done>All 4 success case tests pass, validating artifact checks pass with valid exports.</done>
</task>

<task type="auto">
  <name>Task 2: Test _validate_artifacts failure cases</name>
  <files>tests/test_pipeline_refresh.py</files>
  <action>Add failure validation tests:
  1. test_validate_artifacts_raises_missing_files - Verify RuntimeError with "Missing required export files" when files missing
  2. test_validate_artifacts_raises_invalid_metadata_keys - Verify RuntimeError when metadata.json missing required keys
  3. test_validate_artifacts_raises_invalid_annual_trends - Verify RuntimeError when annual_trends.json is not a list
  4. test_validate_artifacts_raises_empty_annual_trends - Verify RuntimeError when annual_trends.json is empty list
  5. test_validate_artifacts_raises_invalid_forecast_structure - Verify RuntimeError when forecast.json missing historical/forecast

Create partial file sets to trigger validation failures. Use pytest.raises(RuntimeError, match="expected_message") to verify exception messages.</action>
  <verify>pytest tests/test_pipeline_refresh.py -k "validate_artifacts" -v</verify>
  <done>All 5 failure case tests pass, validating proper error detection for corrupt/missing exports.</done>
</task>

<task type="auto">
  <name>Task 3: Test _load_json and _canonical_json helpers</name>
  <files>tests/test_pipeline_refresh.py</files>
  <action>Add tests for JSON loading helpers:
  1. test_load_json_parses_valid_file - Verify _load_json returns parsed dict from valid JSON
  2. test_load_json_handles_unicode - Verify _load_json handles Unicode characters in JSON
  3. test_canonical_json_sorts_keys - Verify _canonical_json returns sorted key JSON for comparison
  4. test_canonical_json_removes_whitespace - Verify output has no extra whitespace

Create test JSON files with various key orderings and content. Verify canonicalization produces consistent output.</action>
  <verify>pytest tests/test_pipeline_refresh.py -k "load_json or canonical_json" -v</verify>
  <done>All 4 helper tests pass, validating JSON loading and canonicalization for comparison.</done>
</task>

<task type="auto">
  <name>Task 4: Test _assert_reproducible with mocked exports</name>
  <files>tests/test_pipeline_refresh.py</files>
  <action>Add tests for reproducibility verification:
  1. test_assert_reproducible_passes_deterministic_exports - Mock export_all to return same data twice, verify no exception
  2. test_assert_reproducible_detects_differences - Mock export_all to return different data, verify RuntimeError raised
  3. test_assert_reproducible_compares_all_required_files - Verify each file in _REQUIRED_FILES compared

Use unittest.mock.patch to mock export_all function. Create deterministic vs non-deterministic scenarios. For the difference test, mock the first run to return different content than second run.</action>
  <verify>pytest tests/test_pipeline_refresh.py -k "assert_reproducible" -v</verify>
  <done>All 3 reproducibility tests pass, validating export consistency detection.</done>
</task>

<task type="auto">
  <name>Task 5: Test refresh CLI run command</name>
  <files>tests/test_pipeline_refresh.py</files>
  <action>Add CLI integration tests:
  1. test_refresh_run_creates_exports - Use CliRunner to invoke refresh run command, verify exports created
  2. test_refresh_run_validates_artifacts - Verify validation performed after export
  3. test_refresh_run_custom_output_dir - Verify --output-dir option works correctly
  4. test_refresh_run_with_verify_reproducibility - Verify --verify-reproducibility flag triggers reproducibility check

Use typer.testing.CliRunner with app from pipeline.refresh_data. Mock export_all to avoid real data loading. Verify exit_code == 0 and stdout contains expected messages.</action>
  <verify>pytest tests/test_pipeline_refresh.py -k "refresh_run" -v</verify>
  <done>All 4 CLI tests pass, validating refresh command invocation and flag handling.</done>
</task>

<task type="auto">
  <name>Task 6: Test refresh with environment variable override</name>
  <files>tests/test_pipeline_refresh.py</files>
  <action>Add test for environment variable configuration:
  1. test_refresh_run_uses_env_output_dir - Verify PIPELINE_OUTPUT_DIR env variable overrides default

Use monkeypatch fixture to set PIPELINE_OUTPUT_DIR environment variable. Run refresh command without --output-dir and verify exports created in env-specified location.</action>
  <verify>pytest tests/test_pipeline_refresh.py -k "env" -v</verify>
  <done>Environment variable override test passes, validating configuration priority.</done>
</task>

</tasks>

<verification>
Run all pipeline refresh tests: pytest tests/test_pipeline_refresh.py -v

Run with coverage: pytest tests/test_pipeline_refresh.py --cov=pipeline/refresh_data --cov-report=term-missing

Verify:
1. All tests pass (target: 15+ tests)
2. Coverage of pipeline/refresh_data.py exceeds 90%
3. Mocked export_all avoids real data operations
4. Tests complete in under 5 seconds
</verification>

<success_criteria>
1. pipeline/refresh_data.py has 90%+ test coverage
2. All validation functions tested with success and failure cases
3. Reproducibility verification tested
4. CLI command tested with CliRunner
5. Environment variable configuration validated
</success_criteria>

<output>
After completion, create `.planning/phases/13-pipeline-and-supporting-tests/13-02-SUMMARY.md` with:
- Number of tests added
- Coverage achieved for pipeline/refresh_data.py
- Validation error message coverage
</output>
