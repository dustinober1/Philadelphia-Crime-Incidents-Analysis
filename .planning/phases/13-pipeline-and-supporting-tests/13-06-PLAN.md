---
phase: 13-pipeline-and-supporting-tests
plan: 06
type: execute
wave: 2
depends_on: []
files_modified:
  - tests/test_visualization_plots.py
  - tests/test_visualization_helpers.py
autonomous: true

must_haves:
  truths:
    - Visualization functions return valid Figure objects
    - Chart properties (title, labels, data) are correctly set
    - Style configuration applies consistent settings
    - Matplotlib Agg backend prevents display issues in tests
  artifacts:
    - path: tests/test_visualization_plots.py
      provides: Plot function tests
      min_lines: 200
    - path: tests/test_visualization_helpers.py
      provides: Helper function tests
      min_lines: 100
    - path: analysis/visualization/plots.py
      provides: plot_line, plot_bar, plot_heatmap functions
    - path: analysis/visualization/helpers.py
      provides: save_figure function
    - path: analysis/visualization/style.py
      provides: setup_style function
    - path: analysis/visualization/forecast_plots.py
      provides: Forecast plotting functions
  key_links:
    - from: tests/test_visualization_*.py
      to: analysis/visualization/*.py
      via: Agg backend setup and Figure object assertions
      pattern: "matplotlib.use\\(['\"]Agg"
---

<objective>
Create tests for visualization modules (analysis/visualization/). Test plot generation functions, style configuration, and figure saving helpers. Use Agg backend for headless testing and validate Figure structure rather than pixel-perfect rendering.

Purpose: Achieve comprehensive coverage of visualization modules (SUPP-02 requirement).

Output: New test_visualization_plots.py and test_visualization_helpers.py with 25+ tests.
</objective>

<execution_context>
@/Users/dustinober/.claude/get-shit-done/workflows/execute-plan.md
@/Users/dustinober/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/phases/13-pipeline-and-supporting-tests/13-RESEARCH.md
@.planning/milestones/v1.3-ROADMAP.md
@.planning/REQUIREMENTS.md

# Prior Phase Decisions
- Use matplotlib.use('Agg') backend for headless testing
- Test Figure structure, not pixel values
- Close figures after testing to prevent memory leaks
- Import sample_crime_df fixture for test data
</context>

<tasks>

<task type="auto">
  <name>Task 1: Test setup_style function</name>
  <files>tests/test_visualization_helpers.py</files>
  <action>Create tests/test_visualization_helpers.py with Agg backend and style tests:
  1. test_setup_style_applies_rcparams - Verify setup_style modifies plt.rcParams correctly
  2. test_setup_style_figure_size - Verify figsize set to (12, 6)
  3. test_setup_style_font_sizes - Verify font.size=11, titlesize=14, labelsize=12
  4. test_setup_style_grid_settings - Verify grid enabled with alpha=0.3
  5. test_setup_style_savefig_settings - Verify dpi=300, bbox='tight', facecolor='white'
  6. test_setup_style_seabern_palette - Verify seaborn palette uses COLORS

Set matplotlib.use('Agg') at top of file. Import setup_style from analysis.visualization.style. Verify plt.rcParams values match expected settings.</action>
  <verify>pytest tests/test_visualization_helpers.py -k "setup_style" -v</verify>
  <done>All 6 setup_style tests pass, validating style configuration.</done>
</task>

<task type="auto">
  <name>Task 2: Test save_figure helper</name>
  <files>tests/test_visualization_helpers.py</files>
  <action>Add save_figure tests:
  1. test_save_figure_png_format - Verify PNG saved with DPI=300
  2. test_save_figure_svg_format - Verify SVG saved (DPI ignored for vector formats)
  3. test_save_figure_pdf_format - Verify PDF saved (DPI ignored for vector formats)
  4. test_save_figure_invalid_format_raises - Verify ValueError for invalid format ("jpg")
  5. test_save_figure_creates_directories - Verify parent directories created if needed
  6. test_save_figure_bbox_inches_tight - Verify bbox_inches='tight' applied

Use tmp_path for output. Create test Figure with plt.subplots(). Call save_figure and verify file exists. Test error case with pytest.raises(ValueError).</action>
  <verify>pytest tests/test_visualization_helpers.py -k "save_figure" -v</verify>
  <done>All 6 save_figure tests pass, validating figure saving logic.</done>
</task>

<task type="auto">
  <name>Task 3: Test plot_line function</name>
  <files>tests/test_visualization_plots.py</files>
  <action>Create tests/test_visualization_plots.py with plot_line tests:
  1. test_plot_line_returns_figure - Verify function returns plt.Figure instance
  2. test_plot_line_title_and_labels - Verify title, xlabel, ylabel set correctly
  3. test_plot_line_default_color - Verify uses COLORS["Violent"] when color not specified
  4. test_plot_line_custom_color - Verify custom color parameter applied
  5. test_plot_line_data_plotted - Verify line data matches input DataFrame
  6. test_plot_line_empty_dataframe - Verify handles empty DataFrame gracefully

Use sample_crime_df with date and value columns. Call plot_line and verify Figure properties. Check ax.lines[0] for data and color. Use plt.close(fig) after each test.</action>
  <verify>pytest tests/test_visualization_plots.py -k "plot_line" -v</verify>
  <done>All 6 plot_line tests pass, validating line chart generation.</done>
</task>

<task type="auto">
  <name>Task 4: Test plot_bar function</name>
  <files>tests/test_visualization_plots.py</files>
  <action>Add plot_bar tests:
  1. test_plot_bar_returns_figure - Verify returns plt.Figure instance
  2. test_plot_bar_title_and_labels - Verify title and labels set correctly
  3. test_plot_bar_default_color - Verify uses COLORS["Property"] when color not specified
  4. test_plot_bar_custom_color - Verify custom color applied
  5. test_plot_bar_data_plotted - Verify bar count matches DataFrame rows
  6. test_plot_bar_handles_negative_values - Verify negative values plotted correctly
  7. test_plot_bar_grid_enabled - Verify grid shown on y-axis

Create test DataFrame with category and value columns. Verify ax.patches contains bars. Check bar heights match input values.</action>
  <verify>pytest tests/test_visualization_plots.py -k "plot_bar" -v</verify>
  <done>All 7 plot_bar tests pass, validating bar chart generation.</done>
</task>

<task type="auto">
  <name>Task 5: Test plot_heatmap function</name>
  <files>tests/test_visualization_plots.py</files>
  <action>Add plot_heatmap tests:
  1. test_plot_heatmap_returns_figure - Verify returns plt.Figure instance
  2. test_plot_heatmap_title_set - Verify title applied correctly
  3. test_plot_heatmap_default_figsize - Verify figsize defaults to (12, 10)
  4. test_plot_heatmap_custom_figsize - Verify custom figsize applied
  5. test_plot_heatmap_uses_triangular_mask - Verify upper triangle masked
  6. test_plot_heatmap_annot_default - Verify annotations shown by default

Create correlation DataFrame with numeric data. Call plot_heatmap and verify Figure properties. Check for heatmap elements and mask.</action>
  <verify>pytest tests/test_visualization_plots.py -k "plot_heatmap" -v</verify>
  <done>All 6 plot_heatmap tests pass, validating heatmap generation.</done>
</task>

<task type="auto">
  <name>Task 6: Test forecast_plots functions</name>
  <files>tests/test_visualization_plots.py</files>
  <action>Add forecast_plots tests:
  1. test_plot_forecast_with_intervals_returns_figure - Verify returns Figure
  2. test_plot_forecast_with_intervals_plots_components - Verify actual, forecast, and confidence interval plotted
  3. test_plot_forecast_components_returns_figure - Verify components Figure created
  4. test_plot_residuals_diagnostics_returns_figure - Verify diagnostics Figure with 4 subplots
  5. test_plot_feature_importance_returns_figure - Verify horizontal bar chart created
  6. test_plot_correlation_matrix_returns_figure - Verify correlation heatmap created

Import from analysis.visualization.forecast_plots. Create test data with pandas Series and DataFrames. Verify Figure structure (number of axes, titles). Use plt.close() after each test.</action>
  <verify>pytest tests/test_visualization_plots.py -k "forecast" -v</verify>
  <done>All 6 forecast plot tests pass, validating forecast visualization functions.</done>
</task>

<task type="auto">
  <name>Task 7: Test visualization edge cases</name>
  <files>tests/test_visualization_plots.py, tests/test_visualization_helpers.py</files>
  <action>Add edge case tests:
  1. test_plot_line_single_point - Verify handles single data point
  2. test_plot_bar_single_category - Verify handles single category
  3. test_save_figure_overwrites_existing - Verify overwrites existing file
  4. test_save_figure_custom_dpi - Verify custom DPI parameter works
  5. test_plot_heatmap_empty_dataframe - Verify handles empty correlation matrix

Test boundary conditions and edge cases to ensure robustness.</action>
  <verify>pytest tests/test_visualization_plots.py tests/test_visualization_helpers.py -k "edge or single or empty or custom" -v</verify>
  <done>All 5 edge case tests pass, validating robustness at boundaries.</done>
</task>

</tasks>

<verification>
Run all visualization tests: pytest tests/test_visualization_plots.py tests/test_visualization_helpers.py -v

Run with coverage: pytest tests/test_visualization_plots.py tests/test_visualization_helpers.py --cov=analysis/visualization --cov-report=term-missing

Verify:
1. All tests pass (target: 35+ tests)
2. Coverage of analysis/visualization/ exceeds 85%
3. Tests use Agg backend (no display required)
4. All figures closed to prevent memory leaks
</verification>

<success_criteria>
1. analysis/visualization/ has 85%+ test coverage
2. plot_line, plot_bar, plot_heatmap tested
3. save_figure helper tested
4. setup_style configuration tested
5. Forecast plot functions tested
6. Edge cases covered
7. Agg backend used consistently
</success_criteria>

<output>
After completion, create `.planning/phases/13-pipeline-and-supporting-tests/13-06-SUMMARY.md` with:
- Number of tests added
- Coverage achieved for analysis/visualization/
- List of functions tested
- Note on any excluded functions (e.g., shap_summary requiring optional shap library)
</output>
