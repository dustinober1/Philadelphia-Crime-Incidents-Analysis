---
phase: 13-pipeline-and-supporting-tests
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - tests/test_pipeline_export.py
autonomous: true

must_haves:
  truths:
    - Pipeline export operations can be tested without real data files
    - All export functions generate valid JSON/GeoJSON output structures
    - External dependencies (GeoPandas, Prophet) are mocked appropriately
    - Export orchestration calls all sub-functions in correct order
  artifacts:
    - path: tests/test_pipeline_export.py
      provides: Pipeline export operation tests
      min_lines: 200
    - path: pipeline/export_data.py
      provides: Export functions under test (existing)
  key_links:
    - from: tests/test_pipeline_export.py
      to: pipeline/export_data.py
      via: unittest.mock.patch for data loading and external dependencies
      pattern: "with patch\\(['\"]pipeline.export_data.load_crime_data"
---

<objective>
Expand test coverage for pipeline export operations (pipeline/export_data.py) by testing export functions with mocked data loaders and external dependencies. Tests validate JSON/GeoJSON output structure, export orchestration, and error handling without requiring real data files or heavy dependencies.

Purpose: Achieve comprehensive coverage of pipeline export operations to ensure data export reliability while keeping tests fast through mocking.

Output: Expanded test_pipeline_export.py with 15+ new tests covering all export functions.
</objective>

<execution_context>
@/Users/dustinober/.claude/get-shit-done/workflows/execute-plan.md
@/Users/dustinober/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/phases/13-pipeline-and-supporting-tests/13-RESEARCH.md
@.planning/milestones/v1.3-ROADMAP.md
@.planning/REQUIREMENTS.md

# Prior Phase Decisions (from Phase 11-12)
- Mock GeoPandas operations at import boundary for fast tests
- Use unittest.mock.patch for external dependencies
- Test workflow logic, not library internals
- pytest.raises(KeyError) for unhandled exception testing
</context>

<tasks>

<task type="auto">
  <name>Task 1: Test helper functions (_write_json, _to_records, _ensure_dir)</name>
  <files>tests/test_pipeline_export.py</files>
  <action>Add tests for pipeline export helper functions:
  1. test_write_json_creates_valid_file - Verify _write_json creates file with valid JSON content
  2. test_write_json_handles_path_object - Verify _write_json accepts Path objects
  3. test_to_records_converts_dataframe - Verify _to_records converts DataFrame to list of dicts
  4. test_to_records_handles_datetime - Verify datetime values converted to ISO format
  5. test_to_records_handles_none_values - Verify None values preserved in output
  6. test_ensure_dir_creates_directory - Verify _ensure_dir creates parent directories
  7. test_ensure_dir_idempotent - Verify _ensure_dir safe to call on existing directory

Use tmp_path fixture for file operations. Import functions from pipeline.export_data after patching heavy dependencies if needed.</action>
  <verify>pytest tests/test_pipeline_export.py -k "helper" -v</verify>
  <done>All 7 helper function tests pass, validating JSON serialization, datetime handling, and directory creation.</done>
</task>

<task type="auto">
  <name>Task 2: Test _export_trends function</name>
  <files>tests/test_pipeline_export.py</files>
  <action>Add tests for _export_trends export function:
  1. test_export_trends_creates_annual_json - Verify annual_trends.json created with year/category/count structure
  2. test_export_trends_creates_monthly_json - Verify monthly_trends.json created with month/category data
  3. test_export_trends_creates_covid_comparison - Verify covid_comparison.json with Pre/During/Post periods
  4. test_export_trends_handles_empty_dataframe - Verify graceful handling of empty input

Use sample_crime_df fixture with dispatch_date and ucr_general columns. Mock classify_crime_category and extract_temporal_features if they have side effects. Validate JSON structure using json.loads() on output files.</action>
  <verify>pytest tests/test_pipeline_export.py -k "export_trends" -v</verify>
  <done>All 4 _export_trends tests pass, validating trend JSON generation with proper temporal aggregation.</done>
</task>

<task type="auto">
  <name>Task 3: Test _export_seasonality function</name>
  <files>tests/test_pipeline_export.py</files>
  <action>Add tests for _export_seasonality export function:
  1. test_export_seasonality_creates_seasonality_json - Verify seasonality.json with by_month/by_day_of_week/by_hour
  2. test_export_seasonality_creates_robbery_heatmap - Verify robbery_heatmap.json with hour/day_of_week matrix
  3. test_export_seasonality_handles_missing_hour - Verify hour NaN values filled with 0 before grouping

Use sample_crime_df with temporal features. Validate JSON structure contains expected keys and data is aggregated correctly.</action>
  <verify>pytest tests/test_pipeline_export.py -k "export_seasonality" -v</verify>
  <done>All 3 _export_seasonality tests pass, validating seasonal pattern aggregation and robbery heatmap generation.</done>
</task>

<task type="auto">
  <name>Task 4: Test _export_spatial function with mocked GeoPandas</name>
  <files>tests/test_pipeline_export.py</files>
  <action>Add tests for _export_spatial export function:
  1. test_export_spatial_without_geopandas - Verify returns early when HAS_GEOPANDAS is False
  2. test_export_spatial_creates_districts_geojson - Mock gpd.read_file and gpd.sjoin, verify districts.geojson created
  3. test_export_spatial_creates_tracts_geojson - Mock tracts read and spatial join, verify tracts.geojson with crime_rate
  4. test_export_spatial_creates_hotspots_and_corridors - Mock file reads, verify hotspot_centroids.geojson and corridors.geojson
  5. test_export_spatial_creates_spatial_summary - Verify spatial_summary.json with counts

Use unittest.mock.patch to mock gpd module at import boundary: patch("pipeline.export_data.gpd"). Create mock GeoDataFrames with to_file method to verify writes.</action>
  <verify>pytest tests/test_pipeline_export.py -k "export_spatial" -v</verify>
  <done>All 5 _export_spatial tests pass, validating GeoJSON export generation without real GeoPandas operations.</done>
</task>

<task type="auto">
  <name>Task 5: Test _export_policy function</name>
  <files>tests/test_pipeline_export.py</files>
  <action>Add tests for _export_policy export function:
  1. test_export_policy_creates_retail_theft_trend - Verify retail_theft_trend.json with UCR 600-699 filtered data
  2. test_export_policy_creates_vehicle_crime_trend - Verify vehicle_crime_trend.json with UCR 700-799 filtered data
  3. test_export_policy_creates_composition - Verify crime_composition.json with year/crime_category aggregation
  4. test_export_policy_handles_missing_event_file - Verify event_impact.json is empty list when event file missing
  5. test_export_policy_loads_event_file_when_exists - Verify event_impact.json populated when event file exists

Use sample_crime_df with ucr_general column. Mock repo_root path and event file existence with tmp_path.</action>
  <verify>pytest tests/test_pipeline_export.py -k "export_policy" -v</verify>
  <done>All 5 _export_policy tests pass, validating policy-specific exports with proper UCR filtering.</done>
</task>

<task type="auto">
  <name>Task 6: Test _export_forecasting function</name>
  <files>tests/test_pipeline_export.py</files>
  <action>Add tests for _export_forecasting export function:
  1. test_export_forecasting_with_prophet - Mock Prophet, verify forecast.json with historical/forecast/model
  2. test_export_forecasting_fallback_without_prophet - Set HAS_PROPHET=False, verify LinearFallback model used
  3. test_export_forecasting_creates_classification_features - Verify classification_features.json with feature/importance
  4. test_export_forecasting_classification_with_sklearn - Mock RandomForestClassifier, verify feature importances calculated
  5. test_export_forecasting_classification_fallback - Verify default importances when HAS_SKLEARN=False

Use unittest.mock.patch to mock Prophet and sklearn. Verify forecast structure contains historical, forecast, and model fields.</action>
  <verify>pytest tests/test_pipeline_export.py -k "export_forecasting" -v</verify>
  <done>All 5 _export_forecasting tests pass, validating time series forecast and classification feature export.</done>
</task>

<task type="auto">
  <name>Task 7: Test _export_metadata function</name>
  <files>tests/test_pipeline_export.py</files>
  <action>Add tests for _export_metadata export function:
  1. test_export_metadata_creates_json - Verify metadata.json created with all required fields
  2. test_export_metadata_includes_colors - Verify COLORS dict exported in metadata
  3. test_export_metadata_date_range - Verify date_start and date_end from dispatch_date range
  4. test_export_metadata_total_incidents - Verify total_incidents matches DataFrame length
  5. test_export_metadata_timestamp_format - Verify last_updated is ISO format with timezone

Use sample_crime_df with dispatch_date column. Validate all ExportMetadata fields present in output JSON.</action>
  <verify>pytest tests/test_pipeline_export.py -k "export_metadata" -v</verify>
  <done>All 5 _export_metadata tests pass, validating metadata structure and field values.</done>
</task>

<task type="auto">
  <name>Task 8: Test export_all orchestration function</name>
  <files>tests/test_pipeline_export.py</files>
  <action>Add tests for export_all orchestration:
  1. test_export_all_creates_geo_subdirectory - Verify geo/ directory created under output_dir
  2. test_export_all_calls_all_export_functions - Mock all _export_* functions, verify each called once
  3. test_export_all_returns_resolved_path - Verify function returns absolute Path to output directory
  4. test_export_all_handles_relative_path - Verify relative output_dir converted to absolute
  5. test_export_all_loads_clean_data - Verify load_crime_data called with clean=True

Use unittest.mock.patch with multiple context managers or patch.multiple to mock all dependencies. Verify call order and arguments.</action>
  <verify>pytest tests/test_pipeline_export.py -k "export_all" -v</verify>
  <done>All 5 export_all tests pass, validating export orchestration and subdirectory creation.</done>
</task>

</tasks>

<verification>
Run all pipeline export tests: pytest tests/test_pipeline_export.py -v

Run with coverage: pytest tests/test_pipeline_export.py --cov=pipeline/export_data --cov-report=term-missing

Verify:
1. All new tests pass (target: 30+ tests total)
2. Coverage of pipeline/export_data.py exceeds 85%
3. Tests complete in under 10 seconds (mocking effective)
4. No external dependencies (GeoPandas, Prophet) actually imported during tests
</verification>

<success_criteria>
1. pipeline/export_data.py has 85%+ test coverage
2. All export functions (_export_*) have tests validating output structure
3. Helper functions (_write_json, _to_records, _ensure_dir) tested
4. export_all orchestration tested with mocked dependencies
5. Tests run in under 10 seconds without real data files
</success_criteria>

<output>
After completion, create `.planning/phases/13-pipeline-and-supporting-tests/13-01-SUMMARY.md` with:
- Number of tests added
- Coverage achieved for pipeline/export_data.py
- Any uncovered lines and rationale for exclusion
- Time measurements for test execution
</output>
