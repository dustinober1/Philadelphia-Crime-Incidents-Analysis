---
phase: 13-pipeline-and-supporting-tests
plan: 03
type: execute
wave: 1
depends_on: [13-01, 13-02]
files_modified:
  - tests/test_pipeline_export.py
  - tests/test_pipeline_refresh.py
autonomous: true

must_haves:
  truths:
    - Pipeline error handling paths are covered by tests
    - Missing file scenarios are handled appropriately
    - Invalid data scenarios raise informative errors
    - External dependency unavailability is handled gracefully
  artifacts:
    - path: tests/test_pipeline_export.py
      provides: Pipeline export error path tests (extended)
    - path: tests/test_pipeline_refresh.py
      provides: Pipeline refresh error path tests (extended)
  key_links:
    - from: tests/test_pipeline_*.py
      to: pipeline/export_data.py, pipeline/refresh_data.py
      via: pytest.raises() and monkeypatch for error simulation
      pattern: "pytest.raises\\(.*Error"
---

<objective>
Add error handling tests to achieve comprehensive coverage of pipeline error paths. Test missing files, invalid data, external dependency failures, and edge cases to ensure robust pipeline operations.

Purpose: Complete pipeline error path coverage (PIPE-03 requirement) and achieve 90%+ coverage across pipeline modules.

Output: Extended test files with error path tests covering all exception branches.
</objective>

<execution_context>
@/Users/dustinober/.claude/get-shit-done/workflows/execute-plan.md
@/Users/dustinober/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/phases/13-pipeline-and-supporting-tests/13-RESEARCH.md
@.planning/milestones/v1.3-ROADMAP.md
@.planning/REQUIREMENTS.md

# Prior Phase Decisions
- Test both success and failure paths
- Use monkeypatch to simulate error conditions
- Validate error messages with match parameter in pytest.raises
</context>

<tasks>

<task type="auto">
  <name>Task 1: Test export error handling for missing dependencies</name>
  <files>tests/test_pipeline_export.py</files>
  <action>Add error handling tests for optional dependencies:
  1. test_export_spatial_returns_early_without_geopandas - Verify _export_spatial returns without error when HAS_GEOPANDAS=False
  2. test_export_forecasting_uses_fallback_without_prophet - Verify _export_forecasting uses LinearFallback when HAS_PROPHET=False
  3. test_export_forecasting_uses_defaults_without_sklearn - Verify classification_features uses defaults when HAS_SKLEARN=False

Patch the HAS_* flags in pipeline.export_data module to simulate missing dependencies. Verify functions complete without raising exceptions and produce valid output.</action>
  <verify>pytest tests/test_pipeline_export.py -k "missing_dependency or without_geopandas or without_prophet or without_sklearn" -v</verify>
  <done>All dependency handling tests pass, validating graceful fallback behavior.</done>
</task>

<task type="auto">
  <name>Task 2: Test export error handling for data issues</name>
  <files>tests/test_pipeline_export.py</files>
  <action>Add tests for problematic input data:
  1. test_export_metadata_empty_dataframe - Verify _export_metadata handles empty DataFrame gracefully
  2. test_export_metadata_missing_dispatch_date - Verify appropriate error when dispatch_date column missing
  3. test_export_trends_empty_dataframe - Verify _export_trends produces valid empty JSON structures
  4. test_export_spatial_empty_coordinates - Verify _export_spatial handles DataFrame with no valid coordinates

Use sample_crime_df modified to trigger edge cases (drop columns, empty DataFrames). Verify functions either handle gracefully or raise informative errors.</action>
  <verify>pytest tests/test_pipeline_export.py -k "empty_dataframe or missing_dispatch" -v</verify>
  <done>All data issue tests pass, validating edge case handling.</done>
</task>

<task type="auto">
  <name>Task 3: Test export error handling for file system issues</name>
  <files>tests/test_pipeline_export.py</files>
  <action>Add tests for file system errors:
  1. test_write_json_handles_permission_error - Verify _write_json raises appropriate error on permission denied
  2. test_ensure_dir_handles_permission_error - Verify _ensure_dir raises appropriate error on permission denied
  3. test_export_all_handles_unwritable_directory - Verify export_all raises informative error when output directory not writable

Use monkeypatch to mock Path.write_text and Path.mkdir to raise PermissionError or OSError. Verify error types and messages are appropriate.</action>
  <verify>pytest tests/test_pipeline_export.py -k "permission or unwritable" -v</verify>
  <done>All file system error tests pass, validating proper error propagation.</done>
</task>

<task type="auto">
  <name>Task 4: Test refresh error handling for corrupt artifacts</name>
  <files>tests/test_pipeline_refresh.py</files>
  <action>Add tests for corrupt artifact detection:
  1. test_validate_artifacts_raises_invalid_json - Verify RuntimeError when JSON file is malformed
  2. test_validate_artifacts_raises_wrong_type_metadata - Verify RuntimeError when metadata.json is not a dict
  3. test_validate_artifacts_raises_missing_nested_keys - Verify RuntimeError when nested keys missing (e.g., forecast.historical)
  4. test_load_json_handles_invalid_json - Verify _load_json raises JSONDecodeError for malformed JSON

Create files with invalid JSON content or wrong data types. Verify appropriate exceptions raised with helpful error messages.</action>
  <verify>pytest tests/test_pipeline_refresh.py -k "corrupt or invalid_json or wrong_type" -v</verify>
  <done>All corrupt artifact tests pass, validating error detection for bad export files.</done>
</task>

<task type="auto">
  <name>Task 5: Test refresh error handling for reproducibility failures</name>
  <files>tests/test_pipeline_refresh.py</files>
  <action>Add tests for reproducibility check failures:
  1. test_assert_reproducible_raises_on_difference - Verify RuntimeError when exports differ between runs
  2. test_assert_reproducible_identifies_differing_files - Verify error message lists which files differ

Mock export_all to return different canonical JSON on first vs second call. Verify RuntimeError raised and message includes differing file names.</action>
  <verify>pytest tests/test_pipeline_refresh.py -k "reproducible" -v</verify>
  <done>Reproducibility failure tests pass, validating difference detection.</done>
</task>

<task type="auto">
  <name>Task 6: Test CLI error handling</name>
  <files>tests/test_pipeline_refresh.py</files>
  <action>Add tests for CLI error scenarios:
  1. test_refresh_run_exits_nonzero_on_validation_error - Verify exit_code != 0 when validation fails
  2. test_refresh_run_exits_nonzero_on_reproducibility_failure - Verify exit_code != 0 when reproducibility check fails
  3. test_refresh_run_shows_error_message - Verify stderr contains error message on failure

Use CliRunner to invoke refresh with mocked export_all that triggers validation errors. Verify exit codes and error output.</action>
  <verify>pytest tests/test_pipeline_refresh.py -k "cli_error or nonzero" -v</verify>
  <done>All CLI error handling tests pass, validating proper error reporting through CLI.</done>
</task>

<task type="auto">
  <name>Task 7: Test boundary conditions and edge cases</name>
  <files>tests/test_pipeline_export.py, tests/test_pipeline_refresh.py</files>
  <action>Add tests for boundary conditions:
  1. test_export_metadata_single_row - Verify handles DataFrame with single row
  2. test_export_metadata_future_dates - Verify handles future dates in dispatch_date
  3. test_validate_artifacts_zero_incidents - Verify validation passes when total_incidents is 0
  4. test_export_all_with_none_values - Verify handles DataFrame with None values in various columns

Test edge cases around boundary values (empty, single row, zeros, None, future dates). Ensure functions handle these cases without crashing.</action>
  <verify>pytest tests/test_pipeline_export.py tests/test_pipeline_refresh.py -k "boundary or edge or single_row or zero" -v</verify>
  <done>All boundary condition tests pass, validating robustness at edge cases.</done>
</task>

</tasks>

<verification>
Run all pipeline tests with coverage: pytest tests/test_pipeline_export.py tests/test_pipeline_refresh.py --cov=pipeline --cov-report=term-missing --cov-report=html

Verify:
1. All tests pass (target: 45+ tests total across both files)
2. Combined coverage of pipeline/ exceeds 90%
3. Error paths specifically covered (check coverage report for exception handling branches)
4. No missing branches in error handling code
</verification>

<success_criteria>
1. pipeline/ module has 90%+ test coverage
2. All error handling paths tested
3. Edge cases and boundary conditions covered
4. CLI error exit codes validated
5. Corrupt artifact detection verified
</success_criteria>

<output>
After completion, create `.planning/phases/13-pipeline-and-supporting-tests/13-03-SUMMARY.md` with:
- Total tests added for error handling
- Coverage achieved for pipeline/ module
- List of error paths tested
- Any remaining uncovered lines and rationale
</output>
