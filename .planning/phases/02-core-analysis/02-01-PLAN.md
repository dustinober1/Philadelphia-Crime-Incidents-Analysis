---
phase: 02-core-analysis
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - notebooks/02_exploratory_analysis.ipynb
  - output/figures/exploratory/
  - output/tables/exploratory/
autonomous: true

must_haves:
  truths:
    - "All variables have documented univariate distributions"
    - "Missing value patterns are visualized and interpreted"
    - "Key variable relationships are identified for deeper analysis"
    - "Statistical summaries are saved for downstream notebooks"
  artifacts:
    - path: "notebooks/02_exploratory_analysis.ipynb"
      provides: "Complete exploratory analysis with visualizations"
      min_lines: 200
    - path: "output/figures/exploratory/"
      provides: "Distribution plots for key variables"
      count: 8
    - path: "output/tables/exploratory/summary_stats.csv"
      provides: "Statistical summaries by variable"
    - path: "output/tables/exploratory/correlation_matrix.csv"
      provides: "Variable correlation matrix"
  key_links:
    - from: "notebooks/02_exploratory_analysis.ipynb"
      to: "data/processed/crime_incidents_cleaned.parquet"
      via: "pd.read_parquet()"
      pattern: "read_parquet.*cleaned"
    - from: "notebooks/02_exploratory_analysis.ipynb"
      to: "scripts/config.py"
      via: "import config"
      pattern: "from scripts import config|import config"
---

<objective>
Create comprehensive exploratory analysis notebook examining univariate distributions, missing value patterns, and initial variable relationships to guide subsequent focused analyses.

Purpose: Establish baseline understanding of the cleaned dataset; identify patterns, anomalies, and hypotheses for deeper investigation in specialized notebooks (temporal, geographic, offense-specific).
Output: Notebook 02 with publication-quality distribution plots, summary statistics, correlation analysis, and documented findings.
</objective>

<execution_context>
@~/.config/opencode/get-shit-done/workflows/execute-plan.md
@~/.config/opencode/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-core-analysis/02-RESEARCH.md
@.planning/phases/02-core-analysis/02-CONTEXT.md
@scripts/config.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Load Data and Configure Environment</name>
  <files>notebooks/02_exploratory_analysis.ipynb</files>
  <action>
    Create notebook 02_exploratory_analysis.ipynb with the following setup:
    
    1. Import standard libraries (pandas, numpy, matplotlib, seaborn)
    2. Import scripts/config.py for paths and constants
    3. Load data/processed/crime_incidents_cleaned.parquet
    4. Configure matplotlib for publication-quality figures (300 DPI, colorblind-friendly palettes)
    5. Set up output directories: output/figures/exploratory/, output/tables/exploratory/
    6. Document data shape, columns, and memory usage
    7. Exclude last 30 days for reporting lag (per Phase 1 decision)
    
    Use config.py constants for column names (COL_DATE, COL_DISTRICT, etc.).
    Apply 02-RESEARCH.md Pattern 5 for figure configuration.
  </action>
  <verify>Notebook cell runs without errors; data loads successfully; shape matches Phase 1 output (~3.5M records)</verify>
  <done>Notebook created with data loaded, environment configured, and basic data inspection complete</done>
</task>

<task type="auto">
  <name>Task 2: Univariate Distributions and Missing Value Analysis</name>
  <files>notebooks/02_exploratory_analysis.ipynb</files>
  <action>
    Add comprehensive univariate analysis to the notebook:
    
    1. **Temporal variables:**
       - Distribution of incidents by year, month, day-of-week, hour
       - Histogram with proper binning
       - Summary statistics (mean, median, std, min, max per time unit)
    
    2. **Geographic variables:**
       - Distribution by district (dc_dist) - bar chart
       - Coordinate scatter plot (sample 50k points for performance)
       - Geocoding coverage summary
    
    3. **Offense variables:**
       - UCR code distribution (ucr_general) - bar chart with counts
       - Top 15 text_general_code categories
       - Severity classification (violent/property/other) pie chart
    
    4. **Missing value patterns:**
       - Missing value heatmap by column
       - Missing value co-occurrence analysis
       - Documentation of patterns (e.g., missing coords by offense type)
    
    5. Save figures to output/figures/exploratory/:
       - temporal_distributions.png
       - district_distribution.png
       - ucr_distribution.png
       - top_offenses.png
       - missing_values_heatmap.png
    
    6. Save summary statistics to output/tables/exploratory/summary_stats.csv
    
    Use seaborn for statistical plots; ensure all figures have proper titles, labels, and captions.
  </action>
  <verify>All distribution plots generated and saved; summary_stats.csv created with descriptive statistics for all variables</verify>
  <done>Univariate distributions documented; missing value patterns visualized and interpreted; 5+ figures saved</done>
</task>

<task type="auto">
  <name>Task 3: Bivariate Analysis and Hypothesis Generation</name>
  <files>notebooks/02_exploratory_analysis.ipynb</files>
  <action>
    Add bivariate analysis and correlation exploration:
    
    1. **Cross-tabulations:**
       - District × UCR category (violent/property/other)
       - Day-of-week × Hour heatmap
       - Year × UCR category trends
    
    2. **Correlation analysis:**
       - Correlation matrix for numeric variables
       - Heatmap visualization with annotations
       - Identify strongest correlations for deeper investigation
    
    3. **Initial hypotheses:**
       - Document 5-10 testable hypotheses based on observed patterns
       - Example: "Violent crime shows stronger seasonal variation than property crime"
       - Example: "Certain districts have disproportionate violent crime rates"
    
    4. **Anomaly detection:**
       - Identify outliers in temporal trends (sudden spikes/drops)
       - Flag districts with unusual offense distributions
       - Note any data quality concerns for downstream notebooks
    
    5. Save outputs:
       - correlation_matrix.csv to output/tables/exploratory/
       - cross_tab_district_ucr.csv to output/tables/exploratory/
       - hypotheses.md section in notebook markdown
    
    6. **Notebook conclusion:**
       - Summary of key findings
       - Recommendations for downstream analyses
       - Data quality flags
    
    Ensure all findings reference specific figures/tables.
  </action>
  <verify>Correlation matrix saved; cross-tabulations complete; 5-10 hypotheses documented; notebook runs end-to-end without errors</verify>
  <done>Bivariate analysis complete; hypotheses generated; notebook finalized with findings summary and recommendations</done>
</task>

</tasks>

<verification>
- [ ] Notebook 02 runs without errors from top to bottom
- [ ] All figures saved to output/figures/exploratory/ (5+ figures)
- [ ] All tables saved to output/tables/exploratory/ (2+ CSVs)
- [ ] Summary statistics include all key variables
- [ ] Missing value patterns documented with interpretations
- [ ] Hypotheses are specific and testable
- [ ] Notebook references config.py for all paths
</verification>

<success_criteria>
- Notebook 02 complete with univariate and bivariate analysis
- 8+ publication-quality figures generated
- Summary statistics and correlation matrix saved for downstream use
- 5-10 testable hypotheses documented
- Data quality flags identified for other notebooks
- Analysis aligns with known Philadelphia patterns (summer peaks, weekday variation)
</success_criteria>

<output>
After completion, create `.planning/phases/02-core-analysis/02-01-SUMMARY.md`
</output>
