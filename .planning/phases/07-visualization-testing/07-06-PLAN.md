---
phase: 07-visualization-testing
plan: 06
type: execute
wave: 4
depends_on: ["07-03", "07-04", "07-05"]
files_modified:
  - tests/test_integration_output_verification.py
autonomous: true

must_haves:
  truths:
    - "Developer can verify CLI outputs match expected patterns"
    - "Integration tests check key statistics rather than exact data matches"
    - "Tests validate that outputs are structurally correct"
    - "Coverage target of 90% is approachable with current test suite"
  artifacts:
    - path: "tests/test_integration_output_verification.py"
      provides: "Integration tests verifying CLI output structure"
      exports: ["test_chief_trends_output_structure", "test_patrol_hotspots_output_structure"]
      min_lines: 80
  key_links:
    - from: "tests/test_integration_output_verification.py"
      to: "tests/test_cli_*.py"
      via: "Builds on CLI test patterns"
      pattern: "runner.invoke.*assert.*Path.exists"
</key_links>
---

<objective>
Create integration tests that verify CLI command outputs are structurally correct and contain expected patterns. These tests validate that generated reports and figures have the expected format without requiring exact pixel-perfect matches.

Purpose: Provide additional confidence that CLI commands produce correct output artifacts and achieve the 90% coverage target by testing output validation paths.

Output: tests/test_integration_output_verification.py with integration tests for key CLI commands
</objective>

<execution_context>
@/Users/dustinober/.claude/get-shit-done/workflows/execute-plan.md
@/Users/dustinober/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07-visualization-testing/07-RESEARCH.md

# Existing CLI tests to build on
@tests/test_cli_chief.py
@tests/test_cli_patrol.py
@tests/test_cli_policy.py
@tests/test_cli_forecasting.py

# Coverage target from requirements
@pyproject.toml  # --cov-fail-under=90
</context>

<tasks>

<task type="auto">
  <name>Create integration tests for output verification</name>
  <files>tests/test_integration_output_verification.py</files>
  <action>
Create tests/test_integration_output_verification.py with:

1. Import CliRunner, Path, pytest
2. Import app from analysis.cli.main
3. Create runner = CliRunner()
4. Add pytest.mark.integration decorator to tests
5. Create test_chief_trends_output_structure():
   - Invoke chief trends --fast --version integration-test
   - Verify output directory exists: reports/integration-test/chief/
   - Verify annual_trends_report_summary.txt exists
   - Read file and verify it contains expected headers (e.g., "Annual Trends", "Total Incidents")
6. Create test_patrol_hotspots_output_structure():
   - Invoke patrol hotspots --fast --version integration-test
   - Verify output directory exists
   - Verify at least one output file exists (PNG or TXT)
7. Create test_policy_retail_theft_output_structure():
   - Invoke policy retail-theft --fast --version integration-test
   - Verify output files exist
   - Check output contains expected keywords
8. Create test_forecasting_classification_output_structure():
   - Invoke forecasting classification --fast --version integration-test
   - Skip if sklearn not available
   - Verify model metrics file exists

DO NOT:
- Don't check exact data values (use tolerance or pattern matching)
- Don't require pixel-perfect image comparison (use file existence)
- Don't skip all tests (mark individual tests with pytest.importorskip for optional deps)
  </action>
  <verify>
# Run integration tests only
pytest tests/test_integration_output_verification.py -v -m integration

# Or run all tests including integration
pytest tests/ -v -m integration
  </verify>
  <done>
test_integration_output_verification.py exists with 4+ integration tests verifying output structure for key commands; tests use --version integration-test to isolate outputs
  </done>
</task>

<task type="auto">
  <name>Run full test suite and measure coverage</name>
  <files>tests/test_integration_output_verification.py</files>
  <action>
Run the complete test suite and measure coverage:

1. Run all tests with coverage:
   ```bash
   pytest tests/ --cov=analysis --cov-report=term-missing --cov-report=html
   ```
2. Check if 90% target is met
3. Identify modules with < 90% coverage
4. Note which modules need additional tests (for future work, not this plan)
5. Run integration tests separately to verify they work

DO NOT:
- Don't add more tests to reach 90% in this plan (coverage may need Phase 8 migration)
- Don't fail the plan if coverage < 90% (note the gap in SUMMARY)
  </action>
  <verify>
# Run full suite with coverage
pytest tests/ --cov=analysis --cov-report=term-missing | grep -A 5 "TOTAL"

# Check which modules are below 90%
pytest tests/ --cov=analysis --cov-report=term-missing | grep "%" | awk '$4 < 90.0'
  </verify>
  <done>
Full test suite runs successfully; coverage report shows current percentage; gaps identified for Phase 8 (notebook migration will cover CLI paths)
  </done>
</task>

</tasks>

<verification>
Run these verification steps after all tasks complete:

1. Run integration tests:
   ```bash
   pytest tests/test_integration_output_verification.py -v -m integration
   ```
   Expected: All integration tests pass

2. Run complete test suite with coverage:
   ```bash
   pytest tests/ --cov=analysis --cov-report=term-missing --cov-report=html
   ```
   Expected: All tests pass, coverage report generated

3. Check specific CLI module coverage:
   ```bash
   pytest tests/ --cov=analysis/cli --cov-report=term-missing
   ```
   Expected: > 70% coverage for CLI modules (will improve in Phase 8)

4. Verify output files are created correctly:
   ```bash
   ls -la reports/integration-test/
   ```
   Expected: chief/, patrol/, policy/, forecasting/ directories with output files

5. Clean up integration test outputs:
   ```bash
   rm -rf reports/integration-test/
   ```
</verification>

<success_criteria>
1. tests/test_integration_output_verification.py exists with 4+ integration tests
2. Integration tests verify output structure (not exact content)
3. Integration tests use --version integration-test flag
4. Full test suite (unit + integration + CLI) passes
5. Coverage report shows current percentage (note gaps if < 90%)
6. Integration tests are marked with @pytest.mark.integration
7. Optional dependencies handled gracefully
</success_criteria>

<output>
After completion, create `.planning/phases/07-visualization-testing/07-06-SUMMARY.md` with:
- Performance metrics (duration, files created)
- Integration tests added (4+ tests)
- Current coverage percentage (note if < 90%)
- Coverage gaps identified (which modules need more tests)
- Integration test isolation (--version flag pattern)
- Any deviations from plan
- Git commit hash
</output>
