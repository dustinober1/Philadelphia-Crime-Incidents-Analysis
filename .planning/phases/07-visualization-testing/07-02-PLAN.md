---
phase: 07-visualization-testing
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - tests/conftest.py
  - tests/fixtures/sample_crime_data.csv
autonomous: true

must_haves:
  truths:
    - "Developer can run tests without loading full 3.4M-row dataset"
    - "Sample fixture provides representative crime data (~100 rows)"
    - "Fixture includes all required columns (dispatch_date, ucr_general, point_x, point_y, objectid)"
    - "Tests complete in < 10 seconds using sample fixtures"
  artifacts:
    - path: "tests/conftest.py"
      provides: "Shared pytest fixtures for all tests"
      exports: ["sample_crime_df", "tmp_output_dir"]
    - path: "tests/fixtures/sample_crime_data.csv"
      provides: "Sample dataset for fast tests"
      min_lines: 50
  key_links:
    - from: "All test files"
      to: "tests/conftest.py"
      via: "pytest fixture discovery"
      pattern: "def test_*(sample_crime_df)"
</key_links>
---

<objective>
Create pytest fixtures in tests/conftest.py to provide small sample datasets for fast unit tests. This avoids loading the full 3.4M-row crime dataset in every test, ensuring tests run quickly and developers run them frequently.

Purpose: Enable fast test execution (< 10 seconds) by providing representative sample data via pytest fixtures rather than loading full production data.

Output: tests/conftest.py with sample_crime_df and tmp_output_dir fixtures; optional CSV fixture file
</objective>

<execution_context>
@/Users/dustinober/.claude/get-shit-done/workflows/execute-plan.md
@/Users/dustinober/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07-visualization-testing/07-RESEARCH.md

# Existing test patterns to follow
@tests/test_classification.py  # Parametrized tests, clear organization
@tests/test_data_loading.py  # 85% coverage with efficient tests
</context>

<tasks>

<task type="auto">
  <name>Create conftest.py with sample data fixtures</name>
  <files>tests/conftest.py</files>
  <action>
Create tests/conftest.py with:

1. Import pytest, pandas as pd, numpy as np, Path from pathlib
2. Create @pytest.fixture for sample_crime_df():
   - Use np.random.seed(42) for reproducibility
   - Create 100-row DataFrame with columns:
     - objectid: range(1, 101)
     - dispatch_date: pd.date_range("2020-01-01", periods=100, freq="D")
     - ucr_general: random choice from [100, 200, 300, 500, 600, 700, 800]
     - point_x: random uniform(-75.3, -74.95)
     - point_y: random uniform(39.85, 40.15)
     - dc_dist: random choice from range(1, 24)
   - Add Google-style docstring
3. Create @pytest.fixture for tmp_output_dir(tmp_path):
   - Create output directory under tmp_path
   - Return Path object
   - Add docstring
4. Add module docstring explaining fixture usage

DO NOT:
- Don't load from CSV unless necessary (create in-memory DataFrame)
- Don't create fixture that loads full dataset
- Don't use session scope (each test gets fresh data)
  </action>
  <verify>
# Test fixture is discoverable and provides expected data
python -c "
import sys
sys.path.insert(0, 'tests')
from conftest import sample_crime_df
import pandas as pd
import inspect

# Verify function signature
sig = inspect.signature(sample_crime_df.func)
print('Fixture signature:', sig)

# Verify fixture can be called
import pytest
from _pytest.fixtures import FixtureRequest
print('sample_crime_df is a fixture:', hasattr(sample_crime_df, '_pytestfixturefunction'))
"
  </verify>
  <done>
sample_crime_df fixture creates 100-row DataFrame with representative crime data; tmp_output_dir fixture provides temporary output directory for tests
  </done>
</task>

<task type="auto">
  <name>Verify fixtures work with existing tests</name>
  <files>tests/conftest.py</files>
  <action>
Test the new fixtures by running a simple verification:

1. Create inline test using sample_crime_df fixture
2. Run pytest to verify fixture is discoverable
3. Check that data has expected columns and types
4. Verify tmp_output_dir creates valid path

DO NOT:
- Don't modify existing test files (this is just verification)
- Don't create permanent test files (use pytest --collect-only or inline verification)
  </action>
  <verify>
# Run pytest to verify fixtures are available
pytest --collect-only -q 2>&1 | head -20
# Should show sample_crime_df and tmp_output_dir as available fixtures

# Quick inline test
pytest -v -k "test_" --co -q 2>&1 | grep -i fixture | head -10
  </verify>
  <done>
pytest discovers sample_crime_df and tmp_output_dir fixtures; fixtures provide data with correct schema (objectid, dispatch_date, ucr_general, point_x, point_y, dc_dist)
  </done>
</task>

</tasks>

<verification>
Run these verification steps after all tasks complete:

1. Check fixture discovery:
   ```bash
   pytest --fixtures -q 2>&1 | grep -A 3 "sample_crime_df"
   ```
   Expected: Shows fixture docstring and function signature

2. Verify fixture provides correct columns:
   ```bash
   python -c "
   import sys
   import pytest
   from pathlib import Path

   # Create minimal conftest test
   test_code = '''
def test_sample_fixture(sample_crime_df):
    import pandas as pd
    assert isinstance(sample_crime_df, pd.DataFrame)
    assert len(sample_crime_df) == 100
    assert 'objectid' in sample_crime_df.columns
    assert 'dispatch_date' in sample_crime_df.columns
    assert 'ucr_general' in sample_crime_df.columns
    assert 'point_x' in sample_crime_df.columns
    assert 'point_y' in sample_crime_df.columns
    assert 'dc_dist' in sample_crime_df.columns
    print('Fixture verification passed')
'''
   Path('test_fixture_verify.py').write_text(test_code)
   "
   ```
   Then: `pytest test_fixture_verify.py -v && rm test_fixture_verify.py`

3. Verify tmp_output_dir fixture:
   ```bash
   python -c "
   import pytest
   from pathlib import Path
   test_code = '''
def test_tmp_dir(tmp_output_dir):
    assert tmp_output_dir.exists()
    assert tmp_output_dir.is_dir()
    print('tmp_output_dir works')
'''
   Path('test_tmp_verify.py').write_text(test_code)
   "
   ```
   Then: `pytest test_tmp_verify.py -v && rm test_tmp_verify.py`

4. Check test execution time:
   ```bash
   time pytest tests/test_classification.py -v
   ```
   Expected: Completes in < 5 seconds
</verification>

<success_criteria>
1. pytest --fixtures shows sample_crime_df and tmp_output_dir
2. sample_crime_df provides DataFrame with 100 rows and required columns
3. tmp_output_dir provides valid Path object for test outputs
4. Existing tests still pass with new fixtures available
5. No import errors when importing from conftest.py
6. Fixtures use np.random.seed(42) for reproducibility
</success_criteria>

<output>
After completion, create `.planning/phases/07-visualization-testing/07-02-SUMMARY.md` with:
- Performance metrics (duration, files created)
- Fixtures added (sample_crime_df, tmp_output_dir)
- Sample data schema (100 rows, columns defined)
- Any deviations from plan
- Git commit hash
</output>
