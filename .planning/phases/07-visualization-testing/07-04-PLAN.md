---
phase: 07-visualization-testing
plan: 04
type: execute
wave: 2
depends_on: ["07-01", "07-02"]
files_modified:
  - tests/test_cli_patrol.py
autonomous: true

must_haves:
  truths:
    - "Developer can run pytest and test all Patrol CLI commands"
    - "CLI tests use --fast flag to avoid loading full dataset"
    - "Tests verify exit_code == 0 and expected output files exist"
    - "Tests use CliRunner from typer.testing for clean invocation"
  artifacts:
    - path: "tests/test_cli_patrol.py"
      provides: "End-to-end CLI tests for Patrol commands"
      exports: ["test_patrol_hotspots", "test_patrol_robbery_heatmap", "test_patrol_district_severity", "test_patrol_census_rates"]
      min_lines: 120
  key_links:
    - from: "tests/test_cli_patrol.py"
      to: "analysis/cli/main.app"
      via: "CliRunner invocation"
      pattern: "runner.invoke\(app, \['patrol'"
    - from: "tests/test_cli_patrol.py"
      to: "tests/conftest.py"
      via: "fixture usage"
      pattern: "def test_*(tmp_output_dir)"
</key_links>
---

<objective>
Create end-to-end CLI tests for all Patrol commands (hotspots, robbery-heatmap, district-severity, census-rates) using typer.testing.CliRunner. These tests verify that CLI commands execute successfully, produce expected output, and use the --fast flag for quick execution.

Purpose: Ensure the 4 Patrol CLI commands work end-to-end, generate correct output files, and can be tested automatically without manual CLI invocation.

Output: tests/test_cli_patrol.py with 8 tests covering all Patrol commands (2 per command)
</objective>

<execution_context>
@/Users/dustinober/.claude/get-shit-done/workflows/execute-plan.md
@/Users/dustinober/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07-visualization-testing/07-RESEARCH.md

# Patrol CLI commands to test
@analysis/cli/patrol.py  # hotspots, robbery-heatmap, district-severity, census-rates commands
@analysis/cli/main.py  # Main typer app with 'patrol' command group

# Pattern established in 07-03
@tests/test_cli_chief.py  # Follow same structure for consistency
</context>

<tasks>

<task type="auto">
  <name>Create test_cli_patrol.py with Patrol command tests</name>
  <files>tests/test_cli_patrol.py</files>
  <action>
Create tests/test_cli_patrol.py following the same pattern as test_cli_chief.py:

1. Import CliRunner from typer.testing
2. Import app from analysis.cli.main
3. Import Path from pathlib
4. Create runner = CliRunner() at module level
5. Create test class TestPatrolHotspots with tests:
   - test_patrol_hotspots_basic(): Invoke with --fast, assert exit_code==0, check output contains "Hotspots"
   - test_patrol_hotspots_output_files(): Invoke with --fast and --version test, assert output files exist
6. Create test class TestPatrolRobberyHeatmap with tests:
   - test_patrol_robbery_heatmap_basic(): Invoke with --fast, assert exit_code==0
   - test_patrol_robbery_heatmap_output_files(): Assert heatmap files created
7. Create test class TestPatrolDistrictSeverity with tests:
   - test_patrol_district_severity_basic(): Invoke with --fast, assert exit_code==0
   - test_patrol_district_severity_output_files(): Assert severity analysis files created
8. Create test class TestPatrolCensusRates with tests:
   - test_patrol_census_rates_basic(): Invoke with --fast, assert exit_code==0
   - test_patrol_census_rates_output_files(): Assert census rates files created

DO NOT:
- Don't omit --fast flag (spatial operations are expensive)
- Don't test without --version test (clutters reports/ directory)
- Don't assert exact heatmap image content (check file existence only)
  </action>
  <verify>
# Run the new tests
pytest tests/test_cli_patrol.py -v --tb=short

# Should show all tests passing
# Expected: ~8 tests (2 per command)
  </verify>
  <done>
test_cli_patrol.py exists with 8 tests covering hotspots, robbery-heatmap, district-severity, and census-rates commands; all tests use --fast flag and verify exit_code==0 and output file existence
  </done>
</task>

<task type="auto">
  <name>Verify tests pass and measure execution time</name>
  <files>tests/test_cli_patrol.py</files>
  <action>
Run the Patrol CLI tests and verify:

1. All tests pass (exit_code == 0, output files exist)
2. Tests complete in reasonable time (< 60 seconds for all 8 tests - spatial ops are slower)
3. Output files are created in correct location (reports/test/patrol/)
4. No unexpected errors or warnings
5. geopandas optional dependency is handled gracefully (skip tests if not available)

If tests fail:
- Check that CLI commands exist in analysis/cli/patrol.py
- Verify geopandas is installed or tests handle ImportError
- Ensure --fast flag is properly passed
- Check that output directory structure is correct

DO NOT:
- Don't proceed if basic tests fail (debug first)
- Don't accept tests that take > 120 seconds (indicates --fast not working or geopandas issues)
  </action>
  <verify>
# Run tests with timing
time pytest tests/test_cli_patrol.py -v

# Check coverage contribution
pytest tests/test_cli_patrol.py --cov=analysis/cli/patrol --cov-report=term-missing
  </verify>
  <done>
All 8 Patrol CLI tests pass in < 60 seconds; coverage of analysis/cli/patrol.py increases; output files are created correctly; optional geopandas dependency handled gracefully
  </done>
</task>

</tasks>

<verification>
Run these verification steps after all tasks complete:

1. Run all Patrol CLI tests:
   ```bash
   pytest tests/test_cli_patrol.py -v
   ```
   Expected: 8 tests passing (possibly fewer if geopandas not installed)

2. Check test execution time:
   ```bash
   time pytest tests/test_cli_patrol.py -v
   ```
   Expected: < 60 seconds

3. Verify output files created:
   ```bash
   ls -la reports/test/patrol/
   ```
   Expected: Hotspots, heatmap, severity, and census output files

4. Check CLI coverage increases:
   ```bash
   pytest tests/test_cli_patrol.py --cov=analysis/cli/patrol --cov-report=term-missing
   ```
   Expected: > 50% coverage of patrol.py

5. Verify all CLI tests (Chief + Patrol) still pass together:
   ```bash
   pytest tests/test_cli_chief.py tests/test_cli_patrol.py -v
   ```
   Expected: 14 tests passing (6 Chief + 8 Patrol)
</verification>

<success_criteria>
1. tests/test_cli_patrol.py exists with 8 tests (2 per command)
2. All tests use CliRunner from typer.testing
3. All tests use --fast flag for quick execution
4. All tests verify exit_code == 0
5. All tests check expected output in result.stdout
6. Output file tests verify files exist in reports/test/patrol/
7. All tests pass in < 60 seconds total
8. Coverage of analysis/cli/patrol.py increases measurably
9. Tests handle missing geopandas gracefully (skip or mock)
</success_criteria>

<output>
After completion, create `.planning/phases/07-visualization-testing/07-04-SUMMARY.md` with:
- Performance metrics (duration, files created)
- Tests added (8 tests for 4 commands)
- Coverage improvement for patrol.py
- Execution time benchmarks
- Geopandas optional dependency handling
- Any deviations from plan
- Git commit hash
</output>
