---
phase: 07-visualization-testing
plan: 03
type: execute
wave: 2
depends_on: ["07-01", "07-02"]
files_modified:
  - tests/test_cli_chief.py
autonomous: true

must_haves:
  truths:
    - "Developer can run pytest and test all Chief CLI commands"
    - "CLI tests use --fast flag to avoid loading full dataset"
    - "Tests verify exit_code == 0 and expected output files exist"
    - "Tests use CliRunner from typer.testing for clean invocation"
  artifacts:
    - path: "tests/test_cli_chief.py"
      provides: "End-to-end CLI tests for Chief commands"
      exports: ["test_chief_trends", "test_chief_seasonality", "test_chief_covid"]
      min_lines: 100
  key_links:
    - from: "tests/test_cli_chief.py"
      to: "analysis/cli/main.app"
      via: "CliRunner invocation"
      pattern: "runner.invoke\(app, \['chief'"
    - from: "tests/test_cli_chief.py"
      to: "tests/conftest.py"
      via: "fixture usage"
      pattern: "def test_*(tmp_output_dir)"
</key_links>
---

<objective>
Create end-to-end CLI tests for all Chief commands (trends, seasonality, covid) using typer.testing.CliRunner. These tests verify that CLI commands execute successfully, produce expected output, and use the --fast flag for quick execution.

Purpose: Ensure the 3 Chief CLI commands work end-to-end, generate correct output files, and can be tested automatically without manual CLI invocation.

Output: tests/test_cli_chief.py with 3 test functions covering all Chief commands
</objective>

<execution_context>
@/Users/dustinober/.claude/get-shit-done/workflows/execute-plan.md
@/Users/dustinober/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07-visualization-testing/07-RESEARCH.md

# Chief CLI commands to test
@analysis/cli/chief.py  # trends, seasonality, covid commands with Rich progress
@analysis/cli/main.py  # Main typer app with 'chief' command group

# CLI testing pattern from research
from typer.testing import CliRunner
from analysis.cli.main import app

runner = CliRunner()

def test_chief_trends_command():
    result = runner.invoke(app, ["chief", "trends", "--fast"])
    assert result.exit_code == 0
    assert "Annual Trends" in result.output
</context>

<tasks>

<task type="auto">
  <name>Create test_cli_chief.py with Chief command tests</name>
  <files>tests/test_cli_chief.py</files>
  <action>
Create tests/test_cli_chief.py with:

1. Import CliRunner from typer.testing
2. Import app from analysis.cli.main
3. Import Path from pathlib
4. Create runner = CliRunner() at module level
5. Create test class TestChiefTrends with tests:
   - test_chief_trends_basic(): Invoke with --fast, assert exit_code==0, check output contains "Annual Trends"
   - test_chief_trends_output_files(): Invoke with --fast and --version test, assert output files exist
   - test_chief_trends_date_range(): Test --start-year and --end-year overrides
6. Create test class TestChiefSeasonality with tests:
   - test_chief_seasonality_basic(): Invoke with --fast, assert exit_code==0, check output
   - test_chief_seasonality_output_files(): Assert expected files created
7. Create test class TestChiefCovid with tests:
   - test_chief_covid_basic(): Invoke with --fast, assert exit_code==0
   - test_chief_covid_output_files(): Assert expected files created

DO NOT:
- Don't omit --fast flag (tests would take too long)
- Don't test without --version test (clutters reports/ directory)
- Don't assert exact output content (check key phrases only)
  </action>
  <verify>
# Run the new tests
pytest tests/test_cli_chief.py -v --tb=short

# Should show all tests passing
# Expected: ~6 tests (2 per command)
  </verify>
  <done>
test_cli_chief.py exists with 6 tests covering trends, seasonality, and covid commands; all tests use --fast flag and verify exit_code==0 and output file existence
  </done>
</task>

<task type="auto">
  <name>Verify tests pass and measure execution time</name>
  <files>tests/test_cli_chief.py</files>
  <action>
Run the Chief CLI tests and verify:

1. All tests pass (exit_code == 0, output files exist)
2. Tests complete in reasonable time (< 30 seconds for all 6 tests)
3. Output files are created in correct location (reports/test/chief/)
4. No unexpected errors or warnings

If tests fail:
- Check that CLI commands exist in analysis/cli/chief.py
- Verify CliRunner import works
- Ensure --fast flag is properly passed
- Check that output directory structure is correct

DO NOT:
- Don't proceed if tests fail (debug first)
- Don't accept tests that take > 60 seconds (indicates --fast not working)
  </action>
  <verify>
# Run tests with timing
time pytest tests/test_cli_chief.py -v

# Check coverage contribution
pytest tests/test_cli_chief.py --cov=analysis/cli/chief --cov-report=term-missing
  </verify>
  <done>
All 6 Chief CLI tests pass in < 30 seconds; coverage of analysis/cli/chief.py increases; output files are created correctly
  </done>
</task>

</tasks>

<verification>
Run these verification steps after all tasks complete:

1. Run all Chief CLI tests:
   ```bash
   pytest tests/test_cli_chief.py -v
   ```
   Expected: 6 tests passing

2. Check test execution time:
   ```bash
   time pytest tests/test_cli_chief.py -v
   ```
   Expected: < 30 seconds

3. Verify output files created:
   ```bash
   ls -la reports/test/chief/
   ```
   Expected: Annual trends, seasonality, and COVID output files

4. Check CLI coverage increases:
   ```bash
   pytest tests/test_cli_chief.py --cov=analysis/cli/chief --cov-report=term-missing
   ```
   Expected: > 50% coverage of chief.py

5. Run with verbose output to see Rich progress in action:
   ```bash
   pytest tests/test_cli_chief.py::TestChiefTrends::test_chief_trends_basic -v -s 2>&1 | head -20
   ```
</verification>

<success_criteria>
1. tests/test_cli_chief.py exists with 6 tests (2 per command)
2. All tests use CliRunner from typer.testing
3. All tests use --fast flag for quick execution
4. All tests verify exit_code == 0
5. All tests check expected output in result.stdout
6. Output file tests verify files exist in reports/test/chief/
7. All tests pass in < 30 seconds total
8. Coverage of analysis/cli/chief.py increases measurably
</success_criteria>

<output>
After completion, create `.planning/phases/07-visualization-testing/07-03-SUMMARY.md` with:
- Performance metrics (duration, files created)
- Tests added (6 tests for 3 commands)
- Coverage improvement for chief.py
- Execution time benchmarks
- Any deviations from plan
- Git commit hash
</output>
