---
phase: 07-visualization-testing
plan: 05
type: execute
wave: 3
depends_on: ["07-02"]
files_modified:
  - tests/test_cli_policy.py
  - tests/test_cli_forecasting.py
autonomous: true

must_haves:
  truths:
    - "Developer can run pytest and test all Policy and Forecasting CLI commands"
    - "CLI tests use --fast flag to avoid loading full dataset"
    - "Tests verify exit_code == 0 and expected output files exist"
    - "Tests use CliRunner from typer.testing for clean invocation"
  artifacts:
    - path: "tests/test_cli_policy.py"
      provides: "End-to-end CLI tests for Policy commands"
      exports: ["test_policy_retail_theft", "test_policy_vehicle_crimes", "test_policy_composition", "test_policy_events"]
      min_lines: 120
    - path: "tests/test_cli_forecasting.py"
      provides: "End-to-end CLI tests for Forecasting commands"
      exports: ["test_forecasting_time_series", "test_forecasting_classification"]
      min_lines: 60
  key_links:
    - from: "tests/test_cli_policy.py"
      to: "analysis/cli/main.app"
      via: "CliRunner invocation"
      pattern: "runner.invoke\(app, \['policy'"
    - from: "tests/test_cli_forecasting.py"
      to: "analysis/cli/main.app"
      via: "CliRunner invocation"
      pattern: "runner.invoke\(app, \['forecasting'"
</key_links>
---

<objective>
Create end-to-end CLI tests for all Policy (4 commands) and Forecasting (2 commands) using typer.testing.CliRunner. These tests verify that CLI commands execute successfully, produce expected output, and use the --fast flag for quick execution.

Purpose: Ensure the remaining 6 CLI commands (Policy + Forecasting) work end-to-end, generate correct output files, and achieve comprehensive test coverage across all 13 CLI commands.

Output: tests/test_cli_policy.py with 8 tests and tests/test_cli_forecasting.py with 4 tests
</objective>

<execution_context>
@/Users/dustinober/.claude/get-shit-done/workflows/execute-plan.md
@/Users/dustinober/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07-visualization-testing/07-RESEARCH.md

# Policy and Forecasting CLI commands to test
@analysis/cli/policy.py  # retail-theft, vehicle-crimes, composition, events commands
@analysis/cli/forecasting.py  # time-series, classification commands
@analysis/cli/main.py  # Main typer app

# Pattern established in 07-03 and 07-04
@tests/test_cli_chief.py  # Follow same structure
@tests/test_cli_patrol.py  # Follow same structure
</context>

<tasks>

<task type="auto">
  <name>Create test_cli_policy.py with Policy command tests</name>
  <files>tests/test_cli_policy.py</files>
  <action>
Create tests/test_cli_policy.py following the same pattern as test_cli_chief.py and test_cli_patrol.py:

1. Import CliRunner from typer.testing
2. Import app from analysis.cli.main
3. Import Path from pathlib
4. Create runner = CliRunner() at module level
5. Create test class TestPolicyRetailTheft with tests:
   - test_policy_retail_theft_basic(): Invoke with --fast, assert exit_code==0, check output contains "Retail Theft"
   - test_policy_retail_theft_output_files(): Invoke with --fast and --version test, assert output files exist
6. Create test class TestPolicyVehicleCrimes with tests:
   - test_policy_vehicle_crimes_basic(): Invoke with --fast, assert exit_code==0
   - test_policy_vehicle_crimes_output_files(): Assert vehicle crime analysis files created
7. Create test class TestPolicyComposition with tests:
   - test_policy_composition_basic(): Invoke with --fast, assert exit_code==0
   - test_policy_composition_output_files(): Assert composition analysis files created
8. Create test class TestPolicyEvents with tests:
   - test_policy_events_basic(): Invoke with --fast, assert exit_code==0
   - test_policy_events_output_files(): Assert event impact analysis files created

DO NOT:
- Don't omit --fast flag (policy analyses are expensive)
- Don't test without --version test (clutters reports/ directory)
- Don't assert exact output content (check key phrases only)
  </action>
  <verify>
# Run the new tests
pytest tests/test_cli_policy.py -v --tb=short

# Should show all tests passing
# Expected: ~8 tests (2 per command)
  </verify>
  <done>
test_cli_policy.py exists with 8 tests covering retail-theft, vehicle-crimes, composition, and events commands; all tests use --fast flag and verify exit_code==0 and output file existence
  </done>
</task>

<task type="auto">
  <name>Create test_cli_forecasting.py with Forecasting command tests</name>
  <files>tests/test_cli_forecasting.py</files>
  <action>
Create tests/test_cli_forecasting.py following the same pattern:

1. Import CliRunner from typer.testing
2. Import app from analysis.cli.main
3. Import Path from pathlib
4. Create runner = CliRunner() at module level
5. Create test class TestForecastingTimeSeries with tests:
   - test_forecasting_time_series_basic(): Invoke with --fast, assert exit_code==0, check output contains "Time Series"
   - test_forecasting_time_series_output_files(): Invoke with --fast and --version test, assert forecast files created
6. Create test class TestForecastingClassification with tests:
   - test_forecasting_classification_basic(): Invoke with --fast, assert exit_code==0
   - test_forecasting_classification_output_files(): Assert classification model files created

NOTE: These tests may skip if prophet or sklearn not available (optional dependencies)

DO NOT:
- Don't omit --fast flag (forecasting is very expensive)
- Don't fail tests if prophet/sklearn not available (use pytest.skip)
- Don't test model accuracy (just verify execution and outputs)
  </action>
  <verify>
# Run the new tests
pytest tests/test_cli_forecasting.py -v --tb=short

# Should show all tests passing or skipped
# Expected: ~4 tests (2 per command)
  </verify>
  <done>
test_cli_forecasting.py exists with 4 tests covering time-series and classification commands; tests handle optional dependencies (prophet, sklearn) gracefully with pytest.skip
  </done>
</task>

<task type="auto">
  <name>Verify all Policy and Forecasting tests pass</name>
  <files>tests/test_cli_policy.py tests/test_cli_forecasting.py</files>
  <action>
Run the Policy and Forecasting CLI tests and verify:

1. All tests pass (exit_code == 0, output files exist)
2. Tests complete in reasonable time (< 90 seconds total)
3. Output files are created in correct locations (reports/test/policy/, reports/test/forecasting/)
4. Optional dependencies handled gracefully (prophet, sklearn)
5. All 13 CLI commands now have test coverage

If tests fail:
- Check that CLI commands exist in analysis/cli/policy.py and analysis/cli/forecasting.py
- Verify optional dependencies are handled
- Ensure --fast flag is properly passed
- Check that output directory structure is correct

DO NOT:
- Don't proceed if basic tests fail (debug first)
- Don't accept tests that skip all commands (optional deps should be mocked or tested conditionally)
  </action>
  <verify>
# Run tests with timing
time pytest tests/test_cli_policy.py tests/test_cli_forecasting.py -v

# Check combined CLI test coverage
pytest tests/test_cli_*.py --cov=analysis/cli --cov-report=term-missing
  </verify>
  <done>
All 12 Policy and Forecasting CLI tests pass; all 13 CLI commands have test coverage; combined CLI coverage is > 60%
  </done>
</task>

</tasks>

<verification>
Run these verification steps after all tasks complete:

1. Run all Policy and Forecasting CLI tests:
   ```bash
   pytest tests/test_cli_policy.py tests/test_cli_forecasting.py -v
   ```
   Expected: 12 tests passing (8 Policy + 4 Forecasting)

2. Run ALL CLI tests together:
   ```bash
   pytest tests/test_cli_*.py -v
   ```
   Expected: 26 tests passing (6 Chief + 8 Patrol + 8 Policy + 4 Forecasting)

3. Check test execution time:
   ```bash
   time pytest tests/test_cli_*.py -v
   ```
   Expected: < 180 seconds (3 minutes) for all 26 tests

4. Verify complete CLI coverage:
   ```bash
   pytest tests/test_cli_*.py --cov=analysis/cli --cov-report=term-missing
   ```
   Expected: > 60% coverage for all CLI modules combined

5. Verify all 13 commands have tests:
   ```bash
   grep -E "test_(chief|patrol|policy|forecasting)" tests/test_cli_*.py | grep "def test_" | wc -l
   ```
   Expected: 26 test functions (2 per command)
</verification>

<success_criteria>
1. tests/test_cli_policy.py exists with 8 tests (2 per command)
2. tests/test_cli_forecasting.py exists with 4 tests (2 per command)
3. All tests use CliRunner from typer.testing
4. All tests use --fast flag for quick execution
5. All tests verify exit_code == 0 and expected output
6. All 13 CLI commands have test coverage (26 tests total)
7. Combined CLI test execution time < 3 minutes
8. Coverage of all CLI modules > 60%
9. Optional dependencies (prophet, sklearn, geopandas) handled gracefully
</success_criteria>

<output>
After completion, create `.planning/phases/07-visualization-testing/07-05-SUMMARY.md` with:
- Performance metrics (duration, files created)
- Tests added (12 tests for 6 commands)
- All 13 CLI commands now have test coverage
- Coverage improvement across all CLI modules
- Execution time benchmarks
- Optional dependency handling
- Any deviations from plan
- Git commit hash
</output>
