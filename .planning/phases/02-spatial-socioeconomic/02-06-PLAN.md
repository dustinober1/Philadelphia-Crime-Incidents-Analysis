# Plan 02-06: Integration & Validation

**Phase:** 2 — Spatial & Socioeconomic Analysis
**Wave:** 3 (Integration)
**Requirement:** Phase validation and orchestration
**Depends on:** 02-01, 02-02, 02-03, 02-04, 02-05

## Goal

Cross-reference all Phase 2 outputs for consistency, validate all artifacts exist and are properly formatted, create a Phase 2 summary notebook, and update the orchestrator for Phase 2 automation.

## Context

This plan runs after all Wave 1 and Wave 2 plans complete. It validates:
- Infrastructure from 02-01 (boundaries, config, utilities)
- Hotspot outputs from 02-02 (centroids, heatmaps)
- Robbery temporal patterns from 02-03 (heatmap, recommendations)
- District severity from 02-04 (choropleth, ranking)
- Census tract rates from 02-05 (rates, flagged tracts)

## Tasks

### 1. Create Validation Script

**1.1 Create `scripts/validate_phase2.py`**
```python
"""Validate Phase 2 artifacts and cross-reference outputs."""

from pathlib import Path
import geopandas as gpd
import pandas as pd
import json
import sys

def validate_phase2(repo_root: Path) -> dict:
    """Validate all Phase 2 artifacts exist and are valid."""

    results = {
        'infrastructure': {},
        'hotspots': {},
        'robbery': {},
        'severity': {},
        'census': {},
        'summary': {'passed': 0, 'failed': 0, 'warnings': 0}
    }

    # === Infrastructure (02-01) ===
    boundaries_dir = repo_root / 'data' / 'boundaries'

    # Police districts
    police_path = boundaries_dir / 'police_districts.geojson'
    if police_path.exists():
        try:
            gdf = gpd.read_file(police_path)
            results['infrastructure']['police_districts'] = {
                'exists': True,
                'count': len(gdf),
                'valid': len(gdf) >= 20  # Expect ~25 districts
            }
            results['summary']['passed'] += 1
        except Exception as e:
            results['infrastructure']['police_districts'] = {'exists': True, 'error': str(e)}
            results['summary']['failed'] += 1
    else:
        results['infrastructure']['police_districts'] = {'exists': False}
        results['summary']['failed'] += 1

    # Census tracts
    census_path = boundaries_dir / 'census_tracts_pop.geojson'
    if census_path.exists():
        try:
            gdf = gpd.read_file(census_path)
            has_pop = 'total_pop' in gdf.columns or 'population' in gdf.columns
            results['infrastructure']['census_tracts'] = {
                'exists': True,
                'count': len(gdf),
                'has_population': has_pop
            }
            results['summary']['passed'] += 1
        except Exception as e:
            results['infrastructure']['census_tracts'] = {'exists': True, 'error': str(e)}
            results['summary']['failed'] += 1
    else:
        results['infrastructure']['census_tracts'] = {'exists': False}
        results['summary']['failed'] += 1

    # Config
    config_path = repo_root / 'config' / 'phase2_config.yaml'
    results['infrastructure']['config'] = {'exists': config_path.exists()}
    results['summary']['passed' if config_path.exists() else 'failed'] += 1

    # === Hotspots (02-02) ===
    reports_dir = repo_root / 'reports'

    hotspot_files = [
        ('hotspot_heatmap.png', 'Static heatmap'),
        ('hotspot_heatmap.html', 'Interactive heatmap'),
        ('hotspot_centroids.geojson', 'Cluster centroids')
    ]

    for filename, desc in hotspot_files:
        path = reports_dir / filename
        results['hotspots'][filename] = {
            'exists': path.exists(),
            'description': desc,
            'size_kb': path.stat().st_size / 1024 if path.exists() else 0
        }
        results['summary']['passed' if path.exists() else 'failed'] += 1

    # Validate centroids content
    centroid_path = reports_dir / 'hotspot_centroids.geojson'
    if centroid_path.exists():
        try:
            gdf = gpd.read_file(centroid_path)
            results['hotspots']['centroids_count'] = len(gdf)
        except:
            pass

    # === Robbery (02-03) ===
    robbery_files = [
        ('robbery_temporal_heatmap.png', 'Temporal heatmap'),
        ('robbery_patrol_recommendations.md', 'Patrol recommendations')
    ]

    for filename, desc in robbery_files:
        path = reports_dir / filename
        results['robbery'][filename] = {
            'exists': path.exists(),
            'description': desc
        }
        results['summary']['passed' if path.exists() else 'failed'] += 1

    # === Severity (02-04) ===
    severity_files = [
        ('district_severity_choropleth.png', 'Choropleth map'),
        ('district_severity_ranking.csv', 'Ranking table'),
        ('districts_scored.geojson', 'Scored districts')
    ]

    for filename, desc in severity_files:
        path = reports_dir / filename
        results['severity'][filename] = {
            'exists': path.exists(),
            'description': desc
        }
        results['summary']['passed' if path.exists() else 'failed'] += 1

    # Validate ranking content
    ranking_path = reports_dir / 'district_severity_ranking.csv'
    if ranking_path.exists():
        try:
            df = pd.read_csv(ranking_path)
            results['severity']['districts_ranked'] = len(df)
            results['severity']['has_severity_score'] = 'Severity Score' in df.columns
        except:
            pass

    # === Census (02-05) ===
    census_files = [
        ('tract_crime_rates.png', 'Rate choropleth'),
        ('tract_crime_rates.csv', 'Tract rates'),
        ('flagged_tracts_report.md', 'Flagged tracts')
    ]

    for filename, desc in census_files:
        path = reports_dir / filename
        results['census'][filename] = {
            'exists': path.exists(),
            'description': desc
        }
        results['summary']['passed' if path.exists() else 'failed'] += 1

    # Validate rates content
    rates_path = reports_dir / 'tract_crime_rates.csv'
    if rates_path.exists():
        try:
            df = pd.read_csv(rates_path)
            results['census']['tracts_with_rates'] = len(df)
            results['census']['has_crime_rate'] = 'crime_rate' in df.columns
        except:
            pass

    return results


if __name__ == '__main__':
    repo_root = Path(__file__).parent.parent
    results = validate_phase2(repo_root)

    print("\n" + "="*60)
    print("PHASE 2 VALIDATION RESULTS")
    print("="*60)

    for category, items in results.items():
        if category == 'summary':
            continue
        print(f"\n{category.upper()}")
        print("-" * 40)
        for key, value in items.items():
            if isinstance(value, dict):
                status = "✓" if value.get('exists', False) else "✗"
                print(f"  {status} {key}")
            else:
                print(f"    {key}: {value}")

    print("\n" + "="*60)
    summary = results['summary']
    print(f"SUMMARY: {summary['passed']} passed, {summary['failed']} failed")
    print("="*60)

    sys.exit(0 if summary['failed'] == 0 else 1)
```

### 2. Cross-Reference Outputs

**2.1 Create Cross-Reference Check Script**
```python
# In validate_phase2.py, add cross-reference function

def cross_reference_outputs(repo_root: Path) -> dict:
    """Cross-reference Phase 2 outputs for consistency."""

    issues = []
    reports_dir = repo_root / 'reports'

    # 1. District counts should match across outputs
    severity_gdf = gpd.read_file(reports_dir / 'districts_scored.geojson')
    police_gdf = gpd.read_file(repo_root / 'data' / 'boundaries' / 'police_districts.geojson')

    if len(severity_gdf) != len(police_gdf):
        issues.append(f"District count mismatch: severity has {len(severity_gdf)}, boundaries has {len(police_gdf)}")

    # 2. Census tract counts should be consistent
    tracts_gdf = gpd.read_file(repo_root / 'data' / 'boundaries' / 'census_tracts_pop.geojson')
    rates_df = pd.read_csv(reports_dir / 'tract_crime_rates.csv')

    if len(rates_df) > len(tracts_gdf):
        issues.append(f"More tracts with rates ({len(rates_df)}) than in boundaries ({len(tracts_gdf)})")

    # 3. Check hotspot centroids are within Philadelphia bounds
    centroids_gdf = gpd.read_file(reports_dir / 'hotspot_centroids.geojson')
    philly_bounds = police_gdf.unary_union.bounds  # (minx, miny, maxx, maxy)

    outside_centroids = 0
    for _, row in centroids_gdf.iterrows():
        if not (philly_bounds[0] <= row.geometry.x <= philly_bounds[2] and
                philly_bounds[1] <= row.geometry.y <= philly_bounds[3]):
            outside_centroids += 1

    if outside_centroids > 0:
        issues.append(f"{outside_centroids} hotspot centroids outside Philadelphia bounds")

    return {'issues': issues, 'passed': len(issues) == 0}
```

### 3. Create Phase 2 Summary Notebook

**3.1 Create `notebooks/phase2_summary.ipynb`**

Structure:
- Title: "Phase 2: Spatial & Socioeconomic Analysis Summary"
- Overview of all Phase 2 analyses
- Reproducibility cell
- Validation results display
- Key findings from each notebook
- Combined recommendations
- Next steps for Phase 3

**3.2 Summary Content**
```python
# Load all artifacts and summarize
print("="*60)
print("PHASE 2 SUMMARY: Spatial & Socioeconomic Analysis")
print("="*60)

# Hotspots summary
centroids = gpd.read_file(REPORTS_DIR / 'hotspot_centroids.geojson')
print(f"\n1. HOTSPOT ANALYSIS (PATROL-01)")
print(f"   - Identified {len(centroids)} crime hotspot clusters")
print(f"   - Outputs: PNG heatmap, interactive HTML, GeoJSON centroids")

# Robbery temporal summary
with open(REPORTS_DIR / 'robbery_patrol_recommendations.md') as f:
    rec_content = f.read()
print(f"\n2. ROBBERY TEMPORAL PATTERNS (PATROL-02)")
print("   - Hour × Weekday heatmap created")
print("   - Peak periods identified with patrol recommendations")

# District severity summary
ranking = pd.read_csv(REPORTS_DIR / 'district_severity_ranking.csv')
top_district = ranking.iloc[0]['District']
print(f"\n3. DISTRICT SEVERITY (PATROL-03)")
print(f"   - All 25 districts scored using composite methodology")
print(f"   - Highest severity district: {top_district}")

# Census tract summary
rates = pd.read_csv(REPORTS_DIR / 'tract_crime_rates.csv')
reliable_count = (rates['rate_reliable'] == True).sum() if 'rate_reliable' in rates.columns else len(rates)
print(f"\n4. CENSUS TRACT RATES (HYP-SOCIO)")
print(f"   - {len(rates)} tracts analyzed")
print(f"   - {reliable_count} tracts with reliable population data")
```

### 4. Update Orchestrator

**4.1 Create `analysis/orchestrate_phase2.py`**
```python
"""Phase 2 orchestration: Spatial & Socioeconomic Analysis."""

from pathlib import Path
import subprocess
import sys
from datetime import datetime

NOTEBOOKS = [
    'hotspot_clustering.ipynb',
    'robbery_temporal_heatmap.ipynb',
    'district_severity.ipynb',
    'census_tract_rates.ipynb',
    'phase2_summary.ipynb'
]


def run_notebook(notebook_name: str, repo_root: Path) -> bool:
    """Execute a notebook using jupyter nbconvert."""
    notebook_path = repo_root / 'notebooks' / notebook_name

    if not notebook_path.exists():
        print(f"  ✗ {notebook_name} not found")
        return False

    print(f"  Running {notebook_name}...")

    cmd = [
        'jupyter', 'nbconvert',
        '--to', 'notebook',
        '--execute',
        '--inplace',
        '--ExecutePreprocessor.timeout=600',
        str(notebook_path)
    ]

    result = subprocess.run(cmd, capture_output=True, text=True)

    if result.returncode == 0:
        print(f"  ✓ {notebook_name} completed")
        return True
    else:
        print(f"  ✗ {notebook_name} failed")
        print(result.stderr)
        return False


def validate_phase2(repo_root: Path) -> bool:
    """Run validation script."""
    validate_script = repo_root / 'scripts' / 'validate_phase2.py'

    result = subprocess.run([sys.executable, str(validate_script)],
                           capture_output=True, text=True)
    print(result.stdout)

    return result.returncode == 0


def orchestrate_phase2(repo_root: Path = None) -> dict:
    """Run all Phase 2 notebooks and validate outputs."""

    if repo_root is None:
        repo_root = Path(__file__).parent.parent

    results = {
        'started': datetime.now().isoformat(),
        'notebooks': {},
        'validation': None,
        'success': False
    }

    print("\n" + "="*60)
    print("PHASE 2 ORCHESTRATION")
    print("="*60)

    # Run notebooks
    print("\nExecuting notebooks...")
    all_passed = True
    for notebook in NOTEBOOKS:
        success = run_notebook(notebook, repo_root)
        results['notebooks'][notebook] = success
        if not success:
            all_passed = False

    # Validate
    print("\nValidating outputs...")
    results['validation'] = validate_phase2(repo_root)

    # Summary
    results['success'] = all_passed and results['validation']
    results['completed'] = datetime.now().isoformat()

    print("\n" + "="*60)
    print(f"PHASE 2 {'COMPLETE' if results['success'] else 'FAILED'}")
    print("="*60)

    return results


if __name__ == '__main__':
    repo_root = Path(__file__).parent.parent
    results = orchestrate_phase2(repo_root)
    sys.exit(0 if results['success'] else 1)
```

### 5. Create Artifact Manifest

**5.1 Create `reports/phase2_manifest.json`**
```python
import json
from pathlib import Path
from datetime import datetime

manifest = {
    'phase': 2,
    'name': 'Spatial & Socioeconomic Analysis',
    'created': datetime.now().isoformat(),
    'artifacts': {
        'infrastructure': [
            {'path': 'data/boundaries/police_districts.geojson', 'type': 'geojson'},
            {'path': 'data/boundaries/census_tracts_pop.geojson', 'type': 'geojson'},
            {'path': 'config/phase2_config.yaml', 'type': 'config'}
        ],
        'hotspots': [
            {'path': 'reports/hotspot_heatmap.png', 'type': 'image'},
            {'path': 'reports/hotspot_heatmap.html', 'type': 'html'},
            {'path': 'reports/hotspot_centroids.geojson', 'type': 'geojson'},
            {'path': 'data/processed/crimes_with_clusters.parquet', 'type': 'data'}
        ],
        'robbery': [
            {'path': 'reports/robbery_temporal_heatmap.png', 'type': 'image'},
            {'path': 'reports/robbery_patrol_recommendations.md', 'type': 'markdown'}
        ],
        'severity': [
            {'path': 'reports/district_severity_choropleth.png', 'type': 'image'},
            {'path': 'reports/district_severity_ranking.csv', 'type': 'csv'},
            {'path': 'reports/districts_scored.geojson', 'type': 'geojson'}
        ],
        'census': [
            {'path': 'reports/tract_crime_rates.png', 'type': 'image'},
            {'path': 'reports/tract_crime_rates.csv', 'type': 'csv'},
            {'path': 'reports/tracts_with_rates.geojson', 'type': 'geojson'},
            {'path': 'reports/flagged_tracts_report.md', 'type': 'markdown'},
            {'path': 'data/processed/tract_crime_rates.parquet', 'type': 'data'}
        ]
    }
}

with open(REPORTS_DIR / 'phase2_manifest.json', 'w') as f:
    json.dump(manifest, f, indent=2)
```

### 6. Update STATE.md

**6.1 Add Phase 2 Completion Entry**
After validation passes, update `.planning/STATE.md`:
- Current Position: Phase 2 complete
- New decisions from Phase 2 (severity weights, clustering parameters)
- Artifacts generated summary

### 7. Notebook Completion

**7.1 Artifact Summary Cell in Summary Notebook**
```python
print("\n" + "="*60)
print("PHASE 2 COMPLETE")
print("="*60)
print("\nAll requirements satisfied:")
print("  ✓ PATROL-01: Hotspot clustering")
print("  ✓ PATROL-02: Robbery temporal patterns")
print("  ✓ PATROL-03: District severity scoring")
print("  ✓ HYP-SOCIO: Census tract crime rates")
print(f"\nTotal artifacts: {sum(len(v) for v in manifest['artifacts'].values())}")
print(f"\nRuntime: {time.time() - RUNTIME_START:.1f} seconds")
```

## Validation Criteria

- [ ] `scripts/validate_phase2.py` executes without errors
- [ ] All Phase 2 artifacts exist and pass validation
- [ ] Cross-reference checks pass (district counts match, tracts consistent)
- [ ] `notebooks/phase2_summary.ipynb` runs end-to-end
- [ ] `analysis/orchestrate_phase2.py` can run all notebooks
- [ ] `reports/phase2_manifest.json` lists all artifacts
- [ ] No validation warnings or errors

## Dependencies

- All Wave 1 and Wave 2 plans must complete successfully
- geopandas, pandas (for validation)
- jupyter nbconvert (for orchestration)

## Estimated Time

- Validation script: 20 min
- Cross-reference logic: 15 min
- Summary notebook: 20 min
- Orchestrator: 15 min
- Manifest and cleanup: 10 min
- Testing: 15 min
- **Total: ~95 min**

---
*Plan created: 2026-02-03*
