---
wave: 3
depends_on: ["01-01", "01-02", "01-03", "01-04"]
files_modified:
  - analysis/orchestrate_phase1.py
  - analysis/validate_artifacts.py
  - README.md
  - run_phase1.sh
  - .planning/phases/01-high-level-trends-seasonality/01-05-VALIDATION.md
autonomous: true
---

# Plan 01-05: Integration & Testing

<objective>
Integrate all three refactored notebooks into the orchestration pipeline, verify end-to-end headless execution, validate all Phase 1 success criteria are met, and create comprehensive documentation.

This final wave ensures the complete Phase 1 pipeline works as a cohesive system and produces all required deliverables reliably.
</objective>

<tasks>

<task id="1" type="code">
<name>End-to-End Orchestration Testing</name>
<what>Verify the orchestrator can execute all three notebooks sequentially without manual intervention.</what>
<how>
1. Run full orchestration:
   ```bash
   python analysis/orchestrate_phase1.py --version v1.0
   ```
2. Verify execution order and logging:
   - Log shows: "Starting annual_trend...", "Completed in Xs"
   - Log shows: "Starting seasonality...", "Completed in Xs"
   - Log shows: "Starting covid...", "Completed in Xs"
   - Global summary: "Phase 1 complete: 3 notebooks, X artifacts, Y seconds"
3. Check all artifacts generated in `reports/`:
   - 6+ PNG files (2+ per notebook)
   - 3 markdown reports
   - 3 notebook manifests
   - 1 global phase manifest
4. Verify execution log saved to `reports/execution.log`
5. Test parameter injection for all notebooks
6. Test failure recovery with --continue-on-error flag:
   - Simulate failure in one notebook
   - Verify orchestrator continues with remaining notebooks
7. Test fast mode for quick CI runs:
   ```bash
   python analysis/orchestrate_phase1.py --version v1.0 --fast
   ```
</how>
<files>
- analysis/orchestrate_phase1.py
- reports/execution.log
- reports/phase1_manifest_v1.0.json
</files>
<verify>
```bash
python analysis/orchestrate_phase1.py --version v1.0 --fast && \
  ls reports/*.png | wc -l && \
  ls reports/*.md | wc -l && \
  echo "Orchestration test complete"
```
</verify>
</task>

<task id="2" type="code">
<name>Create Artifact Validation Script</name>
<what>Build validation script to verify all generated artifacts meet quality standards.</what>
<how>
1. Create `analysis/validate_artifacts.py` with checks:
   ```python
   def validate_png_dpi(filepath: Path) -> bool:
       """Check PNG is 300 DPI using PIL."""
       from PIL import Image
       img = Image.open(filepath)
       dpi = img.info.get('dpi', (0, 0))
       return dpi[0] >= 300

   def validate_report_sections(filepath: Path) -> bool:
       """Check report has required sections."""
       content = filepath.read_text()
       required = ['## Summary', '## Methods', '## Findings', '## Limitations']
       return all(section in content for section in required)

   def validate_manifest(filepath: Path) -> bool:
       """Check manifest has required fields and valid hashes."""
       import json
       manifest = json.loads(filepath.read_text())
       required_keys = ['version', 'timestamp', 'artifacts']
       return all(k in manifest for k in required_keys)

   def validate_hash(filepath: Path, expected_hash: str) -> bool:
       """Verify file hash matches manifest."""
       import hashlib
       actual = hashlib.sha256(filepath.read_bytes()).hexdigest()
       return actual == expected_hash
   ```
2. Run validation:
   ```python
   def main():
       reports_dir = Path('reports')

       # Validate PNGs
       for png in reports_dir.glob('*.png'):
           if validate_png_dpi(png):
               print(f"✅ {png.name}: 300 DPI")
           else:
               print(f"❌ {png.name}: Not 300 DPI")

       # Validate reports
       for md in reports_dir.glob('*_report_*.md'):
           if validate_report_sections(md):
               print(f"✅ {md.name}: All sections present")
           else:
               print(f"❌ {md.name}: Missing sections")

       # Validate manifests
       for manifest in reports_dir.glob('*_manifest_*.json'):
           if validate_manifest(manifest):
               print(f"✅ {manifest.name}: Valid manifest")
           else:
               print(f"❌ {manifest.name}: Invalid manifest")
   ```
3. Exit with non-zero code if any validation fails
</how>
<files>
- analysis/validate_artifacts.py
</files>
<verify>
```bash
python analysis/validate_artifacts.py && echo "All artifacts validated"
```
</verify>
</task>

<task id="3" type="code">
<name>Validate Phase 1 Success Criteria</name>
<what>Systematically verify all 4 Phase 1 success criteria from roadmap are met.</what>
<how>
1. Create validation checklist document:
   ```markdown
   # Phase 1 Validation Report

   ## Criterion 1: Annual Aggregation Notebook
   **Requirement:** Reproducible notebook that aggregates incidents annually (last 10 years) and outputs clean trend PNG and Markdown summary.

   - [ ] Notebook runs headless: `papermill ... && echo "PASS"`
   - [ ] PNG exists: `ls reports/annual_trend_v*.png`
   - [ ] Markdown exists: `ls reports/annual_trend_report_v*.md`
   - [ ] Covers 2015-2024 (10 years)
   - [ ] Shows Violent vs Property comparison

   **Status:** ✅ PASS / ❌ FAIL
   **Evidence:** [file paths, output snippets]

   ## Criterion 2: Seasonality Notebook
   **Requirement:** Monthly seasonality decomposition with month-level boxplots and numeric summary.

   - [ ] Notebook runs headless
   - [ ] Boxplot PNG exists: `ls reports/seasonality_boxplot_v*.png`
   - [ ] Report contains percentage: grep "%" report
   - [ ] July vs January comparison stated

   **Status:** ✅ PASS / ❌ FAIL

   ## Criterion 3: COVID Notebook
   **Requirement:** Pre/during/post COVID time series with lockdown annotation and displacement analysis.

   - [ ] Notebook runs headless
   - [ ] Timeline PNG exists with lockdown marker
   - [ ] Displacement chart exists
   - [ ] Residential vs Commercial comparison stated

   **Status:** ✅ PASS / ❌ FAIL

   ## Criterion 4: Headless Execution
   **Requirement:** All analyses run headless via nbconvert and generate artifacts in reports/.

   - [ ] Orchestrator completes all 3 notebooks
   - [ ] All artifacts in reports/ directory
   - [ ] Execution log present

   **Status:** ✅ PASS / ❌ FAIL
   ```
2. Run verification commands and capture output
3. Save to `.planning/phases/01-high-level-trends-seasonality/01-05-VALIDATION.md`
4. If any criterion fails, document remediation plan
</how>
<files>
- .planning/phases/01-high-level-trends-seasonality/01-05-VALIDATION.md
</files>
<verify>
```bash
cat .planning/phases/01-high-level-trends-seasonality/01-05-VALIDATION.md | grep -c "✅ PASS"
```
</verify>
</task>

<task id="4" type="code">
<name>Create Quick-Start Script</name>
<what>Provide turnkey script for first-time users to run Phase 1 with defaults.</what>
<how>
1. Create `run_phase1.sh`:
   ```bash
   #!/bin/bash
   set -e

   echo "============================================"
   echo "  Phase 1: High-Level Trends & Seasonality"
   echo "============================================"

   # Check prerequisites
   if [ ! -f "data/crime_incidents_combined.parquet" ]; then
       echo "ERROR: Crime data not found. Please download data first."
       exit 1
   fi

   if [ ! -f "config/phase1_config.yaml" ]; then
       echo "ERROR: Configuration not found. Run infrastructure setup first."
       exit 1
   fi

   # Parse arguments
   VERSION="${1:-v1.0}"
   FAST_FLAG=""
   if [ "$2" == "--fast" ]; then
       FAST_FLAG="--fast"
       echo "Running in FAST mode (10% sample)"
   fi

   # Run orchestrator
   echo ""
   echo "Starting Phase 1 execution..."
   start_time=$(date +%s)

   python analysis/orchestrate_phase1.py --version "$VERSION" $FAST_FLAG

   end_time=$(date +%s)
   runtime=$((end_time - start_time))

   # Summary
   echo ""
   echo "============================================"
   echo "  Phase 1 Complete!"
   echo "============================================"
   echo "Runtime: ${runtime} seconds"
   echo "Artifacts: $(ls reports/*.png 2>/dev/null | wc -l) PNGs"
   echo "Reports: $(ls reports/*_report_*.md 2>/dev/null | wc -l) Markdown files"
   echo ""
   echo "View results in: reports/"
   ```
2. Make executable: `chmod +x run_phase1.sh`
3. Add help option: `./run_phase1.sh --help`
4. Test on clean environment
</how>
<files>
- run_phase1.sh
</files>
<verify>
```bash
chmod +x run_phase1.sh && ./run_phase1.sh --help 2>/dev/null || ./run_phase1.sh v1.0 --fast
```
</verify>
</task>

<task id="5" type="code">
<name>Update Documentation</name>
<what>Create comprehensive documentation for Phase 1 execution and troubleshooting.</what>
<how>
1. Add to README.md:
   ```markdown
   ## Phase 1: High-Level Trends & Seasonality

   Phase 1 answers three key questions about Philadelphia crime:
   1. **Is Philadelphia getting safer?** (Annual trends analysis)
   2. **Is there a summer crime spike?** (Seasonality analysis)
   3. **How did COVID change the crime landscape?** (Pre/during/post comparison)

   ### Quick Start

   ```bash
   # Full run (takes ~3-5 minutes)
   ./run_phase1.sh

   # Fast mode for testing (takes ~30 seconds)
   ./run_phase1.sh v1.0 --fast
   ```

   ### Manual Execution

   ```bash
   # Run all notebooks
   python analysis/orchestrate_phase1.py --version v1.0

   # Run single notebook
   python analysis/orchestrate_phase1.py --notebook annual_trend

   # Use custom config
   python analysis/orchestrate_phase1.py --config-path config/custom.yaml
   ```

   ### Output Artifacts

   All outputs are saved to `reports/` with versioned filenames:

   | Artifact | Description |
   |----------|-------------|
   | `annual_trend_v*.png` | 10-year crime trend visualization |
   | `seasonality_boxplot_v*.png` | Monthly crime distribution boxplot |
   | `covid_timeline_v*.png` | COVID period comparison timeline |
   | `*_report_v*.md` | Academic-style analysis reports |
   | `*_manifest_v*.json` | Artifact metadata with hashes |

   ### Troubleshooting

   **Config not found:** Run `ls config/phase1_config.yaml` to verify
   **Papermill errors:** Check `reports/execution.log` for details
   **Missing data:** Ensure `data/crime_incidents_combined.parquet` exists
   ```
2. Add configuration documentation
3. Add artifact descriptions
</how>
<files>
- README.md
</files>
<verify>
```bash
grep -q "Phase 1" README.md && grep -q "Quick Start" README.md && echo "README updated"
```
</verify>
</task>

<task id="6" type="code">
<name>Create Phase Completion Summary</name>
<what>Generate summary documenting Phase 1 completion, artifacts, and key findings.</what>
<how>
1. After successful validation, create completion report:
   ```markdown
   # Phase 1 Completion Report

   **Completed:** [timestamp]
   **Version:** v1.0

   ## Summary

   Phase 1 successfully completed with all 4 success criteria met.

   ## Key Findings

   ### Annual Trends (CHIEF-01)
   - Peak crime year: 2017 with 182,543 incidents
   - 2024 shows X% decrease from peak
   - Trend: -2,500 crimes/year (statistically significant)

   ### Seasonality (CHIEF-02)
   - Summer spike confirmed: +18.3% vs winter
   - July vs January: +15.1% difference
   - P-value < 0.001 (highly significant)

   ### COVID Impact (CHIEF-03)
   - Overall crimes decreased X% during lockdown
   - Displacement effect observed:
     - Residential burglaries: -25.5%
     - Commercial burglaries: +75%

   ## Artifacts Generated

   | Type | Count | Location |
   |------|-------|----------|
   | PNGs | 6 | reports/*.png |
   | Reports | 3 | reports/*_report_*.md |
   | Manifests | 4 | reports/*_manifest_*.json |
   | Execution log | 1 | reports/execution.log |

   ## Next Steps

   Proceed to Phase 2: Spatial & Socioeconomic Analysis
   ```
2. Save to `.planning/phases/01-high-level-trends-seasonality/01-05-COMPLETION.md`
</how>
<files>
- .planning/phases/01-high-level-trends-seasonality/01-05-COMPLETION.md
</files>
<verify>
```bash
ls .planning/phases/01-high-level-trends-seasonality/01-05-COMPLETION.md && echo "Completion report created"
```
</verify>
</task>

</tasks>

<must_haves>
- [ ] Orchestrator executes all 3 notebooks successfully in sequence
- [ ] Execution log saved to `reports/execution.log`
- [ ] Global manifest `phase1_manifest_v1.0.json` created with all artifacts
- [ ] Artifact validation script passes all checks (DPI, sections, hashes)
- [ ] All 4 Phase 1 success criteria verified and documented
- [ ] Quick-start script `run_phase1.sh` works with defaults
- [ ] README.md has complete Phase 1 documentation with examples
- [ ] Validation report saved to planning directory
- [ ] Completion report summarizes findings and artifacts
</must_haves>

<deliverables>
- Updated analysis/orchestrate_phase1.py (with full execution support)
- analysis/validate_artifacts.py
- run_phase1.sh
- Updated README.md with Phase 1 section
- .planning/phases/01-high-level-trends-seasonality/01-05-VALIDATION.md
- .planning/phases/01-high-level-trends-seasonality/01-05-COMPLETION.md
- reports/execution.log
- reports/phase1_manifest_v1.0.json
</deliverables>
