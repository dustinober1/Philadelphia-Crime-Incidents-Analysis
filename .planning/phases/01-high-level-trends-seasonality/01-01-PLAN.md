---
wave: 1
depends_on: []
files_modified:
  - analysis/__init__.py
  - analysis/config.py
  - analysis/utils.py
  - analysis/config_loader.py
  - analysis/artifact_manager.py
  - analysis/report_utils.py
  - analysis/orchestrate_phase1.py
  - config/phase1_config.yaml
  - config/report_template.md.j2
  - requirements.txt
  - README.md
autonomous: true
---

# Plan 01-01: Infrastructure Setup

<objective>
Create the foundational infrastructure needed for Phase 1 analysis: the `analysis/` Python module with shared utilities, external YAML configuration system, artifact versioning, report templates, and the orchestration framework.

This wave establishes scaffolding that all three notebooks depend on. Without this infrastructure, notebooks cannot run because they import from `analysis.config` and `analysis.utils` which do not yet exist.
</objective>

<tasks>

<task id="1" type="code">
<name>Create Analysis Module Structure</name>
<what>Scaffold the `analysis/` Python package with shared utilities for data loading, crime classification, and temporal feature extraction.</what>
<how>
1. Create `analysis/__init__.py` to make it a package (expose key functions)
2. Create `analysis/config.py` with:
   - `CRIME_DATA_PATH`: Path to `data/crime_incidents_combined.parquet`
   - `COLORS`: Colorblind-safe palette dict (Violent, Property, Other)
   - `REPORTS_DIR`: Path to `reports/`
3. Create `analysis/utils.py` with functions matching notebook import signatures:
   - `load_data(clean: bool = True) -> pd.DataFrame`: Load parquet, convert categorical dispatch_date to datetime
   - `classify_crime_category(df: pd.DataFrame) -> pd.DataFrame`: Map UCR codes to Violent/Property/Other
   - `extract_temporal_features(df: pd.DataFrame) -> pd.DataFrame`: Add year, month, day_of_week columns
4. All functions must have type hints and NumPy-style docstrings
5. Use `pathlib.Path` for all file operations (no hardcoded absolute paths)
</how>
<files>
- analysis/__init__.py
- analysis/config.py
- analysis/utils.py
</files>
<verify>
```bash
cd /Users/dustinober/Projects/Crime\ Incidents\ Philadelphia
python -c "from analysis.config import CRIME_DATA_PATH, COLORS; print('Config OK:', CRIME_DATA_PATH.exists())"
python -c "from analysis.utils import load_data, classify_crime_category, extract_temporal_features; df = load_data(); print('Loaded', len(df), 'records')"
```
</verify>
</task>

<task id="2" type="code">
<name>Create External Configuration System</name>
<what>Build YAML-based configuration system with parameters for all three notebooks, supporting versioning and environment settings.</what>
<how>
1. Create `config/` directory
2. Create `config/phase1_config.yaml` with sections:
   ```yaml
   version: "v1.0"
   environment:
     output_dir: "reports"
     dpi: 300
     fast_sample_frac: 0.1
   
   annual_trend:
     params:
       start_year: 2015
       end_year: 2024
       min_complete_months: 12
     outputs:
       png: "annual_trend_{version}.png"
       report: "annual_trend_report_{version}.md"
   
   seasonality:
     params:
       summer_months: [6, 7, 8]
       winter_months: [1, 2, 3]
       significance_level: 0.05
     outputs:
       png: "seasonality_boxplot_{version}.png"
       report: "seasonality_report_{version}.md"
   
   covid:
     params:
       lockdown_date: "2020-03-01"
       before_years: [2018, 2019]
       during_years: [2020, 2021]
       after_start_year: 2023
     outputs:
       png: "covid_timeline_{version}.png"
       report: "covid_report_{version}.md"
   ```
3. Create `analysis/config_loader.py` with `Phase1Config` class:
   - `__init__(config_path: Path)`: Load and validate YAML
   - `get_notebook_params(notebook_name: str) -> dict`: Return params section
   - `get_output_path(notebook_name: str, artifact_type: str, version: str) -> Path`: Format path with version
   - Add basic validation: required keys present, valid date formats
</how>
<files>
- config/phase1_config.yaml
- analysis/config_loader.py
</files>
<verify>
```bash
python -c "from analysis.config_loader import Phase1Config; c = Phase1Config(); print('annual_trend params:', c.get_notebook_params('annual_trend'))"
```
</verify>
</task>

<task id="3" type="code">
<name>Implement Artifact Versioning System</name>
<what>Create artifact management utilities for versioned outputs and manifest generation with SHA256 hashes.</what>
<how>
1. Create `analysis/artifact_manager.py` with functions:
   - `get_versioned_path(template: str, version: str) -> Path`: Replace {version} placeholder
   - `compute_file_hash(filepath: Path) -> str`: Calculate SHA256 hash
   - `get_git_commit() -> Optional[str]`: Extract current git commit hash (fallback gracefully)
   - `create_version_manifest(version: str, artifacts: List[Path], params: dict, runtime_seconds: float) -> dict`: Generate manifest with timestamp, git hash, artifact hashes, parameters
   - `save_manifest(manifest: dict, output_path: Path)`: Write JSON manifest
2. Manifest schema:
   ```json
   {
     "version": "v1.0",
     "timestamp": "2026-02-02T10:30:00Z",
     "git_commit": "abc123...",
     "runtime_seconds": 45.2,
     "parameters": {...},
     "artifacts": [
       {"path": "reports/trend_v1.0.png", "sha256": "..."},
       ...
     ]
   }
   ```
</how>
<files>
- analysis/artifact_manager.py
</files>
<verify>
```bash
python -c "from analysis.artifact_manager import get_versioned_path, get_git_commit; print(get_versioned_path('trend_{version}.png', 'v1.0')); print('Git:', get_git_commit())"
```
</verify>
</task>

<task id="4" type="code">
<name>Create Report Template System</name>
<what>Build Jinja2-based markdown report template for consistent academic-style output format.</what>
<how>
1. Create `config/report_template.md.j2` with sections:
   ```markdown
   # {{ title }}
   *Generated: {{ timestamp }} | Version: {{ version }}*
   
   ## Summary
   {{ summary }}
   
   ## Methods
   {{ methods }}
   
   ## Data Quality Summary
   {{ data_quality_table }}
   
   ## Findings
   {{ findings }}
   
   ## Limitations
   {{ limitations }}
   
   ## Technical Details
   - Records analyzed: {{ n_records }}
   - Date range: {{ date_range }}
   - Git commit: {{ git_commit }}
   ```
2. Create `analysis/report_utils.py` with:
   - `generate_data_quality_summary(df: pd.DataFrame) -> dict`: Compute missing %, date range, record count, year distribution
   - `render_report_template(template_path: Path, context: dict) -> str`: Load Jinja2 template and render
   - `format_data_quality_table(summary: dict) -> str`: Format as markdown table
</how>
<files>
- config/report_template.md.j2
- analysis/report_utils.py
</files>
<verify>
```bash
python -c "from analysis.report_utils import generate_data_quality_summary; from analysis.utils import load_data; df = load_data(); print(generate_data_quality_summary(df))"
```
</verify>
</task>

<task id="5" type="code">
<name>Build Orchestration Script</name>
<what>Create Python orchestrator that executes all three notebooks headlessly using papermill with logging and error handling.</what>
<how>
1. Create `analysis/orchestrate_phase1.py` with:
   - CLI arguments: `--version`, `--config-path`, `--fast`, `--notebook`, `--continue-on-error`
   - Notebook execution via papermill with parameter injection
   - Logging to console and `reports/execution.log`
   - Error handling with descriptive messages
   - Execution time tracking per notebook
   - Global manifest generation at completion
2. Execution flow:
   ```python
   def run_notebook(notebook_path, output_path, params):
       start = time.time()
       pm.execute_notebook(notebook_path, output_path, parameters=params)
       return time.time() - start
   
   def main():
       for notebook in ['annual_trend', 'seasonality', 'covid']:
           logger.info(f"Starting {notebook}...")
           runtime = run_notebook(...)
           logger.info(f"Completed {notebook} in {runtime:.1f}s")
   ```
3. Create `reports/` directory if it doesn't exist
</how>
<files>
- analysis/orchestrate_phase1.py
</files>
<verify>
```bash
python analysis/orchestrate_phase1.py --help
```
</verify>
</task>

<task id="6" type="code">
<name>Update Project Dependencies and Documentation</name>
<what>Ensure dependencies are documented and add Phase 1 execution instructions to README.</what>
<how>
1. Verify papermill, pyyaml, jinja2 are in requirements.txt (add if missing with version pins)
2. Add to README.md:
   ```markdown
   ## Running Phase 1 Analyses
   
   ### Quick Start
   python analysis/orchestrate_phase1.py --version v1.0
   
   ### Options
   - `--fast`: Run with 10% sample for quick testing
   - `--notebook annual_trend`: Run single notebook
   - `--config-path custom.yaml`: Use custom configuration
   
   ### Outputs
   All artifacts are saved to `reports/` with versioned filenames.
   ```
</how>
<files>
- requirements.txt
- README.md
</files>
<verify>
```bash
grep -q "papermill" requirements.txt && echo "papermill found"
grep -q "Phase 1" README.md && echo "README updated"
```
</verify>
</task>

</tasks>

<must_haves>
- [ ] `analysis/` module imports without errors: `from analysis.config import CRIME_DATA_PATH`
- [ ] `analysis/utils.py` functions work: `load_data()` returns DataFrame with 3M+ records
- [ ] `config/phase1_config.yaml` exists and parses correctly
- [ ] `Phase1Config` class provides notebook parameters
- [ ] `artifact_manager.py` can compute SHA256 hashes and extract git commit
- [ ] `report_utils.py` can generate data quality summary
- [ ] `orchestrate_phase1.py` has working `--help` command
- [ ] README.md contains Phase 1 execution instructions
</must_haves>

<deliverables>
- analysis/__init__.py
- analysis/config.py
- analysis/utils.py
- analysis/config_loader.py
- analysis/artifact_manager.py
- analysis/report_utils.py
- analysis/orchestrate_phase1.py
- config/phase1_config.yaml
- config/report_template.md.j2
- Updated requirements.txt
- Updated README.md
</deliverables>
