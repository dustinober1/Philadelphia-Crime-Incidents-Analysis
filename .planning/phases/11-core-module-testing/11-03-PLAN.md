---
phase: 11-core-module-testing
plan: 03
type: execute
wave: 1
depends_on: []
files_modified:
  - tests/test_models_validation.py
autonomous: true
user_setup: []

must_haves:
  truths:
    - "tests/test_models_validation.py exists with comprehensive tests"
    - "All functions in models/validation.py have test coverage"
    - "Tests use synthetic regression data"
    - "Tests mock statsmodels for autocorrelation checks"
  artifacts:
    - path: "tests/test_models_validation.py"
      provides: "Unit tests for model validation utilities"
      min_lines: 350
      exports: ["TestTimeSeriesCVScore", "TestWalkForwardValidation", "TestComputeRegressionMetrics", "TestComputeForecastAccuracy", "TestCreateModelCard", "TestCheckResidualAutocorrelation", "TestValidateTemporalSplit"]
  key_links:
    - from: "tests/test_models_validation.py"
      to: "analysis/models/validation.py"
      via: "import and function call testing"
      pattern: "from analysis.models.validation import"
---

<objective>
Create comprehensive unit tests for models/validation.py covering time series cross-validation, walk-forward validation, regression metrics, forecast accuracy with MASE, model cards, residual autocorrelation, and temporal split validation.

**Purpose:** Ensure model validation utilities are tested with synthetic data. Test metric calculations and cross-validation logic.

**Output:** test_models_validation.py with 30-40 tests using synthetic regression data.
</objective>

<execution_context>
@/Users/dustinober/.claude/get-shit-done/workflows/execute-plan.md
@/Users/dustinober/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/phases/11-core-module-testing/11-RESEARCH.md
@.planning/ROADMAP.md
@.planning/STATE.md

# Lock honored decisions from Phase 10
- pytest-xdist: Use -nauto for local development
- Branch coverage: Set branch=true
- Testing priority: High-priority modules (data processing, validation, utilities) first

# Reference existing patterns
@tests/conftest.py
@tests/test_data_validation.py
@tests/test_temporal.py
</context>

<tasks>

<task type="auto">
  <name>Create test_models_validation.py with CV and regression metrics tests</name>
  <files>tests/test_models_validation.py</files>
  <action>
    Create tests/test_models_validation.py with the following test structure:

    **Class TestTimeSeriesCVScore:**
    - test_returns_dict_with_scores_statistics: Verify returns dict with 'scores', 'mean', 'std', 'min', 'max' keys
    - test_scores_array_length_matches_n_splits: Verify len(scores) == n_splits
    - test_mean_computed_correctly: Verify mean equals scores.mean()
    - test_std_computed_correctly: Verify std equals scores.std()
    - test_custom_n_splits_parameter: Verify n_splits parameter affects score count
    - test_custom_scoring_parameter: Verify scoring parameter passed through
    - test_default_scoring_neg_mean_absolute_error: Verify default scoring is 'neg_mean_absolute_error'

    **Class TestComputeRegressionMetrics:**
    - test_returns_dict_with_expected_metrics: Verify returns 'mae', 'rmse', 'r2', 'mape', 'bias' keys
    - test_mae_calculation_with_synthetic_data: Verify MAE = mean(|y_true - y_pred|)
    - test_rmse_calculation_with_synthetic_data: Verify RMSE = sqrt(mean((y_true - y_pred)^2))
    - test_r2_calculation_perfect_prediction: Verify R2 = 1.0 for perfect prediction
    - test_mape_calculation_with_synthetic_data: Verify MAPE = mean(|(y_true - y_pred) / y_true|) * 100
    - test_bias_calculation: Verify bias = mean(y_pred - y_true)
    - test_prefix_added_to_all_metrics: Verify all metric names have prefix when provided
    - test_empty_prefix_no_underscore: Verify prefix="" gives 'mae' not '_mae'
    - test_handles_nan_values: Verify NaN filtered from both y_true and y_pred

    Use sklearn.ensemble.RandomForestRegressor with n_estimators=5 for fast CV tests. Use synthetic data: y_true = [1,2,3], y_pred = [1.1, 2.1, 3.1].
  </action>
  <verify>pytest tests/test_models_validation.py::TestTimeSeriesCVScore tests/test_models_validation.py::TestComputeRegressionMetrics -v</verify>
  <done>15 tests pass for CV and regression metrics</done>
</task>

<task type="auto">
  <name>Add forecast accuracy and model card tests</name>
  <files>tests/test_models_validation.py</files>
  <action>
    Add the following test classes to test_models_validation.py:

    **Class TestComputeForecastAccuracy:**
    - test_includes_all_regression_metrics: Verify includes mae, rmse, r2, mape, bias from base metrics
    - test_mase_with_naive_forecast: Verify MASE = mae / naive_error when seasonality=None
    - test_mase_with_seasonal_naive_forecast: Verify MASE uses seasonal diff when seasonality provided
    - test_mase_infinite_when_naive_error_zero: Verify MASE = np.inf when naive_error is 0
    - test_naive_forecast_uses_diff: Verify naive_error = mean(|actual.diff()|)

    **Class TestCreateModelCard:**
    - test_returns_dict_with_expected_fields: Verify has 'model_name', 'model_type', 'n_features', 'features', 'train_performance', 'test_performance', 'limitations', 'created_at'
    - test_n_features_matches_feature_list_length: Verify n_features == len(features)
    - test_train_metrics_preserved: Verify train_metrics dict included unchanged
    - test_test_metrics_preserved: Verify test_metrics dict included unchanged
    - test_limitations_default_to_empty_list: Verify limitations=[] when None provided
    - test_created_at_is_isoformat_timestamp: Verify created_at is string in ISO format
    - test_custom_limitations_preserved: Verify custom limitations list included

    Use pd.Series with datetime index for forecast accuracy tests. Mock current time for model card tests or just verify format is ISO string.
  </action>
  <verify>pytest tests/test_models_validation.py::TestComputeForecastAccuracy tests/test_models_validation.py::TestCreateModelCard -v</verify>
  <done>12 tests pass for forecast accuracy and model cards</done>
</task>

<task type="auto">
  <name>Add autocorrelation and temporal split validation tests</name>
  <files>tests/test_models_validation.py</files>
  <action>
    Add the following test classes to test_models_validation.py:

    **Class TestCheckResidualAutocorrelation:**
    - test_returns_dict_with_diagnostics: Verify returns dict with expected keys
    - test_ljung_box_pvalues_array_exists: Verify 'ljung_box_pvalues' key exists and is array
    - test_significant_lags_count_exists: Verify 'significant_lags' key exists and is int
    - test_autocorrelation_detected_boolean_exists: Verify 'autocorrelation_detected' key exists
    - test_handles_statsmodels_import_error: Verify returns dict with 'error' key when statsmodels unavailable
    - test_handles_short_residual_series: Verify appropriate handling when len(residuals) < max_lag
    - test_custom_max_lag_parameter: Verify max_lag affects lag count

    **Class TestValidateTemporalSplit:**
    - test_returns_dict_with_validation_results: Verify has 'train_end', 'test_start', 'gap_days', 'valid_temporal_order', 'sufficient_gap', 'train_size', 'test_size', 'test_ratio'
    - test_valid_temporal_order_true_when_sorted: Verify valid_temporal_order=True when train_max < test_min
    - test_valid_temporal_order_false_when_overlap: Verify valid_temporal_order=False when dates overlap
    - test_gap_days_calculation: Verify gap_days = (test_min - train_max).days
    - test_sufficient_gap_true_when_gap_met: Verify sufficient_gap=True when gap >= min_gap_days
    - test_sufficient_gap_false_when_gap_insufficient: Verify sufficient_gap=False when gap < min_gap_days
    - test_train_size_test_size_counts: Verify train_size = len(train_dates), test_size = len(test_dates)
    - test_test_ratio_calculation: Verify test_ratio = test_size / (train_size + test_size)
    - test_custom_min_gap_days_parameter: Verify min_gap_days affects sufficient_gap result
    - test_handles_datetime_series: Verify works with pd.Series of datetime timestamps

    For autocorrelation tests, mock statsmodels.stats.diagnostic.acorr_ljungbox to return predictable results. Use pd.Series with np.random.randn() for synthetic residuals.
  </action>
  <verify>pytest tests/test_models_validation.py::TestCheckResidualAutocorrelation tests/test_models_validation.py::TestValidateTemporalSplit -v</verify>
  <done>15 tests pass for autocorrelation and temporal split validation</done>
</task>

<task type="auto">
  <name>Add walk-forward validation tests and coverage check</name>
  <files>tests/test_models_validation.py</files>
  <action>
    Add the following test class to test_models_validation.py:

    **Class TestWalkForwardValidation:**
    - test_returns_dataframe_with_results: Verify returns pd.DataFrame
    - test_dataframe_has_expected_columns: Verify has 'train_end_idx', 'test_start_idx', 'n_train', 'n_test', 'error' columns
    - test_initial_train_size_parameter: Verify first iteration uses correct initial_train_size
    - test_step_size_parameter: Verify step_size affects iteration increments
    - test_custom_metric_function: Verify metric_fn parameter used for error calculation
    - test_stops_before_end_of_data: Verify loop terminates correctly
    - test_handles_empty_test_set_break: Verify breaks when X_test is empty
    - test_predictions_and_actuals_stored: Verify 'predictions' and 'actuals' columns contain arrays

    Use sklearn.linear_model.LinearRegression or dummy model for fast walk-forward tests. Create synthetic X, y with pd.date_range index.

    Then run coverage report:

    ```bash
    pytest tests/test_models_validation.py --cov=analysis.models.validation --cov-report=term-missing --cov-report=html
    ```

    Target: 80%+ coverage for models/validation.py. Add tests for any missing branches.
  </action>
  <verify>pytest tests/test_models_validation.py --cov=analysis.models.validation --cov-report=term-missing</verify>
  <done>8 walk-forward tests pass; coverage shows 80%+ for models/validation.py</done>
</task>

</tasks>

<verification>
- All 30-40 tests in test_models_validation.py pass with pytest -nauto
- Coverage report shows 80%+ coverage for models/validation.py
- No tests require real datasets or slow model training
- All tests follow behavior-focused pattern from TESTING_QUALITY_CRITERIA.md
- Tests use synthetic regression data per RESEARCH.md patterns
</verification>

<success_criteria>
- test_models_validation.py created with 30-40 tests
- Tests cover all functions in models/validation.py
- 80%+ coverage achieved for models/validation.py
- All tests pass with pytest -nauto in under 30 seconds
- Tests mock statsmodels when testing autocorrelation (avoid slow dependency)
</success_criteria>

<output>
After completion, create `.planning/phases/11-core-module-testing/11-03-SUMMARY.md`
</output>
