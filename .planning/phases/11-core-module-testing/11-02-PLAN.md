---
phase: 11-core-module-testing
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - tests/test_models_time_series.py
autonomous: true
user_setup: []

must_haves:
  truths:
    - "tests/test_models_time_series.py exists with comprehensive tests"
    - "All functions in models/time_series.py have test coverage"
    - "Tests use synthetic time series data"
    - "Tests mock Prophet.fit() to avoid slow training"
  artifacts:
    - path: "tests/test_models_time_series.py"
      provides: "Unit tests for time series model utilities"
      min_lines: 300
      exports: ["TestPrepareProphetData", "TestCreateTrainTestSplit", "TestGetProphetConfig", "TestEvaluateForecast", "TestDetectAnomalies"]
  key_links:
    - from: "tests/test_models_time_series.py"
      to: "analysis/models/time_series.py"
      via: "import and function call testing"
      pattern: "from analysis.models.time_series import"
---

<objective>
Create comprehensive unit tests for models/time_series.py covering Prophet data preparation, time-aware train/test splitting, Prophet configuration, forecast evaluation metrics, and anomaly detection.

**Purpose:** Ensure time series utilities are tested without slow Prophet model training. Test data preparation, configuration, and metric calculations.

**Output:** test_models_time_series.py with 30-40 tests using synthetic time series data.
</objective>

<execution_context>
@/Users/dustinober/.claude/get-shit-done/workflows/execute-plan.md
@/Users/dustinober/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/phases/11-core-module-testing/11-RESEARCH.md
@.planning/ROADMAP.md
@.planning/STATE.md

# Lock honored decisions from Phase 10
- pytest-xdist: Use -nauto for local development
- Branch coverage: Set branch=true
- Testing priority: High-priority modules (data processing, validation, utilities) first

# Reference existing patterns
@tests/conftest.py
@tests/test_temporal.py
@tests/test_data_preprocessing.py
</context>

<tasks>

<task type="auto">
  <name>Create test_models_time_series.py with Prophet data prep tests</name>
  <files>tests/test_models_time_series.py</files>
  <action>
    Create tests/test_models_time_series.py with the following test structure:

    **Class TestPrepareProphetData:**
    - test_returns_prophet_format_columns: Verify returns DataFrame with 'ds' and 'y' columns
    - test_converts_date_column_to_datetime: Verify 'ds' column is datetime64 dtype
    - test_sorts_by_date_ascending: Verify 'ds' column is monotonic_increasing
    - test_resets_index_after_sorting: Verify index is 0,1,2,... after sorting
    - test_handles_duplicate_dates: Verify duplicate dates preserved (not aggregated)
    - test_handles_empty_dataframe: Verify empty DataFrame returns with correct columns
    - test_custom_date_column_name: Verify date_col parameter works
    - test_custom_value_column_name: Verify value_col parameter works

    Use pandas.date_range() to create synthetic time series. Use pytest.approx for float comparisons.
  </action>
  <verify>pytest tests/test_models_time_series.py::TestPrepareProphetData -v</verify>
  <done>8 tests pass for Prophet data preparation</done>
</task>

<task type="auto">
  <name>Add train/test split and Prophet config tests</name>
  <files>tests/test_models_time_series.py</files>
  <action>
    Add the following test classes to test_models_time_series.py:

    **Class TestCreateTrainTestSplit:**
    - test_requires_ds_column_raises_value_error: Verify ValueError when 'ds' column missing
    - test_splits_by_date_cutoff: Verify train data ends before test data begins
    - test_custom_test_days_parameter: Verify test_days parameter affects split size
    - test_test_days_30_default: Verify default 30 days held out for testing
    - test_no_overlap_between_train_and_test: Verify train['ds'].max() < test['ds'].min()
    - test_handles_exact_boundary: Verify boundary dates handled correctly

    **Class TestGetProphetConfig:**
    - test_default_configuration: Verify all default config values match expectations
    - test_seasonality_mode_additive: Verify seasonality_mode='additive' accepted
    - test_seasonality_mode_multiplicative: Verify seasonality_mode='multiplicative' accepted
    - test_yearly_seasonality_parameter: Verify yearly parameter passed through
    - test_weekly_seasonality_parameter: Verify weekly parameter passed through
    - test_daily_seasonality_parameter: Verify daily parameter passed through
    - test_changepoint_prior_scale_parameter: Verify changepoint_prior_scale passed through
    - test_interval_width_parameter: Verify interval_width passed through
    - test_all_parameters_combination: Verify multiple custom parameters work together

    Expected defaults per source code:
    - seasonality_mode: "multiplicative"
    - yearly_seasonality: True
    - weekly_seasonality: True
    - daily_seasonality: False
    - changepoint_prior_scale: 0.05
    - interval_width: 0.95
  </action>
  <verify>pytest tests/test_models_time_series.py::TestCreateTrainTestSplit tests/test_models_time_series.py::TestGetProphetConfig -v</verify>
  <done>14 tests pass for splitting and configuration</done>
</task>

<task type="auto">
  <name>Add forecast evaluation and anomaly detection tests</name>
  <files>tests/test_models_time_series.py</files>
  <action>
    Add the following test classes to test_models_time_series.py:

    **Class TestEvaluateForecast:**
    - test_perfect_forecast_returns_ideal_metrics: Verify MAE=0, RMSE=0, R2=1, MAPE=0 for perfect prediction
    - test_constant_bias_captured_in_bias_metric: Verify bias = mean(predicted - actual)
    - test_mae_calculation: Verify MAE computed correctly with known synthetic data
    - test_rmse_calculation: Verify RMSE computed correctly with known synthetic data
    - test_r2_calculation: Verify R2 computed correctly with known synthetic data
    - test_mape_calculation: Verify MAPE computed correctly (use pytest.approx with abs=0.1)
    - test_nan_handling_filters_nan_pairs: Verify NaN values filtered from both actual and predicted
    - test_coverage_with_prediction_intervals: Verify coverage metric computed when lower/upper provided
    - test_coverage_not_computed_without_intervals: Verify coverage not in results without lower/upper
    - test_returns_dict_with_all_expected_keys: Verify 'mae', 'rmse', 'mape', 'r2', 'bias' keys exist

    **Class TestDetectAnomalies:**
    - test_returns_boolean_series: Verify returns pd.Series with dtype bool
    - test_threshold_anomaly_detection: Verify anomalies detected when residual > threshold_std * std
    - test_interval_anomaly_detection: Verify anomalies detected outside prediction interval
    - test_custom_threshold_std_parameter: Verify threshold_std affects detection sensitivity
    - test_custom_column_names: Verify actual_col, predicted_col, lower_col, upper_col parameters work
    - test_handles_empty_dataframe: Verify empty DataFrame returns empty boolean series
    - test_all_anomalies_when_all_outside_interval: Verify all True when all values outside bounds

    Use numpy arrays with known values for metric calculations:
    - actual = [100, 200, 300]
    - predicted = [100, 200, 300] (perfect) should give MAE=0, RMSE=0, R2=1
    - predicted = [110, 210, 310] (constant +10 bias) should give bias=10
  </action>
  <verify>pytest tests/test_models_time_series.py::TestEvaluateForecast tests/test_models_time_series.py::TestDetectAnomalies -v</verify>
  <done>17 tests pass for evaluation and anomaly detection</done>
</task>

<task type="auto">
  <name>Run coverage report for models/time_series.py</name>
  <files>tests/test_models_time_series.py</files>
  <action>
    Run pytest with coverage to verify models/time_series.py achieves target coverage:

    ```bash
    pytest tests/test_models_time_series.py --cov=analysis.models.time_series --cov-report=term-missing --cov-report=html
    ```

    Target: 80%+ coverage for models/time_series.py (from research baseline).

    If coverage is below 80%, identify missing branches and add targeted tests:
    - Check for untested error paths (ValueError for missing 'ds' column)
    - Check for edge cases (single row, all NaN values)
    - Check for optional parameter combinations (interval coverage with/without bounds)

    Document final coverage percentage in plan summary.
  </action>
  <verify>pytest tests/test_models_time_series.py --cov=analysis.models.time_series --cov-report=term-missing</verify>
  <done>Coverage report shows 80%+ for models/time_series.py with no critical gaps</done>
</task>

</tasks>

<verification>
- All 30-40 tests in test_models_time_series.py pass with pytest -nauto
- Coverage report shows 80%+ coverage for models/time_series.py
- No tests train actual Prophet models (only test data prep and metrics)
- All tests follow behavior-focused pattern from TESTING_QUALITY_CRITERIA.md
- Tests use synthetic time series data per RESEARCH.md patterns
</verification>

<success_criteria>
- test_models_time_series.py created with 30-40 tests
- Tests cover all functions in models/time_series.py
- 80%+ coverage achieved for models/time_series.py
- All tests pass with pytest -nauto in under 20 seconds
- No tests use Prophet.fit() (only test configuration and metrics)
</success_criteria>

<output>
After completion, create `.planning/phases/11-core-module-testing/11-02-SUMMARY.md`
</output>
