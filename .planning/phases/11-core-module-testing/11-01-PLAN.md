---
phase: 11-core-module-testing
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - tests/test_models_classification.py
autonomous: true
user_setup: []

must_haves:
  truths:
    - "tests/test_models_classification.py exists with comprehensive tests"
    - "All functions in models/classification.py have test coverage"
    - "Tests use synthetic data (no real model training)"
    - "Tests focus on workflow, not model accuracy"
  artifacts:
    - path: "tests/test_models_classification.py"
      provides: "Unit tests for classification model utilities"
      min_lines: 400
      exports: ["TestCreateTimeAwareSplit", "TestGetTimeSeriesCV", "TestTrainRandomForest", "TestTrainXGBoost", "TestExtractFeatureImportance", "TestEvaluateClassifier", "TestHandleClassImbalance"]
  key_links:
    - from: "tests/test_models_classification.py"
      to: "analysis/models/classification.py"
      via: "import and function call testing"
      pattern: "from analysis.models.classification import"
---

<objective>
Create comprehensive unit tests for models/classification.py covering time-aware splitting, model training workflows, feature importance extraction, classifier evaluation, and class imbalance handling.

**Purpose:** Ensure classification model utilities are tested without running slow model training. Test data flow, preprocessing, and evaluation logic.

**Output:** test_models_classification.py with 40-50 tests using synthetic data and mock models.
</objective>

<execution_context>
@/Users/dustinober/.claude/get-shit-done/workflows/execute-plan.md
@/Users/dustinober/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/phases/11-core-module-testing/11-RESEARCH.md
@.planning/ROADMAP.md
@.planning/STATE.md

# Lock honored decisions from Phase 10
- pytest-xdist: Use -nauto for local development
- Branch coverage: Set branch=true
- Testing priority: High-priority modules (data processing, validation, utilities) first

# Reference existing patterns
@tests/conftest.py
@tests/test_classification.py
@tests/test_temporal.py
</context>

<tasks>

<task type="auto">
  <name>Create test_models_classification.py with time-aware split tests</name>
  <files>tests/test_models_classification.py</files>
  <action>
    Create tests/test_models_classification.py with the following test structure:

    **Class TestCreateTimeAwareSplit:**
    - test_returns_four_split_dataframes: Verify (X_train, X_test, y_train, y_test) tuple returned
    - test_maintains_temporal_ordering: Verify train data ends before test data begins
    - test_ensure_sorted_default_true: Verify data sorted by index when ensure_sorted=True
    - test_ensure_sorted_false_skips_sorting: Verify sorting skipped when ensure_sorted=False
    - test_custom_test_size: Verify test_size parameter affects split ratio
    - test_handles_duplicate_index_stably: Verify stable sort (mergesort) for duplicate timestamps
    - test_x_y_length_mismatch_raises_value_error: Verify ValueError when X and y lengths differ after sorting
    - test_empty_dataframe_raises: Verify appropriate handling for empty input

    Use pandas DataFrames with datetime index for testing. Use sample_crime_df fixture pattern from conftest.py as inspiration.
  </action>
  <verify>pytest tests/test_models_classification.py::TestCreateTimeAwareSplit -v</verify>
  <done>8 tests pass for time-aware split functionality</done>
</task>

<task type="auto">
  <name>Add model training and feature importance tests</name>
  <files>tests/test_models_classification.py</files>
  <action>
    Add the following test classes to test_models_classification.py:

    **Class TestGetTimeSeriesCV:**
    - test_returns_time_series_split: Verify returns sklearn.model_selection.TimeSeriesSplit
    - test_custom_n_splits: Verify n_splits parameter passed through
    - test_custom_max_train_size: Verify max_train_size parameter passed through
    - test_default_parameters: Verify defaults (n_splits=5, max_train_size=None)

    **Class TestTrainRandomForest:**
    - test_returns_fitted_model_and_scaler: Verify returns (model, scaler) tuple
    - test_scale_features_false_returns_none_scaler: Verify scaler=None when scale_features=False
    - test_model_has_expected_attributes: Verify model has feature_importances_ attribute
    - test_scaler_fitted_correctly: Verify scaler.n_features_in_ matches X_train columns
    - test_custom_hyperparameters: Verify n_estimators, max_depth, etc. passed through
    - test_random_state_reproducible: Verify same random_state produces same results (not accuracy)

    **Class TestExtractFeatureImportance:**
    - test_returns_dataframe_with_feature_importance: Verify returns DataFrame with correct columns
    - test_sorted_by_importance_descending: Verify importance column is monotonic decreasing
    - test_top_n_parameter: Verify top_n filters to N features
    - test_top_n_none_returns_all: Verify top_n=None returns all features

    Use sklearn.ensemble.RandomForestClassifier with n_estimators=10, random_state=42 for fast tests. Never assert on accuracy metrics.
  </action>
  <verify>pytest tests/test_models_classification.py::TestGetTimeSeriesCV tests/test_models_classification.py::TestTrainRandomForest tests/test_models_classification.py::TestExtractFeatureImportance -v</verify>
  <done>15 tests pass for model training and feature importance</done>
</task>

<task type="auto">
  <name>Add classifier evaluation and class imbalance tests</name>
  <files>tests/test_models_classification.py</files>
  <action>
    Add the following test classes to test_models_classification.py:

    **Class TestEvaluateClassifier:**
    - test_returns_confusion_matrix: Verify confusion_matrix key exists and is numpy array
    - test_returns_classification_report: Verify classification_report key contains metrics
    - test_with_probabilities_adds_roc_auc: Verify roc_auc added when y_prob provided
    - test_without_probabilities_no_roc_auc: Verify roc_auc not in results when y_prob=None
    - test_target_names_in_report: Verify target_names passed to classification_report
    - test_single_class_roc_auc_handles_gracefully: Verify roc_auc=None for single class edge case

    **Class TestHandleClassImbalance:**
    - test_returns_dict_mapping_classes_to_weights: Verify returns dict[int, float]
    - test_balanced_weights_for_imbalanced_data: Create synthetic imbalanced classes, verify weights computed
    - test_all_classes_present_in_keys: Verify all unique y_train values are keys
    - test_weights_inversely_proportional_to_frequency: Verify rare classes get higher weights
    - test_handles_binary_classification: Verify works with 2 classes
    - test_handles_multiclass_classification: Verify works with 3+ classes

    **Class TestTrainXGBoost (if xgboost available):**
    - test_returns_fitted_model_and_scaler: Verify returns (model, scaler) tuple
    - test_scale_features_defaults_to_false: Verify XGBoost typically doesn't need scaling
    - test_custom_hyperparameters: Verify n_estimators, max_depth, learning_rate passed through
    - test_eval_metric_set_to_logloss: Verify eval_metric configured

    Import xgboost conditionally (try/except ImportError) and skip tests if not available. Use synthetic binary classification data (y=[0,1,0,1,...]) for tests.
  </action>
  <verify>pytest tests/test_models_classification.py::TestEvaluateClassifier tests/test_models_classification.py::TestHandleClassImbalance tests/test_models_classification.py::TestTrainXGBoost -v</verify>
  <done>18 tests pass for evaluation and class imbalance handling</done>
</task>

<task type="auto">
  <name>Run coverage report for models/classification.py</name>
  <files>tests/test_models_classification.py</files>
  <action>
    Run pytest with coverage to verify models/classification.py achieves target coverage:

    ```bash
    pytest tests/test_models_classification.py --cov=analysis.models.classification --cov-report=term-missing --cov-report=html
    ```

    Target: 80%+ coverage for models/classification.py (from research baseline).

    If coverage is below 80%, identify missing branches and add targeted tests:
    - Check for untested error paths (ValueError, TypeError)
    - Check for edge cases (empty DataFrames, single row, all same values)
    - Check for optional parameter combinations not tested

    Document final coverage percentage in plan summary.
  </action>
  <verify>pytest tests/test_models_classification.py --cov=analysis.models.classification --cov-report=term-missing</verify>
  <done>Coverage report shows 80%+ for models/classification.py with no critical gaps</done>
</task>

</tasks>

<verification>
- All 40-50 tests in test_models_classification.py pass with pytest -nauto
- Coverage report shows 80%+ coverage for models/classification.py
- No tests use real data files or slow model training (n_estimators > 100)
- All tests follow behavior-focused pattern from TESTING_QUALITY_CRITERIA.md
- Tests use synthetic data and mock models per RESEARCH.md patterns
</verification>

<success_criteria>
- test_models_classification.py created with 40-50 tests
- Tests cover all functions in models/classification.py
- 80%+ coverage achieved for models/classification.py
- All tests pass with pytest -nauto in under 30 seconds
- No tests assert model accuracy (only workflow and data flow)
</success_criteria>

<output>
After completion, create `.planning/phases/11-core-module-testing/11-01-SUMMARY.md`
</output>
