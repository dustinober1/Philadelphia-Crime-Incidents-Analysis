---
phase: 03-visualization-reporting
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - notebooks/08_dashboard_precomputation.ipynb
  - output/dashboard/monthly_counts.parquet
  - output/dashboard/yearly_counts.parquet
  - output/dashboard/district_counts.parquet
  - output/dashboard/offense_counts.parquet
  - output/dashboard/cross_factor_counts.parquet
  - output/dashboard/sample_50k.parquet
autonomous: true
user_setup: []

must_haves:
  truths:
    - "Clean dataset loaded from Phase 2 (~3.5M records)"
    - "All aggregations computed and saved for dashboard performance"
    - "Sample of 50k records created for scatter plots"
    - "All files saved in output/dashboard/ with correct schema"
  artifacts:
    - path: "notebooks/08_dashboard_precomputation.ipynb"
      provides: "Data preparation and aggregation notebook"
      min_lines: 80
    - path: "output/dashboard/monthly_counts.parquet"
      provides: "Time-series aggregation (month/year/count)"
      contains: "year, month, count columns"
    - path: "output/dashboard/district_counts.parquet"
      provides: "Geographic aggregation (district/count)"
      contains: "district, count columns"
    - path: "output/dashboard/offense_counts.parquet"
      provides: "Offense type aggregation (offense_category/count)"
      contains: "offense_category, count columns"
    - path: "output/dashboard/sample_50k.parquet"
      provides: "Sampled data for scatter plots"
      min_rows: 50000
  key_links:
    - from: "08_dashboard_precomputation.ipynb"
      to: "data/processed/crime_incidents_cleaned.parquet"
      via: "pd.read_parquet()"
      pattern: "read_parquet.*cleaned"
    - from: "08_dashboard_precomputation.ipynb"
      to: "output/dashboard/*.parquet"
      via: "df.groupby().to_parquet()"
      pattern: "to_parquet.*dashboard"
---

<objective>
Prepare and pre-compute data aggregations for dashboard performance optimization

Purpose: Dashboard will query 3.5M records. Pre-computing aggregations reduces runtime from minutes to seconds.
Output: Aggregated datasets saved as parquet files for dashboard consumption
</objective>

<execution_context>
@~/.config/opencode/get-shit-done/workflows/execute-plan.md
@~/.config/opencode/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-visualization-reporting/03-CONTEXT.md
@.planning/phases/03-visualization-reporting/03-RESEARCH.md
@scripts/config.py
@data/processed/crime_incidents_cleaned.parquet
</context>

<tasks>

<task type="auto">
  <name>Task 1: Load cleaned dataset and create aggregations</name>
  <files>notebooks/08_dashboard_precomputation.ipynb, output/dashboard/*.parquet</files>
  <action>
Create notebook `notebooks/08_dashboard_precomputation.ipynb` with:

1. **Imports and config** (2 cells):
   - Import pandas, numpy, plotly.express
   - Import config.py from scripts/
   - Create output/dashboard/ directory if not exists

2. **Load cleaned data** (1 cell):
   - Read `data/processed/crime_incidents_cleaned.parquet`
   - Print shape: f"Loaded {len(df)} records"
   - Print columns: df.columns.tolist()

3. **Time-series aggregations** (3 cells):
   - Monthly aggregation: `df.groupby(['year', 'month']).size().reset_index(name='count')`
   - Yearly aggregation: `df.groupby('year').size().reset_index(name='count')`
   - Save both to output/dashboard/monthly_counts.parquet and yearly_counts.parquet
   - Print each aggregation shape

4. **Geographic aggregation** (2 cells):
   - District aggregation: `df.groupby('district').size().reset_index(name='count')`
   - Save to output/dashboard/district_counts.parquet
   - Print number of districts

5. **Offense type aggregation** (2 cells):
   - UCR category aggregation: `df.groupby('ucr_general').size().reset_index(name='count')`
   - Offense category aggregation (if exists): `df.groupby('offense_category').size().reset_index(name='count')`
   - Save to output/dashboard/offense_counts.parquet
   - Print distribution

6. **Cross-factor aggregations** (3 cells):
   - Monthly by offense category: `df.groupby(['year', 'month', 'ucr_general']).size().reset_index(name='count')`
   - District by offense category: `df.groupby(['district', 'ucr_general']).size().reset_index(name='count')`
   - Save to output/dashboard/cross_factor_counts.parquet
   - Print shapes

7. **Sample for scatter plots** (2 cells):
   - Sample 50k records: `df.sample(n=50000, random_state=42)`
   - Save to output/dashboard/sample_50k.parquet
   - Verify: f"Sampled {len(sample)} records from original {len(df)}"

All aggregations saved as parquet for fast loading in dashboard.
</action>
  <verify>
Run all cells in notebook without errors. Verify parquet files exist:
- `ls -la output/dashboard/*.parquet` shows 6 files
- `pd.read_parquet('output/dashboard/sample_50k.parquet')` has exactly 50,000 rows
- All aggregations have expected columns (year, month, count; district, count; etc.)
</verify>
  <done>
Notebook runs to completion with all 6 parquet files saved:
- monthly_counts.parquet, yearly_counts.parquet
- district_counts.parquet, offense_counts.parquet
- cross_factor_counts.parquet, sample_50k.parquet
Each file verified for correct schema and non-zero records.
</done>
</task>

</tasks>

<verification>
- Notebook runs without errors
- All 6 parquet files created in output/dashboard/
- Sample file has exactly 50,000 rows
- Aggregations have expected columns and non-zero counts
- Files are loadable via pd.read_parquet()
</verification>

<success_criteria>
Dashboard data preparation complete with all aggregations saved. Ready for dashboard app development.
</success_criteria>

<output>
After completion, create `.planning/phases/03-visualization-reporting/03-01-SUMMARY.md`
</output>
