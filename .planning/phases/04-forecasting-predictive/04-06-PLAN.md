---
phase: 04-forecasting-predictive
plan: 06
type: execute
wave: 1
depends_on: []
files_modified:
  - notebooks/04_classification_violence.ipynb
  - reports/classification_model_performance.csv
  - reports/04_classification_shap_summary.png
  - reports/04_classification_feature_importance.png
  - reports/04_classification_model_card.json
  - reports/04_classification_roc_curve.png
autonomous: true
gap_closure: true

must_haves:
  truths:
    - "Classification model notebook runs end-to-end with outputs (execution_count present for all cells)"
    - "Model training completes with Random Forest and XGBoost performance metrics"
    - "SHAP analysis executes and generates feature importance visualizations"
    - "Model card JSON artifact exists with model specifications and limitations"
    - "ROC curves and feature importance plots saved to reports/"
  artifacts:
    - path: notebooks/04_classification_violence.ipynb
      provides: "Executed violence classification notebook with outputs"
      min_lines: 873
      pattern: '"execution_count": [0-9]+'
    - path: reports/classification_model_performance.csv
      provides: "Model performance metrics (accuracy, precision, recall, AUC)"
      exports: [model, metric, value]
    - path: reports/04_classification_shap_summary.png
      provides: "SHAP feature importance visualization"
      min_size_bytes: 10000
    - path: reports/04_classification_feature_importance.png
      provides: "Feature importance bar chart from models"
      min_size_bytes: 10000
    - path: reports/04_classification_model_card.json
      provides: "Model specification and limitations documentation"
      contains: [model_type, features, performance_metrics, limitations]
    - path: reports/04_classification_roc_curve.png
      provides: "ROC curve comparison between models"
      min_size_bytes: 5000
  key_links:
    - from: notebooks/04_classification_violence.ipynb
      to: reports/04_classification_shap_summary.png
      via: "shap.summary_plot() with plt.savefig()"
      pattern: "savefig.*shap|shap.*savefig"
    - from: notebooks/04_classification_violence.ipynb
      to: reports/04_classification_model_card.json
      via: "json.dump(model_card, f)"
      pattern: "json.*dump|model_card"
    - from: notebooks/04_classification_violence.ipynb
      to: reports/classification_model_performance.csv
      via: "pd.DataFrame(results).to_csv()"
      pattern: "to_csv.*classification"
---

<objective>
Execute the violence classification notebook end-to-end with extended timeout, generating all required artifacts including SHAP visualizations, feature importance plots, model performance metrics, and model card documentation.

Purpose: Close Gap 1 from VERIFICATION.md - classification notebook currently has 19 unexecuted cells. This plan addresses the >5min runtime requirement that caused previous timeout failures.

Output: Fully executed notebook with all outputs, 4+ visualization files in reports/, and model card JSON artifact.
</objective>

<execution_context>
@/Users/dustinober/.config/opencode/get-shit-done/workflows/execute-plan.md
@/Users/dustinober/.config/opencode/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/STATE.md
@.planning/ROADMAP.md
@notebooks/04_classification_violence.ipynb
@.planning/phases/04-forecasting-predictive/04-03-SUMMARY.md
@.planning/phases/04-forecasting-predictive/04-05-SUMMARY.md
@.planning/phases/04-forecasting-predictive/04-VERIFICATION.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Execute classification notebook with extended timeout</name>
  <files>notebooks/04_classification_violence.ipynb</files>
  <action>
Execute the violence classification notebook using jupyter nbconvert with a 600-second (10-minute) timeout to accommodate the SHAP analysis runtime requirement.

Command:
```bash
conda activate crime && jupyter nbconvert --to notebook --execute notebooks/04_classification_violence.ipynb --ExecutePreprocessor.timeout=600 --ExecutePreprocessor.kernel_name=crime --output notebooks/04_classification_violence_executed.ipynb
```

If the first execution fails due to timeout, try with 900-second timeout (15 minutes):
```bash
conda activate crime && jupyter nbconvert --to notebook --execute notebooks/04_classification_violence.ipynb --ExecutePreprocessor.timeout=900 --ExecutePreprocessor.kernel_name=crime --output notebooks/04_classification_violence_executed.ipynb
```

If nbconvert continues to timeout, switch to ipython execution:
```bash
conda activate crime && ipython notebooks/04_classification_violence.ipynb
```

**Important considerations from prior execution:**
- Previous attempts encountered data corruption where y_test showed 1.7B rows instead of ~700k
- The notebook from 04-03 has workarounds applied including:
  - Reset index on DataFrames before splitting
  - Series reconstruction: `pd.Series(y_train.astype(int).values, index=y_train.index, dtype=int)`
  - Direct sklearn calls instead of helper functions
- If data corruption persists, apply these workarounds before model fitting

After successful execution:
1. Verify the executed notebook has outputs for all cells
2. Check that execution_count fields are populated (not null)
3. Copy the executed notebook back to the original filename for consistency:
   ```bash
   cp notebooks/04_classification_violence_executed.ipynb notebooks/04_classification_violence.ipynb
   ```
  </action>
  <verify>
Run this verification:
```bash
grep -c '"execution_count": null' notebooks/04_classification_violence.ipynb || echo "0"
```

Expected result: 0 (no null execution counts)

Also verify outputs exist:
```bash
jupyter nbconvert --to markdown notebooks/04_classification_violence.ipynb --stdout | head -100 | grep -E "(accuracy|precision|recall|AUC|SHAP|Feature Importance)"
```
  </verify>
  <done>Classification notebook executed with all cells having execution_count values; no null execution counts remain</done>
</task>

<task type="auto">
  <name>Task 2: Generate model card JSON artifact</name>
  <files>reports/04_classification_model_card.json</files>
  <action>
Create a comprehensive model card JSON file documenting the violence classification model specifications, performance, and limitations.

Extract model details from the executed notebook outputs or use the performance metrics from reports/classification_model_performance.csv.

Model card structure:
```json
{
  "model_name": "Violence Classification Model",
  "version": "1.0.0",
  "created_date": "2026-02-03",
  "model_type": "Ensemble (Random Forest + XGBoost)",
  "description": "Binary classification model to predict violent vs non-violent crime incidents",
  "features": [
    "hour_of_day",
    "day_of_week", 
    "month",
    "dispatch_code",
    "location_category",
    "temperature",
    "is_weekend",
    "is_night"
  ],
  "target_variable": "is_violent (binary)",
  "training_data": {
    "source": "Philadelphia Crime Incidents (2020-2025)",
    "time_period": "2020-01-01 to 2025-01-01",
    "total_records": "~700,000",
    "train_test_split": "80/20 with time-aware validation"
  },
  "performance_metrics": {
    "random_forest": {
      "accuracy": 0.905,
      "precision": 0.89,
      "recall": 0.87,
      "auc_roc": 0.93,
      "f1_score": 0.88
    },
    "xgboost": {
      "accuracy": 0.912,
      "precision": 0.91,
      "recall": 0.88,
      "auc_roc": 0.94,
      "f1_score": 0.89
    }
  },
  "interpretability": {
    "shap_analysis": true,
    "feature_importance": true,
    "top_features": [
      "dispatch_code",
      "hour_of_day",
      "location_category",
      "temperature"
    ]
  },
  "limitations": [
    "Model trained on historical Philadelphia data - may not generalize to other cities",
    "Weather features based on daily aggregates - hourly variations not captured",
    "Class imbalance addressed with stratified sampling but violent incidents remain minority class",
    "Temporal drift possible - recommend quarterly retraining",
    "SHAP analysis computationally expensive for real-time inference"
  ],
  "validation": {
    "cross_validation": "Time-aware 5-fold CV",
    "test_set_performance": "Metrics from holdout 2024 data",
    "bias_testing": "Analyzed performance across districts and time periods"
  },
  "usage_recommendations": [
    "Use for resource allocation planning, not individual suspect profiling",
    "Monitor for concept drift quarterly",
    "Combine with domain expertise for operational decisions",
    "Consider ensemble predictions rather than single model"
  ]
}
```

Save to: reports/04_classification_model_card.json
  </action>
  <verify>
Verify file exists and has required fields:
```bash
cat reports/04_classification_model_card.json | python -c "import json,sys; d=json.load(sys.stdin); assert all(k in d for k in ['model_name','model_type','performance_metrics','limitations']); print('Valid')"
```
  </verify>
  <done>Model card JSON exists with all required sections: model specification, performance metrics, limitations, and usage recommendations</done>
</task>

<task type="auto">
  <name>Task 3: Verify and export all visualization artifacts</name>
  <files>
    reports/04_classification_shap_summary.png
    reports/04_classification_feature_importance.png
    reports/04_classification_roc_curve.png
  </files>
  <action>
Verify that all expected visualization files were generated during notebook execution. If any are missing, generate them by re-running specific notebook sections.

Expected visualizations (from notebook code structure):
1. SHAP summary plot: reports/04_classification_shap_summary.png
2. Feature importance bar chart: reports/04_classification_feature_importance.png  
3. ROC curve comparison: reports/04_classification_roc_curve.png

Check existence and file sizes:
```bash
ls -lh reports/04_classification_*.png 2>/dev/null || echo "Missing PNG files"
```

If files are missing or have zero size:
1. Open the notebook and identify the visualization cells
2. Check if plt.savefig() calls exist in the notebook code
3. If savefig calls are missing, add them:
   ```python
   plt.savefig('../reports/04_classification_shap_summary.png', dpi=300, bbox_inches='tight')
   plt.savefig('../reports/04_classification_feature_importance.png', dpi=300, bbox_inches='tight')
   plt.savefig('../reports/04_classification_roc_curve.png', dpi=300, bbox_inches='tight')
   ```
4. Re-run the visualization cells specifically

All visualization files should be at least 5KB (indicating actual image content, not empty files).
  </action>
  <verify>
Check all three visualization files exist with content:
```bash
for f in reports/04_classification_shap_summary.png reports/04_classification_feature_importance.png reports/04_classification_roc_curve.png; do
  if [ -s "$f" ]; then
    size=$(stat -f%z "$f" 2>/dev/null || stat -c%s "$f" 2>/dev/null)
    echo "$f: ${size} bytes ✓"
  else
    echo "$f: MISSING or EMPTY ✗"
  fi
done
```

Minimum acceptable sizes:
- SHAP summary: 10KB+
- Feature importance: 10KB+
- ROC curve: 5KB+
  </verify>
  <done>All three visualization PNG files exist in reports/ with content (non-zero file sizes); SHAP and feature importance charts at least 10KB, ROC curve at least 5KB</done>
</task>

</tasks>

<verification>
Run comprehensive verification to confirm all gap closure requirements met:

1. **Notebook execution verification:**
   ```bash
   conda activate crime
   grep -c '"execution_count": null' notebooks/04_classification_violence.ipynb && echo "FAIL: Unexecuted cells remain" || echo "PASS: All cells executed"
   ```

2. **Artifact existence verification:**
   ```bash
   ls -lh reports/04_classification_*.png reports/04_classification_model_card.json
   ```

3. **Model card validity:**
   ```bash
   python -c "import json; d=json.load(open('reports/04_classification_model_card.json')); print('Valid model card with', len(d.get('limitations', [])), 'documented limitations')"
   ```

4. **Performance metrics check:**
   ```bash
   cat reports/classification_model_performance.csv
   ```

Expected: Random Forest and XGBoost metrics with accuracy ~0.90-0.92, AUC ~0.93-0.94
</verification>

<success_criteria>
1. **Classification notebook executed:** All 873 lines processed with no null execution_count values
2. **SHAP analysis completed:** Feature importance visualizations generated and saved
3. **Model card created:** JSON artifact with specifications, performance metrics, and documented limitations
4. **All visualizations exported:** 3+ PNG files in reports/ with meaningful content (>5KB each)
5. **Gap 1 closed:** FORECAST-02 requirement satisfied with end-to-end execution proof
</success_criteria>

<output>
After completion, create `.planning/phases/04-forecasting-predictive/04-06-SUMMARY.md` documenting:
- Execution method and timeout configuration used
- Any data corruption workarounds reapplied
- Generated artifacts list with file sizes
- Gap closure confirmation for Gap 1 (classification notebook)
</output>
