---
phase: 01-data-foundation
plan: 02
type: execute
wave: 2
depends_on: [01]
files_modified:
  - notebooks/01_data_loading_validation.ipynb
  - scripts/data_loader.py
autonomous: true
must_haves:
  truths:
    - "Raw data is loaded into memory"
    - "Schema is validated against strict rules"
    - "Missing value patterns are quantified"
  artifacts:
    - path: "notebooks/01_data_loading_validation.ipynb"
      provides: "Data audit log"
    - path: "scripts/data_loader.py"
      provides: "Reusable loading logic"
  key_links:
    - from: "notebooks/01_data_loading_validation.ipynb"
      to: "pandera.Schema"
      via: "validation call"
---

<objective>
Load raw data and perform rigorous schema and missing value auditing.

Purpose: Establish the "ground truth" of the data quality before any cleaning occurs.
Output: Initial section of Notebook 01 with Pandera validation and null analysis.
</objective>

<execution_context>
@~/.config/opencode/get-shit-done/workflows/execute-plan.md
@~/.config/opencode/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/phases/01-data-foundation/01-01-SUMMARY.md
@.planning/research/01-stack-research.md
@.planning/REQUIREMENTS.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement Loader & Schema Validation</name>
  <files>scripts/data_loader.py, notebooks/01_data_loading_validation.ipynb</files>
  <action>
    1. Create `scripts/data_loader.py`:
       - Function `load_raw_data()` using pyarrow engine
       - Define `Pandera` schema for crime incidents (validate types, crucial columns)
       - Allow "lazy" validation (report all errors, don't crash on first)
    
    2. Start `notebooks/01_data_loading_validation.ipynb`:
       - Import config and loader
       - Load data
       - Run schema validation and display error report (if any)
       - Document column definitions (Data Dictionary section)
  </action>
  <verify>
    # Run notebook to ensure loading works
    jupyter nbconvert --to notebook --execute notebooks/01_data_loading_validation.ipynb --inplace
  </verify>
  <done>Notebook loads data and outputs schema validation report.</done>
</task>

<task type="auto">
  <name>Task 2: Missing Value Audit</name>
  <files>notebooks/01_data_loading_validation.ipynb</files>
  <action>
    Append to `notebooks/01_data_loading_validation.ipynb`:
    1. Calculate % missing for all columns
    2. Visualize missingness:
       - Heatmap of missing values (seaborn)
       - Missingness by year (is older data worse?)
    3. Check specifically for 'lat'/'lng' missingness
    4. Markdown summary of missing value patterns (Requirement QUAL-02)
  </action>
  <verify>
    jupyter nbconvert --to notebook --execute notebooks/01_data_loading_validation.ipynb --inplace
  </verify>
  <done>Notebook contains visual and tabular analysis of missing data.</done>
</task>

</tasks>

<verification>
Confirm data loads and schema checks run.
</verification>

<success_criteria>
- 3.5M+ records loaded (or whatever exists in raw)
- Schema validation provides clear report of issues
- Missing value patterns documented
</success_criteria>

<output>
After completion, create `.planning/phases/01-data-foundation/01-02-SUMMARY.md`
</output>
