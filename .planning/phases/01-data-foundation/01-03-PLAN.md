---
phase: 01-data-foundation
plan: 03
type: execute
wave: 3
depends_on: [02]
files_modified:
  - notebooks/01_data_loading_validation.ipynb
  - data/processed/crime_incidents_cleaned.parquet
autonomous: true
must_haves:
  truths:
    - "Geocoding quality is characterized"
    - "Reporting lag is quantified"
    - "Clean dataset is saved"
  artifacts:
    - path: "notebooks/01_data_loading_validation.ipynb"
      provides: "Full cleaning pipeline"
    - path: "data/processed/crime_incidents_cleaned.parquet"
      provides: "Gold standard dataset"
  key_links:
    - from: "notebooks/01_data_loading_validation.ipynb"
      to: "data/processed/crime_incidents_cleaned.parquet"
      via: "save_parquet"
---

<objective>
Complete data characterization (geo, lag, seasonality) and save the final clean dataset.

Purpose: Finalize the foundation for Phase 2 analysis.
Output: Completed Notebook 01 and the cleaned parquet file.
</objective>

<execution_context>
@~/.config/opencode/get-shit-done/workflows/execute-plan.md
@~/.config/opencode/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/phases/01-data-foundation/01-02-SUMMARY.md
@.planning/REQUIREMENTS.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Advanced Diagnostics (Geo & Lag)</name>
  <files>notebooks/01_data_loading_validation.ipynb</files>
  <action>
    Append to `notebooks/01_data_loading_validation.ipynb`:
    1. **Geocoding Analysis (QUAL-03):**
       - Calculate valid coordinate % by district
       - Check for "Null Island" (0,0) coordinates
    
    2. **Reporting Lag (QUAL-04):**
       - Calculate lag = `dispatch_date_time` vs `objectid` order (if entry date unavailable) or just analyze recent drop-off
       - Visualize record counts by month for last 12 months to spot lag-induced drop-off
       - Define exclusion cutoff (e.g., last 30 days)
    
    3. **Seasonality Check:**
       - Quick STL decomposition on monthly counts to confirm expected summer peak (sanity check)
  </action>
  <verify>
    jupyter nbconvert --to notebook --execute notebooks/01_data_loading_validation.ipynb --inplace
  </verify>
  <done>Notebook includes geocoding coverage map/table and lag analysis plots.</done>
</task>

<task type="auto">
  <name>Task 2: Final Cleaning & Saving</name>
  <files>notebooks/01_data_loading_validation.ipynb, data/processed/crime_incidents_cleaned.parquet</files>
  <action>
    Append to `notebooks/01_data_loading_validation.ipynb`:
    1. Apply cleaning decisions:
       - Drop duplicates (if any)
       - Exclude data within lag window (if decided)
       - Filter invalid coordinates (optional, or just flag them)
    2. Save cleaned dataframe to `data/processed/crime_incidents_cleaned.parquet`
    3. Print summary of "Raw vs Cleaned" counts
  </action>
  <verify>
    jupyter nbconvert --to notebook --execute notebooks/01_data_loading_validation.ipynb --inplace
    ls -lh data/processed/crime_incidents_cleaned.parquet
  </verify>
  <done>Clean dataset exists on disk; Notebook run is complete.</done>
</task>

</tasks>

<verification>
Verify final dataset exists and is readable.
</verification>

<success_criteria>
- Geocoding and lag analysis documented in notebook
- Clean dataset saved successfully
- Reproducible workflow established
</success_criteria>

<output>
After completion, create `.planning/phases/01-data-foundation/01-03-SUMMARY.md`
</output>
