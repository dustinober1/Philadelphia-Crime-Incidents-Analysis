---
phase: 06-configuration-cli-system
plan: 05
type: execute
wave: 3
depends_on: ["06-03"]
files_modified:
  - analysis/cli/patrol.py
autonomous: true

must_haves:
  truths:
    - "User can run `python -m analysis.cli patrol hotspots --fast` and see progress"
    - "User can run `python -m analysis.cli patrol district-severity --fast` and see output"
    - "All 4 Patrol commands load data and save outputs"
    - "Rich progress bars show for spatial analysis operations"
  artifacts:
    - path: "analysis/cli/patrol.py"
      provides: "Patrol command implementations with analysis logic"
      exports: ["hotspots", "robbery_heatmap", "district_severity", "census_rates"]
  key_links:
    - from: "analysis/cli/patrol.py"
      to: "analysis/data/loading.py"
      via: "load_crime_data() import"
      pattern: "from.*loading.*import"
    - from: "analysis/cli/patrol.py"
      to: "analysis/utils/spatial.py"
      via: "spatial utilities import"
      pattern: "from.*spatial.*import"
---

<objective>
Implement Patrol command group analysis logic (hotspots, robbery-heatmap, district-severity, census-rates) with spatial processing.

Purpose: Complete the Patrol commands with spatial analysis logic, data loading, and Rich progress feedback. These commands involve geospatial operations and clustering.

Output: Working Patrol commands that load data, perform spatial analysis, generate outputs, and show progress bars.
</objective>

<execution_context>
@/Users/dustinober/.claude/get-shit-done/workflows/execute-plan.md
@/Users/dustinober/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/phases/06-configuration-cli-system/06-RESEARCH.md
@.planning/phases/06-configuration-cli-system/06-04-SUMMARY.md

# Data layer and spatial utilities
@analysis/data/loading.py
@analysis/utils/spatial.py
@analysis/utils/classification.py

# Existing orchestration patterns
@analysis/orchestrate_phase2.py

# Config schemas
@analysis/config/schemas/patrol.py
@config/patrol.yaml
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement Patrol hotspots command</name>
  <files>analysis/cli/patrol.py</files>
  <action>
    Update the `hotspots` command in `analysis/cli/patrol.py` with full analysis logic:

    ```python
    @app.command()
    def hotspots(
        eps: float = typer.Option(0.002, help="DBSCAN epsilon parameter (degrees)"),
        min_samples: int = typer.Option(50, help="DBSCAN min_samples parameter"),
        version: str = typer.Option("v1.0", help="Output version tag"),
        fast: bool = typer.Option(False, "--fast", help="Fast mode with 10%% sample"),
    ) -> None:
        """Identify crime hotspots using spatial clustering."""
        from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TaskProgressColumn, TimeRemainingColumn
        from analysis.data.loading import load_crime_data
        from analysis.utils.classification import classify_crime_category
        from pathlib import Path

        config = HotspotsConfig(
            eps_degrees=eps,
            min_samples=min_samples,
            version=version,
            fast_mode=fast,
        )

        console.print(f"[bold blue]Hotspots Analysis[/bold blue]")
        console.print(f"  Epsilon: {config.eps_degrees} degrees")
        console.print(f"  Min samples: {config.min_samples}")
        console.print()

        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            BarColumn(),
            TaskProgressColumn(),
            TimeRemainingColumn(),
        ) as progress:
            load_task = progress.add_task("Loading crime data...", total=100)
            df = load_crime_data(use_cache=config.cache_enabled)
            if config.fast_mode:
                df = df.sample(frac=config.fast_sample_frac, random_state=42)
            progress.update(load_task, advance=100)

            clean_task = progress.add_task("Cleaning coordinates...", total=100)

            # Filter for valid coordinates (Philadelphia bounds)
            df = df[
                (df["lng"].between(config.lon_min, config.lon_max)) &
                (df["lat"].between(config.lat_min, config.lat_max))
            ].dropna(subset=["lng", "lat"])

            progress.update(clean_task, advance=100, description=f"Cleaned to {len(df)} valid points")

            cluster_task = progress.add_task("Running DBSCAN clustering...", total=100)

            # Try to import sklearn for clustering
            try:
                from sklearn.cluster import DBSCAN
                import numpy as np

                coords = df[["lng", "lat"]].values
                clustering = DBSCAN(eps=config.eps_degrees, min_samples=config.min_samples)
                df["cluster"] = clustering.fit_predict(coords)

                n_clusters = len(set(df["cluster"])) - (1 if -1 in df["cluster"].values else 0)
                n_noise = list(df["cluster"]).count(-1)

                progress.update(cluster_task, advance=100, description=f"Found {n_clusters} clusters")

            except ImportError:
                console.print("[yellow]Warning: scikit-learn not available, skipping clustering[/yellow]")
                df["cluster"] = -1
                n_clusters = 0
                n_noise = len(df)

            output_task = progress.add_task("Saving outputs...", total=100)
            output_path = Path(config.output_dir) / config.version / "patrol"
            output_path.mkdir(parents=True, exist_ok=True)

            summary_file = output_path / f"{config.report_name}_summary.txt"
            with open(summary_file, "w") as f:
                f.write(f"Hotspots Analysis Summary\n")
                f.write(f"=" * 40 + "\n")
                f.write(f"DBSCAN parameters:\n")
                f.write(f"  eps: {config.eps_degrees} degrees\n")
                f.write(f"  min_samples: {config.min_samples}\n")
                f.write(f"\nResults:\n")
                f.write(f"  Total points: {len(df)}\n")
                f.write(f"  Clusters found: {n_clusters}\n")
                f.write(f"  Noise points: {n_noise}\n")

            progress.update(output_task, advance=100)

        console.print()
        console.print("[green]:heavy_check_mark:[/green] [bold green]Analysis complete[/bold green]")
        console.print(f"  Clusters found: [cyan]{n_clusters}[/cyan]")
        console.print(f"  Output directory: [cyan]{output_path}[/cyan]")
    ```

    Use sklearn.cluster.DBSCAN for clustering. Handle ImportError gracefully (sklearn may not be installed in all environments).
  </action>
  <verify>
    - `python -m analysis.cli patrol hotspots --eps 0.003 --min-samples 100 --fast` executes with progress
    - `ls reports/v1.0/patrol/` contains hotspots_report_summary.txt
    - Summary shows cluster count and noise point count
  </verify>
  <done>
    Hotspots command loads data, filters coordinates, runs DBSCAN clustering, shows progress, and saves output.
  </done>
</task>

<task type="auto">
  <name>Task 2: Implement robbery-heatmap and district-severity commands</name>
  <files>analysis/cli/patrol.py</files>
  <action>
    Update the `robbery_heatmap` and `district_severity` commands with analysis logic:

    ```python
    @app.command(name="robbery-heatmap")
    def robbery_heatmap(
        time_bin: int = typer.Option(60, help="Time bin size in minutes"),
        grid_size: int = typer.Option(20, help="Grid size for heatmap"),
        version: str = typer.Option("v1.0", help="Output version tag"),
        fast: bool = typer.Option(False, "--fast", help="Fast mode with 10%% sample"),
    ) -> None:
        """Generate temporal heatmap for robbery incidents."""
        from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TaskProgressColumn, TimeRemainingColumn
        from analysis.data.loading import load_crime_data
        from analysis.utils.classification import classify_crime_category
        from pathlib import Path
        import pandas as pd

        config = RobberyConfig(time_bin_size=time_bin, grid_size=grid_size, version=version, fast_mode=fast)

        console.print(f"[bold blue]Robbery Heatmap Analysis[/bold blue]")
        console.print(f"  Time bin: {config.time_bin_size} minutes")
        console.print(f"  Grid size: {config.grid_size}")
        console.print()

        with Progress(
            SpinnerColumn(), TextColumn("[progress.description]{task.description}"),
            BarColumn(), TaskProgressColumn(), TimeRemainingColumn(),
        ) as progress:
            load_task = progress.add_task("Loading data...", total=100)
            df = load_crime_data(use_cache=config.cache_enabled)
            if config.fast_mode:
                df = df.sample(frac=config.fast_sample_frac, random_state=42)
            progress.update(load_task, advance=100)

            filter_task = progress.add_task("Filtering robbery incidents...", total=100)

            # Filter for robbery (UCR code 300)
            df = df[df["ucr_general"].between(300, 399)].copy()

            # Add time column
            df["hour"] = pd.to_datetime(df["dispatch_date"]).dt.hour
            df["time_bin"] = (df["hour"] * 60) // config.time_bin_size

            progress.update(filter_task, advance=100, description=f"Found {len(df)} robbery incidents")

            output_task = progress.add_task("Saving outputs...", total=100)
            output_path = Path(config.output_dir) / config.version / "patrol"
            output_path.mkdir(parents=True, exist_ok=True)

            summary_file = output_path / f"{config.report_name}_summary.txt"
            with open(summary_file, "w") as f:
                f.write(f"Robbery Heatmap Analysis Summary\n")
                f.write(f"=" * 40 + "\n")
                f.write(f"Time bin size: {config.time_bin_size} minutes\n")
                f.write(f"Total robbery incidents: {len(df)}\n")
                f.write(f"\nIncidents by hour:\n")
                hourly_counts = df.groupby("hour").size()
                for hour, count in hourly_counts.items():
                    f.write(f"  {hour:02d}:00 - {count:,.0f}\n")

            progress.update(output_task, advance=100)

        console.print()
        console.print("[green]:heavy_check_mark:[/green] [bold green]Analysis complete[/bold green]")
        console.print(f"  Total robbery incidents: [cyan]{len(df)}[/cyan]")


    @app.command(name="district-severity")
    def district_severity(
        districts: list[int] | None = typer.Option(None, help="Districts to analyze (default: all)"),
        version: str = typer.Option("v1.0", help="Output version tag"),
        fast: bool = typer.Option(False, "--fast", help="Fast mode with 10%% sample"),
    ) -> None:
        """Calculate severity scores by police district."""
        from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TaskProgressColumn, TimeRemainingColumn
        from analysis.data.loading import load_crime_data
        from analysis.utils.classification import classify_crime_category
        from analysis.config import SEVERITY_WEIGHTS
        from pathlib import Path

        config = DistrictConfig(districts=districts, version=version, fast_mode=fast)

        console.print(f"[bold blue]District Severity Analysis[/bold blue]")
        console.print(f"  Districts: {config.districts or 'All'}")
        console.print()

        with Progress(
            SpinnerColumn(), TextColumn("[progress.description]{task.description}"),
            BarColumn(), TaskProgressColumn(), TimeRemainingColumn(),
        ) as progress:
            load_task = progress.add_task("Loading data...", total=100)
            df = load_crime_data(use_cache=config.cache_enabled)
            if config.fast_mode:
                df = df.sample(frac=config.fast_sample_frac, random_state=42)
            progress.update(load_task, advance=100)

            score_task = progress.add_task("Calculating severity scores...", total=100)

            # Filter to specified districts if provided
            if config.districts:
                df = df[df["dc_district"].isin(config.districts)]

            # Calculate severity scores
            df["ucr_hundred"] = (df["ucr_general"] // 100) * 100
            df["severity_weight"] = df["ucr_hundred"].map(SEVERITY_WEIGHTS).fillna(1.0)

            district_scores = df.groupby("dc_district")["severity_weight"].sum().sort_values(ascending=False)

            progress.update(score_task, advance=100, description=f"Scored {len(district_scores)} districts")

            output_task = progress.add_task("Saving outputs...", total=100)
            output_path = Path(config.output_dir) / config.version / "patrol"
            output_path.mkdir(parents=True, exist_ok=True)

            summary_file = output_path / f"{config.report_name}_summary.txt"
            with open(summary_file, "w") as f:
                f.write(f"District Severity Analysis Summary\n")
                f.write(f"=" * 40 + "\n")
                f.write(f"Severity calculated using FBI UCR hierarchy weights\n")
                f.write(f"\nDistrict rankings:\n")
                for district, score in district_scores.head(10).items():
                    f.write(f"  District {district}: {score:,.1f}\n")

            progress.update(output_task, advance=100)

        console.print()
        console.print("[green]:heavy_check_mark:[/green] [bold green]Analysis complete[/bold green]")
        console.print(f"  Districts analyzed: [cyan]{len(district_scores)}[/cyan]")
    ```

    Note: For robbery-heatmap, use UCR code 300-399 for robbery incidents. For district-severity, use SEVERITY_WEIGHTS from analysis.config.
  </action>
  <verify>
    - `python -m analysis.cli patrol robbery-heatmap --fast` executes
    - `python -m analysis.cli patrol district-severity --districts 1 2 3 --fast` executes and filters to specified districts
    - Output files created in reports/v1.0/patrol/
  </verify>
  <done>
    Robbery heatmap and district severity commands execute with data loading, filtering, and output generation.
  </done>
</task>

<task type="auto">
  <name>Task 3: Implement census-rates command</name>
  <files>analysis/cli/patrol.py</files>
  <action>
    Update the `census_rates` command with analysis logic:

    ```python
    @app.command(name="census-rates")
    def census_rates(
        population_threshold: int = typer.Option(100, help="Minimum population for census tract"),
        version: str = typer.Option("v1.0", help="Output version tag"),
        fast: bool = typer.Option(False, "--fast", help="Fast mode with 10%% sample"),
    ) -> None:
        """Calculate crime rates per census tract."""
        from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TaskProgressColumn, TimeRemainingColumn
        from analysis.data.loading import load_crime_data, load_boundary_data
        from pathlib import Path

        config = CensusConfig(population_threshold=population_threshold, version=version, fast_mode=fast)

        console.print(f"[bold blue]Census Rates Analysis[/bold blue]")
        console.print(f"  Population threshold: {config.population_threshold}")
        console.print()

        with Progress(
            SpinnerColumn(), TextColumn("[progress.description]{task.description}"),
            BarColumn(), TaskProgressColumn(), TimeRemainingColumn(),
        ) as progress:
            load_task = progress.add_task("Loading data...", total=100)
            crime_df = load_crime_data(use_cache=config.cache_enabled)
            if config.fast_mode:
                crime_df = crime_df.sample(frac=config.fast_sample_frac, random_state=42)
            progress.update(load_task, advance=100)

            boundary_task = progress.add_task("Loading census boundaries...", total=100)

            try:
                census_gdf = load_boundary_data(config.census_file)
                console.print(f"[green]Loaded {len(census_gdf)} census tracts[/green]")
            except (FileNotFoundError, ImportError) as e:
                console.print(f"[yellow]Warning: Could not load census boundaries: {e}[/yellow]")
                console.print("[yellow]Using aggregated statistics instead[/yellow]")
                census_gdf = None

            progress.update(boundary_task, advance=100)

            analyze_task = progress.add_task("Calculating rates...", total=100)

            # Aggregate by available geographic unit
            if census_gdf is not None and "name20" in crime_df.columns:
                # Join with census data
                tract_counts = crime_df.groupby("name20")["objectid"].count()
            else:
                # Fallback: use point-based aggregation
                console.print("[yellow]Using spatial aggregation fallback[/yellow]")
                tract_counts = None

            progress.update(analyze_task, advance=100)

            output_task = progress.add_task("Saving outputs...", total=100)
            output_path = Path(config.output_dir) / config.version / "patrol"
            output_path.mkdir(parents=True, exist_ok=True)

            summary_file = output_path / f"{config.report_name}_summary.txt"
            with open(summary_file, "w") as f:
                f.write(f"Census Rates Analysis Summary\n")
                f.write(f"=" * 40 + "\n")
                f.write(f"Population threshold: {config.population_threshold}\n")
                f.write(f"Total incidents: {len(crime_df):,.0f}\n")
                if tract_counts is not None:
                    f.write(f"\nTop 10 tracts by incident count:\n")
                    for tract, count in tract_counts.head(10).items():
                        f.write(f"  Tract {tract}: {count:,.0f}\n")

            progress.update(output_task, advance=100)

        console.print()
        console.print("[green]:heavy_check_mark:[/green] [bold green]Analysis complete[/bold green]")
        console.print(f"  Output directory: [cyan]{output_path}[/cyan]")
    ```

    Note: This command may need to handle missing census boundary data gracefully. Use load_boundary_data() if available, otherwise fallback to aggregated statistics.
  </action>
  <verify>
    - `python -m analysis.cli patrol census-rates --fast` executes
    - Command handles missing census data gracefully with warning message
    - Output file created with census statistics
  </verify>
  <done>
    Census rates command loads data, attempts spatial join with census boundaries, calculates rates, and saves output.
  </done>
</task>

</tasks>

<verification>
After completing this plan:

1. Run all 4 Patrol commands in fast mode:
   - `python -m analysis.cli patrol hotspots --fast`
   - `python -m analysis.cli patrol robbery-heatmap --fast`
   - `python -m analysis.cli patrol district-severity --fast`
   - `python -m analysis.cli patrol census-rates --fast`

2. Verify progress bars appear for spatial operations

3. Verify output files created:
   - `ls reports/v1.0/patrol/` should show 4 summary files

4. Test custom arguments:
   - `python -m analysis.cli patrol hotspots --eps 0.001 --min-samples 200 --fast`

Expected outcome: All 4 Patrol commands working with progress feedback, spatial analysis executed, outputs saved.
</verification>

<success_criteria>
1. All 4 Patrol commands execute successfully
2. Hotspots command runs DBSCAN clustering and reports cluster count
3. District-severity command uses SEVERITY_WEIGHTS from config
4. Census-rates command handles missing boundary data gracefully
5. Progress bars show for all operations
</success_criteria>

<output>
After completion, create `.planning/phases/06-configuration-cli-system/06-05-SUMMARY.md` with:
- Commands implemented (hotspots, robbery-heatmap, district-severity, census-rates)
- Spatial analysis approach (DBSCAN, severity scoring, census joins)
- Error handling for missing dependencies (sklearn, geopandas, census data)
- Performance metrics
</output>
