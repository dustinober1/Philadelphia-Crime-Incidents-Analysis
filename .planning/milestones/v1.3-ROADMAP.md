# Milestone v1.3: Testing & Cleanup

**Status:** ðŸš§ IN PROGRESS
**Phases:** 10-15
**Total Plans:** TBD
**Depth:** Standard

## Overview

Achieve 95%+ test coverage across Python codebase (analysis/, api/, pipeline/, CLI) and perform comprehensive repository cleanup to establish a clean quality baseline for future development. This milestone follows a tests-first, cleanup-second approach to ensure all code paths are validated before making removal decisions.

## Phases

### Phase 10: Test Infrastructure & Baseline

**Goal**: Establish testing foundation and measure coverage baseline before writing tests.
**Depends on**: Phase 9 (v1.2 completion)
**Requirements**: INFRA-01, INFRA-02, INFRA-03, INFRA-04

**Success Criteria**:
1. Developer can run tests in parallel using pytest-xdist with 4-8x faster execution
2. Build system enforces 95% coverage threshold via pyproject.toml configuration
3. CI pipeline validates coverage with diff-cover to prevent backsliding
4. Current coverage baseline (16%) is measured and documented with gap analysis

**Details:**
- Configure pytest-xdist for parallel test execution across CPUs
- Add coverage threshold enforcement in pyproject.toml (fail_under=95.0)
- Set up diff-cover for PR-level coverage validation
- Document testing quality criteria requiring meaningful assertions and behavior-focused tests
- Measure current 16% coverage baseline and identify the 56 modules requiring tests

### Phase 11: Core Module Testing

**Goal**: Achieve 60-70% overall coverage by testing highest-impact modules first.
**Depends on**: Phase 10
**Requirements**: CORE-01, CORE-02, CORE-03, CORE-04

**Success Criteria**:
1. All analysis/models/ modules have comprehensive unit tests covering ML and statistical logic
2. All analysis/data/ modules have unit tests covering data loading and validation logic
3. All analysis/utils/ modules have unit tests covering core utility functions
4. Overall coverage reaches 60-70% milestone with meaningful assertions

**Details:**
- Write unit tests for analysis/models/ (classification, time series, validation)
- Write unit tests for analysis/data/ (loading, preprocessing, validation)
- Write unit tests for analysis/utils/ (spatial, temporal, classification utilities)
- Use pytest-mock for mocking I/O operations
- Apply Hypothesis for property-based testing of pandas edge cases
- Establish testing patterns for team to follow in subsequent phases

### Phase 12: API & CLI Testing

**Goal**: Achieve 80-85% overall coverage with service layer validation.
**Depends on**: Phase 11
**Requirements**: API-01, API-02, API-03, API-04, CLI-01, CLI-02, CLI-03, CLI-04

**Success Criteria**:
1. All 11 FastAPI router endpoints have integration tests using TestClient
2. API tests validate request/response contracts and error handling paths
3. All 8 Typer CLI commands have tests using CliRunner
4. CLI tests validate argument parsing, output formatting, and exit codes

**Details:**
- Extend tests/test_api_endpoints.py with comprehensive FastAPI TestClient tests
- Write unit tests for api/services/ with mocked data loaders
- Write CLI integration tests using Typer's CliRunner
- Test async routes with pytest-asyncio
- Mock external dependencies and file I/O appropriately
- Validate error handling and edge cases

### Phase 13: Pipeline & Supporting Tests

**Goal**: Complete coverage to 95% with pipeline and remaining module tests.
**Depends on**: Phase 12
**Requirements**: PIPE-01, PIPE-02, PIPE-03, SUPP-01, SUPP-02, SUPP-03, SUPP-04

**Success Criteria**:
1. Pipeline export and refresh operations have tests with mocked external APIs
2. Pipeline error handling paths are covered by tests
3. Configuration, visualization, and remaining modules have comprehensive tests
4. Overall coverage reaches 95%+ across entire Python codebase

**Details:**
- Write tests for pipeline export operations with mocked external APIs
- Write tests for pipeline refresh operations validating data integrity
- Cover pipeline error handling paths
- Add tests for analysis/config/ modules (configuration parsing and validation)
- Add tests for analysis/visualization/ modules (chart generation logic)
- Test remaining analysis/ modules to reach 95%+ coverage
- Validate output preparation rather than pixel-perfect rendering for visualizations

### Phase 14: Repository Cleanup

**Goal**: Remove unused code, deprecated files, and build artifacts with safety gates.
**Depends on**: Phase 13
**Requirements**: CLEAN-01, CLEAN-02, CLEAN-03, CLEAN-04, CLEAN-05, CLEAN-06

**Success Criteria**:
1. All cache files (.pyc, __pycache__, .DS_Store) removed with automated cleanup
2. Dead code identified with vulture and removed after manual review
3. Unused imports removed automatically with autoflake
4. Deprecated content in scripts/, docs/, notebooks/ identified and removed
5. Safety gates prevent accidental removal of active code (git status check, test validation)

**Details:**
- Run pyclean to remove Python artifacts (*.pyc, __pycache__, .coverage)
- Run vulture to identify unused functions and classes (dead code detection)
- Run autoflake to remove unused imports automatically
- Manually review and remove deprecated scripts, documentation, and notebooks
- Implement safety gates: git status check, import validation, test dependency check
- Update .gitignore to prevent future accumulation of temporary files
- Use quarantine branch for easily restorable deletions

### Phase 15: Quality Validation & CI Integration

**Goal**: Validate test effectiveness and enforce quality in CI for sustainability.
**Depends on**: Phase 14
**Requirements**: QUAL-01, QUAL-02, QUAL-03

**Success Criteria**:
1. Coverage reports generate HTML and JSON outputs showing 95%+ coverage
2. Mutation testing with mutmut validates test quality on critical modules
3. Coverage badge displayed in README showing current coverage percentage
4. CI enforces 95% threshold and diff-cover prevents coverage backsliding

**Details:**
- Generate HTML and JSON coverage reports for review
- Run mutation testing on critical modules to validate test effectiveness
- Generate coverage badge for README display
- Set up GitHub Actions workflow with pytest --cov-fail-under=95
- Configure diff-cover for PR-level validation (--compare-branch=origin/main --fail-under=95)
- Upload test reports and coverage artifacts to CI
- Document make targets for all test operations

---

## Progress

| Phase | Status | Requirements | Coverage Target |
|-------|--------|--------------|-----------------|
| 10 - Infrastructure | Pending | 4/4 | Baseline (16%) |
| 11 - Core Modules | Pending | 4/4 | 60-70% |
| 12 - API & CLI | Pending | 8/8 | 80-85% |
| 13 - Pipeline & Supporting | Pending | 7/7 | 95%+ |
| 14 - Cleanup | Pending | 6/6 | Maintain 95%+ |
| 15 - Quality & CI | Pending | 3/3 | Enforce 95%+ |

**Total Requirements:** 32/32 mapped

---

## Milestone Summary

**Decimal Phases:**

- None planned (may be added via `/gsd-insert-phase` if urgent work discovered)

**Key Decisions:**

- Decision: Tests-first, cleanup-second approach. Rationale: Validate all code paths before removal decisions. Outcome: TBD.
- Decision: Target 95% coverage (not 100%). Rationale: Optimal quality/effort balance, diminishing returns above 95%. Outcome: TBD.
- Decision: Use mutation testing for quality validation. Rationale: Prevent coverage gaming, ensure tests catch bugs. Outcome: TBD.

**Issues to Address:**

- Preventing coverage gaming (hitting percentages without meaningful assertions)
- Avoiding breaking existing functionality while adding tests
- Ensuring deleted code is truly unused (not just untested)
- Managing test suite performance as coverage grows

**Issues Deferred:**

- UI/E2E tests for Next.js frontend (out of scope for quality milestone)
- Load/stress testing (deferred to future milestone)
- 100% coverage obsession (95% is optimal target)

**Technical Debt Expected:**

- None anticipated if tests focus on behavior through public APIs

---

_For current project status, see `.planning/ROADMAP.md`_
