{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "794727e5",
   "metadata": {},
   "source": [
    "# Data Quality Audit Notebook\n",
    "*Philadelphia Crime Incidents Dataset - Comprehensive Quality Assessment*\n",
    "\n",
    "Comprehensive data quality analysis with step-by-step execution and intermediate outputs.\n",
    "\n",
    "## Methodology\n",
    "\n",
    "**Analysis Approach:** Each section validates data completeness, accuracy, consistency, and validity through statistical tests and visualization.\n",
    "\n",
    "**Statistical Rigor:** All tests use 99% confidence intervals. Missing data patterns tested for bias (chi-square tests of independence).\n",
    "\n",
    "**Quality Scoring:** Weighted composite score (40% completeness, 30% accuracy, 15% consistency, 15% validity).\n",
    "\n",
    "**Key Validations:**\n",
    "1. Data Loading & Versioning\n",
    "2. Coordinate Validation & Temporal Features\n",
    "3. Missing Data Patterns\n",
    "4. Coordinate Coverage Analysis\n",
    "5. Duplicate Detection\n",
    "6. Outlier Analysis\n",
    "7. Temporal Gaps Assessment\n",
    "8. Quality Score Calculation\n",
    "9. Report Generation\n",
    "10. Executive Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99f3c9b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete - all libraries imported and configured\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import base64\n",
    "import hashlib\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from pathlib import Path\n",
    "import io\n",
    "import random\n",
    "from typing import Dict, Any, Optional, Union, Tuple\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('default')\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Setup complete - all libraries imported and configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b75d5b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration constants defined\n"
     ]
    }
   ],
   "source": [
    "FIGURE_SIZES = {\n",
    "    \"small\": (8, 6),\n",
    "    \"medium\": (12, 8),\n",
    "    \"wide\": (16, 8),\n",
    "    \"large\": (14, 10),\n",
    "    \"heatmap\": (16, 12),\n",
    "    \"square\": (10, 10),\n",
    "}\n",
    "\n",
    "COLORS = {\n",
    "    \"primary\": \"#1f77b4\",\n",
    "    \"secondary\": \"#ff7f0e\",\n",
    "    \"danger\": \"#d62728\",\n",
    "    \"success\": \"#2ca02c\",\n",
    "    \"warning\": \"#ffbb00\",\n",
    "    \"palette\": \"tab20\",\n",
    "    \"sequential\": \"YlOrRd\",\n",
    "    \"diverging\": \"RdBu_r\",\n",
    "}\n",
    "\n",
    "PHILADELPHIA_BBOX = {\n",
    "    \"lon_min\": -75.28,\n",
    "    \"lon_max\": -74.95,\n",
    "    \"lat_min\": 39.86,\n",
    "    \"lat_max\": 40.14,\n",
    "}\n",
    "\n",
    "STAT_CONFIG = {\n",
    "    \"confidence_level\": 0.99,\n",
    "    \"alpha\": 0.01,\n",
    "    \"bootstrap_n_resamples\": 9999,\n",
    "    \"bootstrap_random_state\": 42,\n",
    "    \"fdr_method\": \"bh\",\n",
    "    \"random_seed\": 42,\n",
    "}\n",
    "\n",
    "print(\"Configuration constants defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "902d5855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utility functions defined\n"
     ]
    }
   ],
   "source": [
    "def image_to_base64(fig) -> str:\n",
    "    buf = io.BytesIO()\n",
    "    fig.savefig(buf, format=\"png\", dpi=100, bbox_inches=\"tight\")\n",
    "    buf.seek(0)\n",
    "    img_str = base64.b64encode(buf.read()).decode(\"utf-8\")\n",
    "    buf.close()\n",
    "    return img_str\n",
    "\n",
    "def create_image_tag(base64_str: str, alt: str = \"\", width: int = 800) -> str:\n",
    "    return f'<img src=\"data:image/png;base64,{base64_str}\" alt=\"{alt}\" width=\"{width}\">'\n",
    "\n",
    "def format_number(num: int | float) -> str:\n",
    "    if isinstance(num, float):\n",
    "        return f\"{num:,.2f}\"\n",
    "    return f\"{num:,}\"\n",
    "\n",
    "def validate_coordinates(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df[\"valid_coord\"] = False\n",
    "    df[\"coord_issue\"] = None\n",
    "    \n",
    "    has_x = \"point_x\" in df.columns\n",
    "    has_y = \"point_y\" in df.columns\n",
    "    \n",
    "    if has_x and has_y:\n",
    "        valid_mask = (\n",
    "            df[\"point_x\"].notna()\n",
    "            & df[\"point_y\"].notna()\n",
    "            & (df[\"point_x\"] >= PHILADELPHIA_BBOX[\"lon_min\"])\n",
    "            & (df[\"point_x\"] <= PHILADELPHIA_BBOX[\"lon_max\"])\n",
    "            & (df[\"point_y\"] >= PHILADELPHIA_BBOX[\"lat_min\"])\n",
    "            & (df[\"point_y\"] <= PHILADELPHIA_BBOX[\"lat_max\"])\n",
    "        )\n",
    "        df.loc[valid_mask, \"valid_coord\"] = True\n",
    "        \n",
    "        missing_mask = df[\"point_x\"].isna() | df[\"point_y\"].isna()\n",
    "        df.loc[missing_mask, \"coord_issue\"] = \"missing\"\n",
    "        \n",
    "        invalid_lon = (\n",
    "            df[\"point_x\"].notna()\n",
    "            & ((df[\"point_x\"] < PHILADELPHIA_BBOX[\"lon_min\"]) | (df[\"point_x\"] > PHILADELPHIA_BBOX[\"lon_max\"]))\n",
    "        )\n",
    "        df.loc[invalid_lon, \"coord_issue\"] = \"invalid_longitude\"\n",
    "        \n",
    "        invalid_lat = (\n",
    "            df[\"point_y\"].notna()\n",
    "            & ((df[\"point_y\"] < PHILADELPHIA_BBOX[\"lat_min\"]) | (df[\"point_y\"] > PHILADELPHIA_BBOX[\"lat_max\"]))\n",
    "        )\n",
    "        df.loc[invalid_lat & ~invalid_lon, \"coord_issue\"] = \"invalid_latitude\"\n",
    "    \n",
    "    return df\n",
    "\n",
    "def extract_temporal_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "\n",
    "    if \"dispatch_datetime\" not in df.columns:\n",
    "        if \"dispatch_date\" in df.columns:\n",
    "            df[\"dispatch_datetime\"] = pd.to_datetime(df[\"dispatch_date\"])\n",
    "        else:\n",
    "            return df\n",
    "\n",
    "    dt = df[\"dispatch_datetime\"].dt\n",
    "\n",
    "    df[\"year\"] = dt.year\n",
    "    df[\"month\"] = dt.month\n",
    "    df[\"day\"] = dt.day\n",
    "    df[\"day_of_week\"] = dt.dayofweek\n",
    "    df[\"day_name\"] = dt.day_name()\n",
    "    df[\"hour\"] = dt.hour\n",
    "    df[\"month_name\"] = dt.month_name()\n",
    "\n",
    "    df[\"is_weekend\"] = df[\"day_of_week\"].isin([5, 6])\n",
    "\n",
    "    df[\"time_period\"] = pd.cut(\n",
    "        df[\"hour\"],\n",
    "        bins=[-1, 6, 12, 18, 24],\n",
    "        labels=[\"Overnight (12am-6am)\", \"Morning (6am-12pm)\", \"Afternoon (12pm-6pm)\", \"Evening (6pm-12am)\"]\n",
    "    )\n",
    "\n",
    "    df[\"season\"] = pd.cut(\n",
    "        df[\"month\"],\n",
    "        bins=[0, 3, 6, 9, 12],\n",
    "        labels=[\"Winter\", \"Spring\", \"Summer\", \"Fall\"]\n",
    "    )\n",
    "\n",
    "    df[\"year_month\"] = df[\"dispatch_datetime\"].dt.to_period(\"M\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_missing_summary(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    missing = df.isnull().sum()\n",
    "    missing_pct = (missing / len(df) * 100).round(2)\n",
    "\n",
    "    summary = pd.DataFrame({\n",
    "        \"column\": df.columns,\n",
    "        \"missing_count\": missing.values,\n",
    "        \"missing_percentage\": missing_pct.values,\n",
    "        \"dtype\": df.dtypes.values,\n",
    "    })\n",
    "\n",
    "    summary = summary[summary[\"missing_count\"] > 0].sort_values(\"missing_count\", ascending=False)\n",
    "\n",
    "    return summary\n",
    "\n",
    "print(\"Utility functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2d572b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reproducibility utilities defined\n"
     ]
    }
   ],
   "source": [
    "class DataVersion:\n",
    "    def __init__(self, data_path: Path | str) -> None:\n",
    "        self.path = Path(data_path)\n",
    "        if not self.path.exists():\n",
    "            raise FileNotFoundError(f\"Data file not found: {self.path}\")\n",
    "\n",
    "        self._metadata = self._compute_metadata()\n",
    "        self.sha256 = self._metadata[\"sha256\"]\n",
    "        self.row_count = self._metadata[\"row_count\"]\n",
    "        self.column_count = self._metadata[\"column_count\"]\n",
    "        self.columns = self._metadata[\"columns\"]\n",
    "        self.date_range = self._metadata.get(\"date_range\")\n",
    "        self.computed_at = self._metadata[\"computed_at\"]\n",
    "\n",
    "    def _compute_metadata(self) -> Dict[str, Any]:\n",
    "        sha256_hash = hashlib.sha256()\n",
    "        chunk_size = 4096\n",
    "\n",
    "        with open(self.path, \"rb\") as f:\n",
    "            while chunk := f.read(chunk_size):\n",
    "                sha256_hash.update(chunk)\n",
    "\n",
    "        try:\n",
    "            df = pd.read_parquet(self.path)\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Failed to read parquet file: {e}\") from e\n",
    "\n",
    "        date_range = None\n",
    "        if \"dispatch_date\" in df.columns:\n",
    "            dates_series = df[\"dispatch_date\"]\n",
    "            if pd.api.types.is_categorical_dtype(dates_series):\n",
    "                dates_series = dates_series.astype(str)\n",
    "\n",
    "            dates = pd.to_datetime(dates_series, errors=\"coerce\")\n",
    "            valid_dates = dates.dropna()\n",
    "            if len(valid_dates) > 0:\n",
    "                min_date = valid_dates.min().strftime(\"%Y-%m-%d\")\n",
    "                max_date = valid_dates.max().strftime(\"%Y-%m-%d\")\n",
    "                date_range = (min_date, max_date)\n",
    "        \n",
    "        return {\n",
    "            \"sha256\": sha256_hash.hexdigest(),\n",
    "            \"row_count\": len(df),\n",
    "            \"column_count\": len(df.columns),\n",
    "            \"columns\": list(df.columns),\n",
    "            \"date_range\": date_range,\n",
    "            \"computed_at\": datetime.now(timezone.utc).isoformat(),\n",
    "        }\n",
    "\n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"path\": str(self.path),\n",
    "            \"sha256\": self.sha256,\n",
    "            \"row_count\": self.row_count,\n",
    "            \"column_count\": self.column_count,\n",
    "            \"columns\": self.columns,\n",
    "            \"date_range\": self.date_range,\n",
    "            \"computed_at\": self.computed_at,\n",
    "        }\n",
    "\n",
    "def set_global_seed(seed: Optional[int] = None) -> int:\n",
    "    if seed is None:\n",
    "        seed = STAT_CONFIG[\"random_seed\"]\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "    return seed\n",
    "\n",
    "def get_analysis_metadata(data_version: Optional[DataVersion] = None, **params: Any) -> Dict[str, Any]:\n",
    "    metadata = {\n",
    "        \"timestamp\": datetime.now(timezone.utc).isoformat(),\n",
    "        \"parameters\": params.copy() if params else {},\n",
    "        \"data_version\": data_version.to_dict() if data_version else None,\n",
    "    }\n",
    "\n",
    "    return metadata\n",
    "\n",
    "print(\"Reproducibility utilities defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b596fbb",
   "metadata": {},
   "source": [
    "## Section 2: Data Loading and Version Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ec4cffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set to: 42\n",
      "Loading data and computing version...\n",
      "Data version: <__main__.DataVersion object at 0x11abc4440>\n",
      "Loading crime incidents data...\n",
      "Loaded 3,496,353 records with 16 columns\n",
      "\n",
      "=== DATA OVERVIEW ===\n",
      "Date Range: 2006-01-01 to 2026-01-20\n",
      "SHA256: 2a45f7eb1102e7f0...\n",
      "\n",
      "=== COLUMN SAMPLE ===\n",
      "['the_geom', 'cartodb_id', 'the_geom_webmercator', 'objectid', 'dc_dist', 'psa', 'dispatch_date_time', 'dispatch_date', 'dispatch_time', 'hour'] ...\n",
      "\n",
      "=== DATA TYPES ===\n",
      "int64                  4\n",
      "float64                4\n",
      "category               1\n",
      "category               1\n",
      "category               1\n",
      "datetime64[ns, UTC]    1\n",
      "category               1\n",
      "category               1\n",
      "category               1\n",
      "category               1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Data loading complete!\n"
     ]
    }
   ],
   "source": [
    "# Set data path\n",
    "DATA_PATH = Path(\"./data/crime_incidents_combined.parquet\")\n",
    "\n",
    "seed = set_global_seed()\n",
    "print(f\"Random seed set to: {seed}\")\n",
    "\n",
    "print(\"Loading data and computing version...\")\n",
    "data_version = DataVersion(DATA_PATH)\n",
    "print(f\"Data version: {data_version}\")\n",
    "\n",
    "print(\"Loading crime incidents data...\")\n",
    "df = pd.read_parquet(DATA_PATH)\n",
    "print(f\"Loaded {len(df):,} records with {len(df.columns)} columns\")\n",
    "\n",
    "print(\"\\n=== DATA OVERVIEW ===\")\n",
    "print(f\"Date Range: {data_version.date_range[0]} to {data_version.date_range[1]}\")\n",
    "print(f\"SHA256: {data_version.sha256[:16]}...\")\n",
    "\n",
    "print(\"\\n=== COLUMN SAMPLE ===\")\n",
    "print(df.columns.tolist()[:10], \"...\")\n",
    "\n",
    "print(\"\\n=== DATA TYPES ===\")\n",
    "print(df.dtypes.value_counts())\n",
    "\n",
    "print(\"\\nData loading complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d70256b",
   "metadata": {},
   "source": [
    "## Section 3: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5a715a",
   "metadata": {},
   "source": [
    "Validating coordinates...\n",
    "Extracting temporal features...\n",
    "\n",
    "=== COORDINATE VALIDATION SUMMARY ===\n",
    "Total Records: 3,496,353\n",
    "Valid Coordinates: 3,440,053\n",
    "Invalid Coordinates: 56,300\n",
    "Valid Percentage: 98.39%\n",
    "Invalid Percentage: 1.61%\n",
    "\n",
    "=== COORDINATE ISSUE BREAKDOWN ===\n",
    "Missing: 55,912 (1.6%)\n",
    "Invalid_Longitude: 388 (0.01%)\n",
    "\n",
    "=== SAMPLE OF PROCESSED DATA ===\n",
    "\n",
    "Data preprocessing complete!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed259eb",
   "metadata": {},
   "source": [
    "## Section 4: Missing Data Analysis\n",
    "\n",
    "This section analyzes patterns in missing data across columns, crime types, and districts to identify potential biases in data collection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8cf5ca",
   "metadata": {},
   "source": [
    "Analyzing missing data patterns...\n",
    "\n",
    "=== MISSING DATA BY COLUMN ===\n",
    "Columns with missing data: 7\n",
    "\n",
    "=== STATISTICAL TEST: MISSINGNESS BY CRIME TYPE ===\n",
    "Test: 8,677.69\n",
    "P-value: 0.00e+00\n",
    "Effect Size (Cramer's V): 0.050 (negligible association)\n",
    "\n",
    "=== MISSING COORDINATES BY CRIME TYPE ===\n",
    "\n",
    "=== STATISTICAL TEST: MISSINGNESS BY DISTRICT ===\n",
    "Test: 209,051.06\n",
    "P-value: 0.00e+00\n",
    "Effect Size (Cramer's V): 0.245 (weak association)\n",
    "\n",
    "=== MISSING COORDINATES BY DISTRICT ===\n",
    "\n",
    "Missing data analysis complete!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a36099",
   "metadata": {},
   "source": [
    "# Missing data visualization\\nprint(\"Creating missing data visualizations...\")\\n\\n# Create missing data heatmap\\nfig, ax = plt.subplots(figsize=FIGURE_SIZES[\"large\"])\\n# Sample for heatmap if too large\\nsample_df = df.sample(n=min(10000, len(df)), random_state=42)\\nmissing_sample = sample_df.isnull().astype(int)\\n\\nsns.heatmap(missing_sample, cbar=True, cmap=\\\"YlOrRd\\\", ax=ax)\\nax.set_title(\\\"Missing Data Pattern Heatmap (Sample of 10,000 records)\\\")\\nax.set_xlabel(\\\"Columns\\\")\\nax.set_ylabel(\\\"Records (Sample)\\\")\\nplt.tight_layout()\\nplt.show()\\n\\n# Missing data bar chart\\nfig, ax = plt.subplots(figsize=FIGURE_SIZES[\"wide\"])\\nif not missing_summary.empty:\\n    missing_plot = missing_summary.head(10).sort_values(\"missing_count\")\\n    bars = ax.barh(missing_plot[\"column\"], missing_plot[\"missing_count\"], color=COLORS[\"warning\"])\\n    ax.set_xlabel(\\\"Missing Count\\\")\\n    ax.set_title(\\\"Top 10 Columns with Missing Data\\\")\\n    ax.set_xscale(\\\"log\\\")\\n    \\n    # Add count labels\\n    for bar in bars:\\n        width = bar.get_width()\\n        ax.text(width, bar.get_y() + bar.get_height()/2, f\\\" {format_number(int(width))}\\\", va=\\\"center\\\", fontsize=9)\\nplt.tight_layout()\\nplt.show()\\n\\nprint(\"Missing data visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ba3027",
   "metadata": {},
   "source": [
    "## Section 5: Coordinate Coverage Analysis\n",
    "\n",
    "This section analyzes the coverage and validity of coordinate data, identifying issues with geographic locations and testing for biases by crime type and district."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef60e6ea",
   "metadata": {},
   "source": [
    "Analyzing coordinate coverage...\n",
    "\n",
    "=== OVERALL COVERAGE ===\n",
    "Total Records: 3,496,353\n",
    "Valid Coordinates: 3,440,053 (98.39%)\n",
    "Invalid Coordinates: 56,300 (1.61%)\n",
    "\n",
    "=== ISSUE BREAKDOWN ===\n",
    "Missing: 55,912 (1.6%)\n",
    "Invalid_Longitude: 388 (0.01%)\n",
    "Coordinate coverage analysis complete!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec7fc1f",
   "metadata": {},
   "source": [
    "\n",
    "=== COORDINATE COVERAGE BY CRIME TYPE ===\n",
    "Highest coverage:\n",
    "\n",
    "Lowest coverage:\n",
    "\n",
    "=== STATISTICAL TEST: COVERAGE BY CRIME TYPE ===\n",
    "Test: 8,692.35\n",
    "P-value: 0.00e+00\n",
    "Effect Size (Cramer's V): 0.050 (negligible association)\n",
    "\n",
    "=== COORDINATE COVERAGE BY DISTRICT ===\n",
    "\n",
    "=== STATISTICAL TEST: COVERAGE BY DISTRICT ===\n",
    "Test: 207,930.76\n",
    "P-value: 0.00e+00\n",
    "Effect Size (Cramer's V): 0.244 (weak association)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f269050",
   "metadata": {},
   "source": [
    "Coordinate coverage visualization complete!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1cd106",
   "metadata": {},
   "source": [
    "## Section 6: Duplicate Detection\n",
    "\n",
    "This section identifies duplicate records in the dataset through multiple approaches: exact duplicates, key column duplicates, and geographic proximity analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5954902",
   "metadata": {},
   "source": [
    "Detecting duplicates...\n",
    "\n",
    "=== EXACT DUPLICATES ===\n",
    "Duplicate Records: 0 (0.00%)\n",
    "objectid: 0 duplicates (0.00%)\n",
    "dc_key: 230 duplicates (0.01%)\n",
    "\n",
    "=== NEAR-DUPLICATES (SAME LOCATION) ===\n",
    "Unique locations with multiple incidents: 79,988\n",
    "Total incidents at multi-incident locations: 3,344,377\n",
    "Percentage of incidents: 97.21%\n",
    "\n",
    "=== POTENTIAL MULTIPLE REPORTS ===\n",
    "Count of unique (date, district, crime_type) with multiple incidents: 764207\n",
    "Total incidents in multi-report combinations: 2,831,870\n",
    "Percentage: 80.99%\n",
    "\n",
    "Duplicate detection complete!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb74df53",
   "metadata": {},
   "source": [
    "## Section 7: Outlier Detection\n",
    "\n",
    "This section identifies outliers in coordinate, numerical, and temporal data using various statistical methods to detect potentially erroneous records."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ea20bb",
   "metadata": {},
   "source": [
    "Detecting outliers...\n",
    "\n",
    "=== COORDINATE OUTLIERS ===\n",
    "Invalid Coordinates: 56,300 (1.61%)\n",
    "\n",
    "Issue breakdown:\n",
    "Missing: 55,912 (1.6%)\n",
    "Invalid_Longitude: 388 (0.01%)\n",
    "\n",
    "=== NUMERICAL OUTLIERS (IQR METHOD) ===\n",
    "Analyzing 12 numerical columns...\n",
    "\n",
    "=== TEMPORAL OUTLIERS ===\n",
    "Date Range: 2006-01-01 to 2026-01-20\n",
    "Future dates (> Dec 31, 2025): 0\n",
    "Pre-2006 dates: 0\n",
    "Total temporal outliers: 0\n",
    "\n",
    "Outlier detection complete!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd92a04",
   "metadata": {},
   "source": [
    "## Section 8: Temporal Gaps Analysis\n",
    "\n",
    "This section analyzes temporal gaps in the data, identifying dates with no incidents, longest gaps without data, and creates a time series visualization of daily incident counts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d95ae2a",
   "metadata": {},
   "source": [
    "Analyzing temporal gaps...\n",
    "\n",
    "=== DATE COVERAGE ===\n",
    "Date Range: 2006-01-01 to 2026-01-20\n",
    "Total Days: 7,325\n",
    "Days with Incidents: 7,324\n",
    "Days without Incidents: 1\n",
    "\n",
    "=== LONGEST GAP WITHOUT DATA ===\n",
    "Start: 2025-03-10\n",
    "End: 2025-03-10\n",
    "Duration: 1 days\n",
    "\n",
    "=== DAILY INCIDENT COUNT STATISTICS ===\n",
    "Mean: 477.4 incidents/day\n",
    "Median: 477.0 incidents/day\n",
    "Min: 61 incidents/day\n",
    "Max: 855 incidents/day\n",
    "Std Dev: 98.0 incidents/day\n",
    "\n",
    "=== DAILY INCIDENT COUNT DISTRIBUTION ===\n",
    "\n",
    "Temporal gaps analysis complete!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fe202b",
   "metadata": {},
   "source": [
    "## Section 9: Quality Score Calculation\n",
    "\n",
    "This section calculates overall and component quality scores for the dataset using standardized metrics. The overall score combines completeness, accuracy, consistency, and validity with appropriate weights to produce a comprehensive quality assessment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c075a3d6",
   "metadata": {},
   "source": [
    "Calculating quality scores...\n",
    "\n",
    "=== COMPLETENESS SCORE ===\n",
    "Score: 96.39%\n",
    "Missing cells: 3,665,097/101,394,237\n",
    "\n",
    "=== ACCURACY SCORE ===\n",
    "Score: 98.39%\n",
    "\n",
    "=== CONSISTENCY SCORE ===\n",
    "Score: 100.00%\n",
    "Duplicate records: 0/3,496,353\n",
    "\n",
    "=== VALIDITY SCORE ===\n",
    "Score: 98.39%\n",
    "\n",
    "=== OVERALL QUALITY SCORE ===\n",
    "Overall Score: 97.83/100 (A (Excellent))\n",
    "\n",
    "=== COMPONENT SCORES ===\n",
    "\n",
    "=== QUALITY SCORE RADAR CHART ===\n",
    "\n",
    "Quality score calculation complete!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913c056e",
   "metadata": {},
   "source": [
    "## Section 10: Comprehensive Analysis Report\n",
    "\n",
    "All analysis results, findings, and recommendations are displayed below as an integrated report within this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a132aa53",
   "metadata": {},
   "source": [
    "Generating integrated markdown report...\n",
    "\n",
    " Integrated report displayed above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d84e33",
   "metadata": {},
   "source": [
    "## Section 11: Executive Summary\n",
    "\n",
    "This section displays the final quality assessment results and provides a comprehensive summary of the data quality analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d82fcaa",
   "metadata": {},
   "source": [
    "\n",
    "======================================================================\n",
    "DATA QUALITY AUDIT - EXECUTIVE SUMMARY\n",
    "======================================================================\n",
    "\n",
    "OVERALL DATA QUALITY SCORE: 97.83/100 (A (Excellent))\n",
    "\n",
    "COMPONENT SCORES:\n",
    "  Completeness: 96.39% (40% weight)\n",
    "  Accuracy: 98.39% (30% weight)\n",
    "  Consistency: 100.0% (15% weight)\n",
    "  Validity: 98.39% (15% weight)\n",
    "\n",
    " KEY FINDINGS:\n",
    "• Coordinate Coverage: 98.39% valid (1.61% invalid)\n",
    "• Duplicate Records: 0 (0.0% of total)\n",
    "\n",
    " RECOMMENDATIONS:\n",
    " GOOD: High coordinate coverage (98.4%). Safe for spatial analysis.\n",
    " GOOD: Low duplicate rate (0.0%). Data is consistent.\n",
    "\n",
    " ANALYSIS LIMITATIONS:\n",
    "• All statistical tests use 99% confidence intervals for conservative inference.\n",
    "• Missing data patterns have been tested for bias (chi-square tests).\n",
    "\n",
    "======================================================================\n",
    " DATA QUALITY AUDIT NOTEBOOK COMPLETE!\n",
    "    All analyses executed successfully\n",
    "    Comprehensive quality assessment completed\n",
    "    Markdown report generated and saved\n",
    "======================================================================"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.14.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
