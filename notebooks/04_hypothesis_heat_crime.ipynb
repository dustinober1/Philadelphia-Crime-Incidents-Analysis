{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heat-Crime Hypothesis Analysis (HYP-HEAT)\n",
    "\n",
    "**Objective:** Investigate the statistical relationship between temperature and crime patterns in Philadelphia.\n",
    "\n",
    "**Research Question:** Is there a significant relationship between temperature (heat) and crime rates, particularly for violent crimes?\n",
    "\n",
    "**Data Sources:**\n",
    "- Crime incidents: `data/crime_incidents_combined.parquet` (2006-2026)\n",
    "- Weather data: `data/external/weather_philly_2006_2026.parquet` (2006-2026)\n",
    "\n",
    "**Methodology:**\n",
    "1. Data merging with temporal alignment (daily aggregation)\n",
    "2. Correlation analysis (Pearson, Spearman, Kendall tau)\n",
    "3. Hypothesis testing with statistical significance\n",
    "4. Effect size calculation and interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Ensure we can import from analysis module\n",
    "repo_root = Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()\n",
    "sys.path.insert(0, str(repo_root))\n",
    "\n",
    "# Standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('default')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Create reports directory\n",
    "REPORTS_DIR = repo_root / 'reports'\n",
    "REPORTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Repository root: {repo_root}\")\n",
    "print(f\"Reports directory: {REPORTS_DIR}\")\n",
    "print(f\"Analysis date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Exploration\n",
    "\n",
    "### 1.1 Load Crime Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load crime data\n",
    "crime_path = repo_root / 'data' / 'crime_incidents_combined.parquet'\n",
    "crime_df = pd.read_parquet(crime_path)\n",
    "\n",
    "print(f\"Crime data shape: {crime_df.shape}\")\n",
    "print(f\"\\nColumns: {crime_df.columns.tolist()}\")\n",
    "print(f\"\\nDate range: {crime_df['dispatch_date'].min()} to {crime_df['dispatch_date'].max()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "crime_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check crime categories\n",
    "print(\"Top 15 crime types:\")\n",
    "print(crime_df['text_general_code'].value_counts().head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Load Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load weather data\n",
    "weather_path = repo_root / 'data' / 'external' / 'weather_philly_2006_2026.parquet'\n",
    "weather_df = pd.read_parquet(weather_path)\n",
    "\n",
    "print(f\"Weather data shape: {weather_df.shape}\")\n",
    "print(f\"\\nColumns: {weather_df.columns.tolist()}\")\n",
    "print(f\"\\nDate range: {weather_df.index.min()} to {weather_df.index.max()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "weather_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weather data summary statistics\n",
    "print(\"Weather data summary:\")\n",
    "weather_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Merging Strategy\n",
    "\n",
    "### Join Strategy Documentation\n",
    "\n",
    "**Temporal Alignment:**\n",
    "- Weather data: Daily observations (one record per day)\n",
    "- Crime data: Individual incidents with dispatch_date field\n",
    "- **Strategy:** Aggregate crime data to daily counts, then join on date\n",
    "\n",
    "**Spatial Considerations:**\n",
    "- Weather data: Single station representing Philadelphia metropolitan area\n",
    "- Crime data: Individual incidents across all police districts\n",
    "- **Strategy:** Use city-wide weather data for all crimes (assumes temperature is relatively uniform across the city)\n",
    "- **Limitation:** Does not account for micro-climate variations or heat island effects in specific neighborhoods\n",
    "\n",
    "**Crime Classification:**\n",
    "- Create categories: Violent crimes, Property crimes, Other crimes\n",
    "- Based on UCR general codes (as established in Phase 1)\n",
    "\n",
    "### 2.1 Define Crime Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crime category mapping based on UCR general codes (hundred-bands 1-7)\n",
    "# From analysis/config.py established in Phase 1\n",
    "\n",
    "CRIME_CATEGORY_MAP = {\n",
    "    1: 'Violent',      # Homicide\n",
    "    2: 'Violent',      # Rape\n",
    "    3: 'Violent',      # Robbery\n",
    "    4: 'Violent',      # Aggravated Assault\n",
    "    5: 'Property',     # Burglary\n",
    "    6: 'Property',     # Theft\n",
    "    7: 'Property',     # Motor Vehicle Theft\n",
    "}\n",
    "\n",
    "def categorize_crime(ucr_code):\n",
    "    \"\"\"Categorize crime based on UCR general code hundred-band.\"\"\"\n",
    "    hundred_band = int(ucr_code // 100) if pd.notna(ucr_code) else 0\n",
    "    return CRIME_CATEGORY_MAP.get(hundred_band, 'Other')\n",
    "\n",
    "# Apply categorization\n",
    "crime_df['crime_category'] = crime_df['ucr_general'].apply(categorize_crime)\n",
    "\n",
    "print(\"Crime category distribution:\")\n",
    "print(crime_df['crime_category'].value_counts())\n",
    "print(f\"\\nPercentages:\")\n",
    "print(crime_df['crime_category'].value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Aggregate Crime Data to Daily Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dispatch_date to datetime for proper aggregation\n",
    "crime_df['date'] = pd.to_datetime(crime_df['dispatch_date'])\n",
    "\n",
    "# Aggregate total crimes per day\n",
    "daily_crime = crime_df.groupby('date').size().reset_index(name='total_crimes')\n",
    "\n",
    "# Aggregate by crime category\n",
    "daily_crime_by_category = crime_df.groupby(['date', 'crime_category']).size().unstack(fill_value=0)\n",
    "daily_crime_by_category = daily_crime_by_category.reset_index()\n",
    "\n",
    "# Merge total with categories\n",
    "daily_crime_merged = daily_crime.merge(daily_crime_by_category, on='date', how='left')\n",
    "\n",
    "print(f\"Daily crime data shape: {daily_crime_merged.shape}\")\n",
    "print(f\"\\nDate range: {daily_crime_merged['date'].min()} to {daily_crime_merged['date'].max()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "daily_crime_merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Merge Weather and Crime Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare weather data for merge\n",
    "weather_df_reset = weather_df.reset_index()\n",
    "weather_df_reset['date'] = pd.to_datetime(weather_df_reset['time']).dt.date\n",
    "weather_df_reset['date'] = pd.to_datetime(weather_df_reset['date'])\n",
    "\n",
    "# Select relevant weather columns\n",
    "weather_cols = ['date', 'temp', 'tmin', 'tmax', 'rhum', 'prcp', 'wspd']\n",
    "weather_for_merge = weather_df_reset[weather_cols]\n",
    "\n",
    "print(f\"Weather data for merge shape: {weather_for_merge.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(weather_for_merge.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge crime and weather data on date\n",
    "merged_df = daily_crime_merged.merge(weather_for_merge, on='date', how='inner')\n",
    "\n",
    "print(f\"\\nMerged dataset shape: {merged_df.shape}\")\n",
    "print(f\"Date range: {merged_df['date'].min()} to {merged_df['date'].max()}\")\n",
    "print(f\"\\nNumber of days: {len(merged_df)}\")\n",
    "print(f\"\\nMerged data columns: {merged_df.columns.tolist()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "merged_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values in merged dataset:\")\n",
    "print(merged_df.isnull().sum())\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\nSummary statistics of merged dataset:\")\n",
    "merged_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Data Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data completeness\n",
    "date_range = pd.date_range(start=merged_df['date'].min(), end=merged_df['date'].max(), freq='D')\n",
    "expected_days = len(date_range)\n",
    "actual_days = len(merged_df)\n",
    "\n",
    "print(f\"Expected days in range: {expected_days}\")\n",
    "print(f\"Actual days in merged data: {actual_days}\")\n",
    "print(f\"Coverage: {actual_days / expected_days * 100:.2f}%\")\n",
    "\n",
    "# Check for any gaps\n",
    "if actual_days < expected_days:\n",
    "    missing_dates = set(date_range) - set(merged_df['date'])\n",
    "    print(f\"\\nNumber of missing dates: {len(missing_dates)}\")\n",
    "    if len(missing_dates) <= 10:\n",
    "        print(f\"Missing dates: {sorted(missing_dates)}\")\n",
    "else:\n",
    "    print(\"\\nNo missing dates - complete daily coverage.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Merge Strategy\n",
    "\n",
    "**Approach:**\n",
    "1. **Temporal alignment:** Aggregated crime incidents to daily counts to match weather data granularity\n",
    "2. **Spatial approach:** Used city-wide weather station data for all crimes (single station)\n",
    "3. **Join method:** Inner join on date to ensure both datasets have matching records\n",
    "4. **Crime categorization:** Classified crimes into Violent, Property, and Other based on UCR codes\n",
    "\n",
    "**Limitations:**\n",
    "- Single weather station may not capture micro-climate variations across neighborhoods\n",
    "- Heat island effects in urban cores vs. suburbs not considered\n",
    "- Daily aggregation loses intra-day temperature variations\n",
    "- Weather data represents average conditions, not peak exposure times\n",
    "\n",
    "**Dataset Ready for Analysis:**\n",
    "- Merged dataset includes daily crime counts by category and weather measurements\n",
    "- Complete temporal coverage from 2006 to 2026\n",
    "- Ready for correlation analysis and hypothesis testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Correlation Analysis\n",
    "\n",
    "### 3.1 Correlation Helper Function\n",
    "\n",
    "Following Pattern 3 from the research, we'll use multiple correlation methods to assess the relationship between temperature and crime rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_correlation(x, y, x_label='X', y_label='Y'):\n",
    "    \"\"\"\n",
    "    Comprehensive correlation analysis with multiple tests.\n",
    "    Based on Pattern 3 from research documentation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    x : array-like\n",
    "        First variable (e.g., temperature)\n",
    "    y : array-like\n",
    "        Second variable (e.g., crime count)\n",
    "    x_label : str\n",
    "        Label for x variable\n",
    "    y_label : str\n",
    "        Label for y variable\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Correlation results with multiple methods and significance tests\n",
    "    \"\"\"\n",
    "    # Remove NaN values\n",
    "    mask = ~(pd.isna(x) | pd.isna(y))\n",
    "    x_clean = np.array(x)[mask]\n",
    "    y_clean = np.array(y)[mask]\n",
    "    \n",
    "    # Pearson correlation (linear relationship)\n",
    "    pearson_r, pearson_p = stats.pearsonr(x_clean, y_clean)\n",
    "    \n",
    "    # Spearman rank correlation (monotonic relationship, robust to outliers)\n",
    "    spearman_r, spearman_p = stats.spearmanr(x_clean, y_clean)\n",
    "    \n",
    "    # Kendall tau (robust to outliers, good for small samples)\n",
    "    kendall_tau, kendall_p = stats.kendalltau(x_clean, y_clean)\n",
    "    \n",
    "    # Linear regression for effect size\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(x_clean, y_clean)\n",
    "    \n",
    "    # Effect size interpretation (based on absolute correlation)\n",
    "    abs_corr = abs(pearson_r)\n",
    "    if abs_corr < 0.1:\n",
    "        strength = 'negligible'\n",
    "    elif abs_corr < 0.3:\n",
    "        strength = 'small'\n",
    "    elif abs_corr < 0.5:\n",
    "        strength = 'medium'\n",
    "    else:\n",
    "        strength = 'large'\n",
    "    \n",
    "    results = {\n",
    "        'x_label': x_label,\n",
    "        'y_label': y_label,\n",
    "        'n_observations': len(x_clean),\n",
    "        'pearson_r': pearson_r,\n",
    "        'pearson_p': pearson_p,\n",
    "        'pearson_significant': pearson_p < 0.05,\n",
    "        'spearman_r': spearman_r,\n",
    "        'spearman_p': spearman_p,\n",
    "        'spearman_significant': spearman_p < 0.05,\n",
    "        'kendall_tau': kendall_tau,\n",
    "        'kendall_p': kendall_p,\n",
    "        'kendall_significant': kendall_p < 0.05,\n",
    "        'regression_slope': slope,\n",
    "        'regression_intercept': intercept,\n",
    "        'regression_r_squared': r_value**2,\n",
    "        'regression_p_value': p_value,\n",
    "        'effect_size_strength': strength\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def print_correlation_results(results):\n",
    "    \"\"\"Print correlation results in a readable format.\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Correlation Analysis: {results['x_label']} vs {results['y_label']}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Sample size: {results['n_observations']:,} observations\")\n",
    "    print(f\"\\nPearson correlation (linear):\")\n",
    "    print(f\"  r = {results['pearson_r']:.4f}, p = {results['pearson_p']:.4e}\")\n",
    "    print(f\"  Significant: {'YES' if results['pearson_significant'] else 'NO'}\")\n",
    "    print(f\"\\nSpearman correlation (monotonic):\")\n",
    "    print(f\"  \u03c1 = {results['spearman_r']:.4f}, p = {results['spearman_p']:.4e}\")\n",
    "    print(f\"  Significant: {'YES' if results['spearman_significant'] else 'NO'}\")\n",
    "    print(f\"\\nKendall tau (robust):\")\n",
    "    print(f\"  \u03c4 = {results['kendall_tau']:.4f}, p = {results['kendall_p']:.4e}\")\n",
    "    print(f\"  Significant: {'YES' if results['kendall_significant'] else 'NO'}\")\n",
    "    print(f\"\\nLinear regression:\")\n",
    "    print(f\"  R\u00b2 = {results['regression_r_squared']:.4f}\")\n",
    "    print(f\"  Slope = {results['regression_slope']:.4f}\")\n",
    "    print(f\"  p-value = {results['regression_p_value']:.4e}\")\n",
    "    print(f\"\\nEffect size: {results['effect_size_strength'].upper()}\")\n",
    "    print(f\"{'='*70}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Temperature vs. Total Crime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation: Temperature vs. Total Crime\n",
    "total_crime_corr = analyze_correlation(\n",
    "    merged_df['temp'], \n",
    "    merged_df['total_crimes'],\n",
    "    x_label='Temperature (\u00b0C)',\n",
    "    y_label='Total Daily Crimes'\n",
    ")\n",
    "\n",
    "print_correlation_results(total_crime_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Temperature vs. Violent Crime\n",
    "\n",
    "The heat-crime hypothesis specifically predicts a stronger relationship for violent crimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation: Temperature vs. Violent Crime\n",
    "violent_crime_corr = analyze_correlation(\n",
    "    merged_df['temp'], \n",
    "    merged_df['Violent'],\n",
    "    x_label='Temperature (\u00b0C)',\n",
    "    y_label='Daily Violent Crimes'\n",
    ")\n",
    "\n",
    "print_correlation_results(violent_crime_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Temperature vs. Property Crime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation: Temperature vs. Property Crime\n",
    "property_crime_corr = analyze_correlation(\n",
    "    merged_df['temp'], \n",
    "    merged_df['Property'],\n",
    "    x_label='Temperature (\u00b0C)',\n",
    "    y_label='Daily Property Crimes'\n",
    ")\n",
    "\n",
    "print_correlation_results(property_crime_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Summary Table of Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary table\n",
    "correlation_summary = pd.DataFrame([\n",
    "    {\n",
    "        'Crime Type': 'Total Crime',\n",
    "        'Pearson r': f\"{total_crime_corr['pearson_r']:.4f}\",\n",
    "        'p-value': f\"{total_crime_corr['pearson_p']:.2e}\",\n",
    "        'Spearman \u03c1': f\"{total_crime_corr['spearman_r']:.4f}\",\n",
    "        'R\u00b2': f\"{total_crime_corr['regression_r_squared']:.4f}\",\n",
    "        'Effect Size': total_crime_corr['effect_size_strength'].title()\n",
    "    },\n",
    "    {\n",
    "        'Crime Type': 'Violent Crime',\n",
    "        'Pearson r': f\"{violent_crime_corr['pearson_r']:.4f}\",\n",
    "        'p-value': f\"{violent_crime_corr['pearson_p']:.2e}\",\n",
    "        'Spearman \u03c1': f\"{violent_crime_corr['spearman_r']:.4f}\",\n",
    "        'R\u00b2': f\"{violent_crime_corr['regression_r_squared']:.4f}\",\n",
    "        'Effect Size': violent_crime_corr['effect_size_strength'].title()\n",
    "    },\n",
    "    {\n",
    "        'Crime Type': 'Property Crime',\n",
    "        'Pearson r': f\"{property_crime_corr['pearson_r']:.4f}\",\n",
    "        'p-value': f\"{property_crime_corr['pearson_p']:.2e}\",\n",
    "        'Spearman \u03c1': f\"{property_crime_corr['spearman_r']:.4f}\",\n",
    "        'R\u00b2': f\"{property_crime_corr['regression_r_squared']:.4f}\",\n",
    "        'Effect Size': property_crime_corr['effect_size_strength'].title()\n",
    "    }\n",
    "])\n",
    "\n",
    "print(\"\\nSummary of Temperature-Crime Correlations:\")\n",
    "print(correlation_summary.to_string(index=False))\n",
    "\n",
    "# Store for later export\n",
    "correlation_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Visualization: Scatter Plots with Trend Lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scatter plots for each crime type\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Total Crime\n",
    "axes[0].scatter(merged_df['temp'], merged_df['total_crimes'], alpha=0.3, s=10)\n",
    "z = np.polyfit(merged_df['temp'].dropna(), merged_df['total_crimes'].dropna(), 1)\n",
    "p = np.poly1d(z)\n",
    "axes[0].plot(merged_df['temp'].sort_values(), p(merged_df['temp'].sort_values()), \n",
    "             'r-', linewidth=2, label=f'Trend (r={total_crime_corr[\"pearson_r\"]:.3f})')\n",
    "axes[0].set_xlabel('Temperature (\u00b0C)', fontsize=12)\n",
    "axes[0].set_ylabel('Total Daily Crimes', fontsize=12)\n",
    "axes[0].set_title('Temperature vs. Total Crime', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Violent Crime\n",
    "axes[1].scatter(merged_df['temp'], merged_df['Violent'], alpha=0.3, s=10, color='red')\n",
    "z = np.polyfit(merged_df['temp'].dropna(), merged_df['Violent'].dropna(), 1)\n",
    "p = np.poly1d(z)\n",
    "axes[1].plot(merged_df['temp'].sort_values(), p(merged_df['temp'].sort_values()), \n",
    "             'darkred', linewidth=2, label=f'Trend (r={violent_crime_corr[\"pearson_r\"]:.3f})')\n",
    "axes[1].set_xlabel('Temperature (\u00b0C)', fontsize=12)\n",
    "axes[1].set_ylabel('Daily Violent Crimes', fontsize=12)\n",
    "axes[1].set_title('Temperature vs. Violent Crime', fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "# Property Crime\n",
    "axes[2].scatter(merged_df['temp'], merged_df['Property'], alpha=0.3, s=10, color='green')\n",
    "z = np.polyfit(merged_df['temp'].dropna(), merged_df['Property'].dropna(), 1)\n",
    "p = np.poly1d(z)\n",
    "axes[2].plot(merged_df['temp'].sort_values(), p(merged_df['temp'].sort_values()), \n",
    "             'darkgreen', linewidth=2, label=f'Trend (r={property_crime_corr[\"pearson_r\"]:.3f})')\n",
    "axes[2].set_xlabel('Temperature (\u00b0C)', fontsize=12)\n",
    "axes[2].set_ylabel('Daily Property Crimes', fontsize=12)\n",
    "axes[2].set_title('Temperature vs. Property Crime', fontsize=14, fontweight='bold')\n",
    "axes[2].legend()\n",
    "axes[2].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(REPORTS_DIR / 'heat_crime_scatterplots.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"Saved: {REPORTS_DIR / 'heat_crime_scatterplots.png'}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 Temperature Bins Analysis\n",
    "\n",
    "Examine crime rates across different temperature ranges to identify potential thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temperature bins\n",
    "merged_df['temp_bin'] = pd.cut(merged_df['temp'], \n",
    "                                bins=[-20, 0, 10, 20, 30, 40],\n",
    "                                labels=['<0\u00b0C', '0-10\u00b0C', '10-20\u00b0C', '20-30\u00b0C', '>30\u00b0C'])\n",
    "\n",
    "# Calculate mean crime rates by temperature bin\n",
    "temp_bin_summary = merged_df.groupby('temp_bin', observed=True).agg({\n",
    "    'total_crimes': ['mean', 'std', 'count'],\n",
    "    'Violent': ['mean', 'std'],\n",
    "    'Property': ['mean', 'std']\n",
    "}).round(2)\n",
    "\n",
    "print(\"\\nCrime Rates by Temperature Range:\")\n",
    "print(temp_bin_summary)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Plot bars for each crime type\n",
    "temp_bins = merged_df.groupby('temp_bin', observed=True).agg({\n",
    "    'total_crimes': 'mean',\n",
    "    'Violent': 'mean',\n",
    "    'Property': 'mean'\n",
    "})\n",
    "\n",
    "temp_bins['total_crimes'].plot(kind='bar', ax=axes[0], color='skyblue')\n",
    "axes[0].set_title('Avg. Total Crime by Temperature', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('Average Daily Crimes', fontsize=12)\n",
    "axes[0].set_xlabel('Temperature Range', fontsize=12)\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "axes[0].grid(alpha=0.3, axis='y')\n",
    "\n",
    "temp_bins['Violent'].plot(kind='bar', ax=axes[1], color='red', alpha=0.7)\n",
    "axes[1].set_title('Avg. Violent Crime by Temperature', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('Average Daily Violent Crimes', fontsize=12)\n",
    "axes[1].set_xlabel('Temperature Range', fontsize=12)\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "axes[1].grid(alpha=0.3, axis='y')\n",
    "\n",
    "temp_bins['Property'].plot(kind='bar', ax=axes[2], color='green', alpha=0.7)\n",
    "axes[2].set_title('Avg. Property Crime by Temperature', fontsize=14, fontweight='bold')\n",
    "axes[2].set_ylabel('Average Daily Property Crimes', fontsize=12)\n",
    "axes[2].set_xlabel('Temperature Range', fontsize=12)\n",
    "axes[2].tick_params(axis='x', rotation=45)\n",
    "axes[2].grid(alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(REPORTS_DIR / 'heat_crime_by_temperature_bins.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"Saved: {REPORTS_DIR / 'heat_crime_by_temperature_bins.png'}\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}