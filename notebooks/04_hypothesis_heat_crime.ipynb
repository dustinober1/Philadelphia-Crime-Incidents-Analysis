{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heat-Crime Hypothesis Analysis (HYP-HEAT)\n",
    "\n",
    "**Objective:** Investigate the statistical relationship between temperature and crime patterns in Philadelphia.\n",
    "\n",
    "**Research Question:** Is there a significant relationship between temperature (heat) and crime rates, particularly for violent crimes?\n",
    "\n",
    "**Data Sources:**\n",
    "- Crime incidents: `data/crime_incidents_combined.parquet` (2006-2026)\n",
    "- Weather data: `data/external/weather_philly_2006_2026.parquet` (2006-2026)\n",
    "\n",
    "**Methodology:**\n",
    "1. Data merging with temporal alignment (daily aggregation)\n",
    "2. Correlation analysis (Pearson, Spearman, Kendall tau)\n",
    "3. Hypothesis testing with statistical significance\n",
    "4. Effect size calculation and interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Ensure we can import from analysis module\n",
    "repo_root = Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()\n",
    "sys.path.insert(0, str(repo_root))\n",
    "\n",
    "# Standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('default')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Create reports directory\n",
    "REPORTS_DIR = repo_root / 'reports'\n",
    "REPORTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Repository root: {repo_root}\")\n",
    "print(f\"Reports directory: {REPORTS_DIR}\")\n",
    "print(f\"Analysis date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Exploration\n",
    "\n",
    "### 1.1 Load Crime Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load crime data\n",
    "crime_path = repo_root / 'data' / 'crime_incidents_combined.parquet'\n",
    "crime_df = pd.read_parquet(crime_path)\n",
    "\n",
    "print(f\"Crime data shape: {crime_df.shape}\")\n",
    "print(f\"\\nColumns: {crime_df.columns.tolist()}\")\n",
    "print(f\"\\nDate range: {crime_df['dispatch_date'].min()} to {crime_df['dispatch_date'].max()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "crime_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check crime categories\n",
    "print(\"Top 15 crime types:\")\n",
    "print(crime_df['text_general_code'].value_counts().head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Load Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load weather data\n",
    "weather_path = repo_root / 'data' / 'external' / 'weather_philly_2006_2026.parquet'\n",
    "weather_df = pd.read_parquet(weather_path)\n",
    "\n",
    "print(f\"Weather data shape: {weather_df.shape}\")\n",
    "print(f\"\\nColumns: {weather_df.columns.tolist()}\")\n",
    "print(f\"\\nDate range: {weather_df.index.min()} to {weather_df.index.max()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "weather_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weather data summary statistics\n",
    "print(\"Weather data summary:\")\n",
    "weather_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Merging Strategy\n",
    "\n",
    "### Join Strategy Documentation\n",
    "\n",
    "**Temporal Alignment:**\n",
    "- Weather data: Daily observations (one record per day)\n",
    "- Crime data: Individual incidents with dispatch_date field\n",
    "- **Strategy:** Aggregate crime data to daily counts, then join on date\n",
    "\n",
    "**Spatial Considerations:**\n",
    "- Weather data: Single station representing Philadelphia metropolitan area\n",
    "- Crime data: Individual incidents across all police districts\n",
    "- **Strategy:** Use city-wide weather data for all crimes (assumes temperature is relatively uniform across the city)\n",
    "- **Limitation:** Does not account for micro-climate variations or heat island effects in specific neighborhoods\n",
    "\n",
    "**Crime Classification:**\n",
    "- Create categories: Violent crimes, Property crimes, Other crimes\n",
    "- Based on UCR general codes (as established in Phase 1)\n",
    "\n",
    "### 2.1 Define Crime Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crime category mapping based on UCR general codes (hundred-bands 1-7)\n",
    "# From analysis/config.py established in Phase 1\n",
    "\n",
    "CRIME_CATEGORY_MAP = {\n",
    "    1: 'Violent',      # Homicide\n",
    "    2: 'Violent',      # Rape\n",
    "    3: 'Violent',      # Robbery\n",
    "    4: 'Violent',      # Aggravated Assault\n",
    "    5: 'Property',     # Burglary\n",
    "    6: 'Property',     # Theft\n",
    "    7: 'Property',     # Motor Vehicle Theft\n",
    "}\n",
    "\n",
    "def categorize_crime(ucr_code):\n",
    "    \"\"\"Categorize crime based on UCR general code hundred-band.\"\"\"\n",
    "    hundred_band = int(ucr_code // 100) if pd.notna(ucr_code) else 0\n",
    "    return CRIME_CATEGORY_MAP.get(hundred_band, 'Other')\n",
    "\n",
    "# Apply categorization\n",
    "crime_df['crime_category'] = crime_df['ucr_general'].apply(categorize_crime)\n",
    "\n",
    "print(\"Crime category distribution:\")\n",
    "print(crime_df['crime_category'].value_counts())\n",
    "print(f\"\\nPercentages:\")\n",
    "print(crime_df['crime_category'].value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Aggregate Crime Data to Daily Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dispatch_date to datetime for proper aggregation\n",
    "crime_df['date'] = pd.to_datetime(crime_df['dispatch_date'])\n",
    "\n",
    "# Aggregate total crimes per day\n",
    "daily_crime = crime_df.groupby('date').size().reset_index(name='total_crimes')\n",
    "\n",
    "# Aggregate by crime category\n",
    "daily_crime_by_category = crime_df.groupby(['date', 'crime_category']).size().unstack(fill_value=0)\n",
    "daily_crime_by_category = daily_crime_by_category.reset_index()\n",
    "\n",
    "# Merge total with categories\n",
    "daily_crime_merged = daily_crime.merge(daily_crime_by_category, on='date', how='left')\n",
    "\n",
    "print(f\"Daily crime data shape: {daily_crime_merged.shape}\")\n",
    "print(f\"\\nDate range: {daily_crime_merged['date'].min()} to {daily_crime_merged['date'].max()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "daily_crime_merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Merge Weather and Crime Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare weather data for merge\n",
    "weather_df_reset = weather_df.reset_index()\n",
    "weather_df_reset['date'] = pd.to_datetime(weather_df_reset['time']).dt.date\n",
    "weather_df_reset['date'] = pd.to_datetime(weather_df_reset['date'])\n",
    "\n",
    "# Select relevant weather columns\n",
    "weather_cols = ['date', 'temp', 'tmin', 'tmax', 'rhum', 'prcp', 'wspd']\n",
    "weather_for_merge = weather_df_reset[weather_cols]\n",
    "\n",
    "print(f\"Weather data for merge shape: {weather_for_merge.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(weather_for_merge.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge crime and weather data on date\n",
    "merged_df = daily_crime_merged.merge(weather_for_merge, on='date', how='inner')\n",
    "\n",
    "print(f\"\\nMerged dataset shape: {merged_df.shape}\")\n",
    "print(f\"Date range: {merged_df['date'].min()} to {merged_df['date'].max()}\")\n",
    "print(f\"\\nNumber of days: {len(merged_df)}\")\n",
    "print(f\"\\nMerged data columns: {merged_df.columns.tolist()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "merged_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values in merged dataset:\")\n",
    "print(merged_df.isnull().sum())\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\nSummary statistics of merged dataset:\")\n",
    "merged_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Data Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data completeness\n",
    "date_range = pd.date_range(start=merged_df['date'].min(), end=merged_df['date'].max(), freq='D')\n",
    "expected_days = len(date_range)\n",
    "actual_days = len(merged_df)\n",
    "\n",
    "print(f\"Expected days in range: {expected_days}\")\n",
    "print(f\"Actual days in merged data: {actual_days}\")\n",
    "print(f\"Coverage: {actual_days / expected_days * 100:.2f}%\")\n",
    "\n",
    "# Check for any gaps\n",
    "if actual_days < expected_days:\n",
    "    missing_dates = set(date_range) - set(merged_df['date'])\n",
    "    print(f\"\\nNumber of missing dates: {len(missing_dates)}\")\n",
    "    if len(missing_dates) <= 10:\n",
    "        print(f\"Missing dates: {sorted(missing_dates)}\")\n",
    "else:\n",
    "    print(\"\\nNo missing dates - complete daily coverage.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Merge Strategy\n",
    "\n",
    "**Approach:**\n",
    "1. **Temporal alignment:** Aggregated crime incidents to daily counts to match weather data granularity\n",
    "2. **Spatial approach:** Used city-wide weather station data for all crimes (single station)\n",
    "3. **Join method:** Inner join on date to ensure both datasets have matching records\n",
    "4. **Crime categorization:** Classified crimes into Violent, Property, and Other based on UCR codes\n",
    "\n",
    "**Limitations:**\n",
    "- Single weather station may not capture micro-climate variations across neighborhoods\n",
    "- Heat island effects in urban cores vs. suburbs not considered\n",
    "- Daily aggregation loses intra-day temperature variations\n",
    "- Weather data represents average conditions, not peak exposure times\n",
    "\n",
    "**Dataset Ready for Analysis:**\n",
    "- Merged dataset includes daily crime counts by category and weather measurements\n",
    "- Complete temporal coverage from 2006 to 2026\n",
    "- Ready for correlation analysis and hypothesis testing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
