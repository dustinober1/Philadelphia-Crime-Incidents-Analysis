{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 02: Exploratory Data Analysis\n",
    "\n",
    "**Purpose:** Comprehensive exploratory analysis of Philadelphia crime incidents (2006-2026)\n",
    "\n",
    "**Objectives:**\n",
    "- Document univariate distributions for all key variables\n",
    "- Analyze missing value patterns and their implications\n",
    "- Identify initial variable relationships and correlations\n",
    "- Generate testable hypotheses for downstream analysis\n",
    "- Produce publication-quality visualizations and summary statistics\n",
    "\n",
    "**Data Source:** `data/processed/crime_incidents_cleaned.parquet` (Phase 1 output)\n",
    "\n",
    "**Methodology:** Follows 02-RESEARCH.md Pattern 5 for publication-quality figures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard data manipulation libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib import rcParams\n",
    "import seaborn as sns\n",
    "\n",
    "# Statistical libraries\n",
    "from scipy import stats\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Configuration\n",
    "import sys\n",
    "sys.path.insert(0, '../scripts')\n",
    "import config\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure matplotlib for publication-quality figures (Pattern 5 from 02-RESEARCH.md)\n",
    "rcParams.update({\n",
    "    'figure.dpi': 300,\n",
    "    'savefig.dpi': 300,\n",
    "    'font.size': 10,\n",
    "    'axes.labelsize': 11,\n",
    "    'axes.titlesize': 12,\n",
    "    'xtick.labelsize': 9,\n",
    "    'ytick.labelsize': 9,\n",
    "    'legend.fontsize': 9,\n",
    "    'figure.figsize': (12, 8),\n",
    "})\n",
    "\n",
    "# Use colorblind-friendly palettes\n",
    "sns.set_palette('viridis')\n",
    "\n",
    "# Set up output directories\n",
    "OUTPUT_FIGURES_DIR = config.FIGURES_DIR / 'exploratory'\n",
    "OUTPUT_TABLES_DIR = config.TABLES_DIR / 'exploratory'\n",
    "OUTPUT_FIGURES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUTPUT_TABLES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"✓ Output directories configured:\")\n",
    "print(f\"  Figures: {OUTPUT_FIGURES_DIR}\")\n",
    "print(f\"  Tables: {OUTPUT_TABLES_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Initial Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleaned dataset from Phase 1\n",
    "df = pd.read_parquet(config.PROCESSED_DATA_DIR / 'crime_incidents_cleaned.parquet')\n",
    "\n",
    "# Document basic data characteristics\n",
    "print(\"=== Dataset Overview ===\")\n",
    "print(f\"Total records: {len(df):,}\")\n",
    "print(f\"Total columns: {len(df.columns)}\")\n",
    "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "print(f\"\\nDate range: {df[config.COL_DATE].min()} to {df[config.COL_DATE].max()}\")\n",
    "print(f\"Years covered: {df['year'].min()} - {df['year'].max()}\")\n",
    "\n",
    "print(\"\\n=== Column Summary ===\")\n",
    "for col in df.columns:\n",
    "    dtype = df[col].dtype\n",
    "    non_null = df[col].notna().sum()\n",
    "    null_pct = (df[col].isna().sum() / len(df)) * 100\n",
    "    print(f\"{col:25s} | {str(dtype):15s} | {non_null:>10,} | {null_pct:>5.1f}% null\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply reporting lag exclusion (per Phase 1 decision: exclude last 30 days)\n",
    "max_date = df[config.COL_DATE].max()\n",
    "cutoff_date = max_date - timedelta(days=30)\n",
    "\n",
    "print(f\"Maximum date in dataset: {max_date.strftime('%Y-%m-%d')}\")\n",
    "print(f\"Exclusion cutoff (30-day lag): {cutoff_date.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "# Create analysis dataset excluding recent data\n",
    "df_analysis = df[df[config.COL_DATE] <= cutoff_date].copy()\n",
    "excluded_count = len(df) - len(df_analysis)\n",
    "\n",
    "print(f\"\\nRecords excluded (last 30 days): {excluded_count:,}\")\n",
    "print(f\"Records for analysis: {len(df_analysis):,}\")\n",
    "print(f\"Analysis date range: {df_analysis[config.COL_DATE].min().strftime('%Y-%m-%d')} to {df_analysis[config.COL_DATE].max().strftime('%Y-%m-%d')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create derived time features for analysis\n",
    "df_analysis['month'] = df_analysis[config.COL_DATE].dt.month\n",
    "df_analysis['day_of_week'] = df_analysis[config.COL_DATE].dt.dayofweek  # 0=Monday\n",
    "df_analysis['day_name'] = df_analysis[config.COL_DATE].dt.day_name()\n",
    "df_analysis['hour'] = df_analysis[config.COL_DATE].dt.hour\n",
    "\n",
    "# Create offense severity classification based on UCR codes\n",
    "# Note: These mappings will be refined based on actual data exploration\n",
    "def classify_offense(ucr_code):\n",
    "    \"\"\"Classify offense by UCR general code.\"\"\"\n",
    "    if ucr_code in config.UCR_VIOLENT:\n",
    "        return 'Violent'\n",
    "    elif ucr_code in config.UCR_PROPERTY:\n",
    "        return 'Property'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "df_analysis['offense_category'] = df_analysis[config.COL_UCR_GENERAL].apply(classify_offense)\n",
    "\n",
    "print(\"✓ Derived features created\")\n",
    "print(f\"\\nOffense category distribution:\")\n",
    "print(df_analysis['offense_category'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Missing Value Analysis\n",
    "\n",
    "Document missing value patterns to understand data quality and potential biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate missing value statistics\n",
    "missing_stats = pd.DataFrame({\n",
    "    'column': df_analysis.columns,\n",
    "    'missing_count': df_analysis.isna().sum(),\n",
    "    'missing_pct': (df_analysis.isna().sum() / len(df_analysis)) * 100,\n",
    "    'dtype': df_analysis.dtypes\n",
    "}).sort_values('missing_pct', ascending=False)\n",
    "\n",
    "print(\"=== Missing Value Summary ===\")\n",
    "print(missing_stats.to_string(index=False))\n",
    "\n",
    "# Save missing value summary\n",
    "missing_stats.to_csv(OUTPUT_TABLES_DIR / 'missing_value_summary.csv', index=False)\n",
    "print(f\"\\n✓ Missing value summary saved to {OUTPUT_TABLES_DIR / 'missing_value_summary.csv'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create missing value heatmap\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "# Select columns with missing values for visualization\n",
    "cols_with_missing = missing_stats[missing_stats['missing_pct'] > 0]['column'].tolist()\n",
    "\n",
    "if cols_with_missing:\n",
    "    # Sample data for visualization (heatmap on full dataset is too large)\n",
    "    sample_size = min(10000, len(df_analysis))\n",
    "    df_sample = df_analysis.sample(n=sample_size, random_state=42)\n",
    "    \n",
    "    # Create binary missing indicator matrix\n",
    "    missing_matrix = df_sample[cols_with_missing].isna().astype(int)\n",
    "    \n",
    "    # Plot heatmap\n",
    "    sns.heatmap(missing_matrix.T, \n",
    "                cmap='RdYlBu_r',\n",
    "                cbar_kws={'label': 'Missing (1) / Present (0)'},\n",
    "                xticklabels=False,\n",
    "                ax=ax)\n",
    "    \n",
    "    ax.set_title('Missing Value Patterns (Sample of 10,000 Records)', fontweight='bold', pad=15)\n",
    "    ax.set_xlabel('Record Index', fontweight='bold')\n",
    "    ax.set_ylabel('Variables', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    fig.savefig(OUTPUT_FIGURES_DIR / 'missing_values_heatmap.png', \n",
    "                dpi=300, bbox_inches='tight', facecolor='white')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n✓ Missing value heatmap saved\")\n",
    "else:\n",
    "    print(\"No missing values detected in dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze missing value co-occurrence patterns\n",
    "print(\"=== Missing Value Co-occurrence Analysis ===\\n\")\n",
    "\n",
    "# Check if missing coordinates correlate with other missing values\n",
    "if df_analysis[config.COL_LAT].isna().any():\n",
    "    missing_coords = df_analysis[config.COL_LAT].isna()\n",
    "    print(f\"Records with missing coordinates: {missing_coords.sum():,} ({missing_coords.mean()*100:.2f}%)\")\n",
    "    \n",
    "    # Check if missing coords correlate with specific offense types\n",
    "    missing_by_offense = df_analysis[missing_coords]['text_general_code'].value_counts().head(10)\n",
    "    print(\"\\nTop offense types with missing coordinates:\")\n",
    "    print(missing_by_offense)\n",
    "    \n",
    "    # Check temporal pattern of missing coords\n",
    "    missing_by_year = df_analysis[missing_coords].groupby('year').size()\n",
    "    total_by_year = df_analysis.groupby('year').size()\n",
    "    missing_rate_by_year = (missing_by_year / total_by_year * 100).fillna(0)\n",
    "    \n",
    "    print(\"\\nMissing coordinate rate by year:\")\n",
    "    for year, rate in missing_rate_by_year.items():\n",
    "        print(f\"  {year}: {rate:.2f}%\")\n",
    "\n",
    "# Check other missing patterns\n",
    "for col in ['psa', 'hour', 'dc_key']:\n",
    "    if df_analysis[col].isna().any():\n",
    "        missing_count = df_analysis[col].isna().sum()\n",
    "        missing_pct = missing_count / len(df_analysis) * 100\n",
    "        print(f\"\\n{col}: {missing_count:,} missing ({missing_pct:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Univariate Distributions: Temporal Variables\n",
    "\n",
    "Analyze the distribution of incidents across time dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temporal distribution analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Distribution by Year\n",
    "yearly_counts = df_analysis.groupby('year').size()\n",
    "axes[0, 0].bar(yearly_counts.index, yearly_counts.values, color='steelblue', alpha=0.8)\n",
    "axes[0, 0].set_xlabel('Year', fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Number of Incidents', fontweight='bold')\n",
    "axes[0, 0].set_title('Crime Incidents by Year', fontweight='bold')\n",
    "axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add trend line\n",
    "z = np.polyfit(yearly_counts.index, yearly_counts.values, 1)\n",
    "p = np.poly1d(z)\n",
    "axes[0, 0].plot(yearly_counts.index, p(yearly_counts.index), \"r--\", alpha=0.8, linewidth=2, label=f'Trend: {z[0]:+.0f}/year')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# 2. Distribution by Month (aggregated across all years)\n",
    "monthly_counts = df_analysis.groupby('month').size()\n",
    "month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "axes[0, 1].bar(range(1, 13), monthly_counts.values, color='forestgreen', alpha=0.8)\n",
    "axes[0, 1].set_xticks(range(1, 13))\n",
    "axes[0, 1].set_xticklabels(month_names)\n",
    "axes[0, 1].set_xlabel('Month', fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Number of Incidents', fontweight='bold')\n",
    "axes[0, 1].set_title('Crime Incidents by Month (All Years Combined)', fontweight='bold')\n",
    "axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Highlight summer months\n",
    "for month in [6, 7, 8]:\n",
    "    axes[0, 1].bar(month, monthly_counts[month], color='orange', alpha=0.8)\n",
    "\n",
    "# 3. Distribution by Day of Week\n",
    "day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "daily_counts = df_analysis.groupby('day_name').size().reindex(day_order)\n",
    "colors = ['steelblue' if day in ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday'] else 'coral' \n",
    "          for day in day_order]\n",
    "axes[1, 0].bar(range(7), daily_counts.values, color=colors, alpha=0.8)\n",
    "axes[1, 0].set_xticks(range(7))\n",
    "axes[1, 0].set_xticklabels([d[:3] for d in day_order])\n",
    "axes[1, 0].set_xlabel('Day of Week', fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Number of Incidents', fontweight='bold')\n",
    "axes[1, 0].set_title('Crime Incidents by Day of Week', fontweight='bold')\n",
    "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add legend\n",
    "weekday_patch = mpatches.Patch(color='steelblue', alpha=0.8, label='Weekday')\n",
    "weekend_patch = mpatches.Patch(color='coral', alpha=0.8, label='Weekend')\n",
    "axes[1, 0].legend(handles=[weekday_patch, weekend_patch])\n",
    "\n",
    "# 4. Distribution by Hour\n",
    "hourly_counts = df_analysis.groupby('hour').size()\n",
    "axes[1, 1].plot(hourly_counts.index, hourly_counts.values, color='purple', linewidth=2, marker='o', markersize=4)\n",
    "axes[1, 1].fill_between(hourly_counts.index, hourly_counts.values, alpha=0.3, color='purple')\n",
    "axes[1, 1].set_xlabel('Hour of Day', fontweight='bold')\n",
    "axes[1, 1].set_ylabel('Number of Incidents', fontweight='bold')\n",
    "axes[1, 1].set_title('Crime Incidents by Hour of Day', fontweight='bold')\n",
    "axes[1, 1].set_xticks(range(0, 24, 2))\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Highlight peak hours\n",
    "peak_hour = hourly_counts.idxmax()\n",
    "axes[1, 1].axvline(x=peak_hour, color='red', linestyle='--', alpha=0.7, label=f'Peak: {peak_hour}:00')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig(OUTPUT_FIGURES_DIR / 'temporal_distributions.png', \n",
    "            dpi=300, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Temporal distributions figure saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate temporal summary statistics\n",
    "temporal_stats = pd.DataFrame({\n",
    "    'Temporal Dimension': ['Year', 'Month', 'Day of Week', 'Hour'],\n",
    "    'Mean': [\n",
    "        yearly_counts.mean(),\n",
    "        monthly_counts.mean(),\n",
    "        daily_counts.mean(),\n",
    "        hourly_counts.mean()\n",
    "    ],\n",
    "    'Std Dev': [\n",
    "        yearly_counts.std(),\n",
    "        monthly_counts.std(),\n",
    "        daily_counts.std(),\n",
    "        hourly_counts.std()\n",
    "    ],\n",
    "    'Min': [\n",
    "        yearly_counts.min(),\n",
    "        monthly_counts.min(),\n",
    "        daily_counts.min(),\n",
    "        hourly_counts.min()\n",
    "    ],\n",
    "    'Max': [\n",
    "        yearly_counts.max(),\n",
    "        monthly_counts.max(),\n",
    "        daily_counts.max(),\n",
    "        hourly_counts.max()\n",
    "    ],\n",
    "    'CV (%)': [\n",
    "        yearly_counts.std() / yearly_counts.mean() * 100,\n",
    "        monthly_counts.std() / monthly_counts.mean() * 100,\n",
    "        daily_counts.std() / daily_counts.mean() * 100,\n",
    "        hourly_counts.std() / hourly_counts.mean() * 100\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"=== Temporal Distribution Summary Statistics ===\")\n",
    "print(temporal_stats.round(2).to_string(index=False))\n",
    "\n",
    "# Key insights\n",
    "print(\"\\n=== Key Temporal Insights ===\")\n",
    "print(f\"• Yearly trend: {z[0]:+.0f} incidents per year\")\n",
    "summer_avg = monthly_counts[[6, 7, 8]].mean()\n",
    "winter_avg = monthly_counts[[12, 1, 2]].mean()\n",
    "seasonal_diff = (summer_avg - winter_avg) / winter_avg * 100\n",
    "print(f\"• Summer vs Winter difference: {seasonal_diff:+.1f}%\")\n",
    "weekend_avg = daily_counts[['Saturday', 'Sunday']].mean()\n",
    "weekday_avg = daily_counts[['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday']].mean()\n",
    "weekend_diff = (weekend_avg - weekday_avg) / weekday_avg * 100\n",
    "print(f\"• Weekend vs Weekday difference: {weekend_diff:+.1f}%\")\n",
    "print(f\"• Peak hour: {peak_hour}:00 ({hourly_counts.max():,} incidents)\")\n",
    "print(f\"• Lowest hour: {hourly_counts.idxmin()}:00 ({hourly_counts.min():,} incidents)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Univariate Distributions: Geographic Variables\n",
    "\n",
    "Analyze the spatial distribution of crime incidents across Philadelphia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# District distribution analysis\n",
    "district_counts = df_analysis.groupby(config.COL_DISTRICT).size().sort_values(ascending=False)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
    "\n",
    "# 1. District bar chart\n",
    "axes[0].barh(range(len(district_counts)), district_counts.values, color='teal', alpha=0.8)\n",
    "axes[0].set_yticks(range(len(district_counts)))\n",
    "axes[0].set_yticklabels([f'District {d}' for d in district_counts.index])\n",
    "axes[0].set_xlabel('Number of Incidents', fontweight='bold')\n",
    "axes[0].set_ylabel('Police District', fontweight='bold')\n",
    "axes[0].set_title('Crime Incidents by Police District', fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3, axis='x')\n",
    "axes[0].invert_yaxis()\n",
    "\n",
    "# Add value labels\n",
    "for i, v in enumerate(district_counts.values):\n",
    "    axes[0].text(v + 1000, i, f'{v:,}', va='center', fontsize=8)\n",
    "\n",
    "# 2. Geographic scatter plot (sample for performance)\n",
    "df_geo = df_analysis.dropna(subset=[config.COL_LAT, config.COL_LON])\n",
    "sample_size = min(50000, len(df_geo))\n",
    "df_sample = df_geo.sample(n=sample_size, random_state=42)\n",
    "\n",
    "scatter = axes[1].scatter(\n",
    "    df_sample[config.COL_LON], \n",
    "    df_sample[config.COL_LAT],\n",
    "    c=df_sample['year'],\n",
    "    cmap='viridis',\n",
    "    alpha=0.3,\n",
    "    s=1\n",
    ")\n",
    "axes[1].set_xlabel('Longitude', fontweight='bold')\n",
    "axes[1].set_ylabel('Latitude', fontweight='bold')\n",
    "axes[1].set_title(f'Geographic Distribution (Sample of {sample_size:,} Incidents)', fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(scatter, ax=axes[1])\n",
    "cbar.set_label('Year', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig(OUTPUT_FIGURES_DIR / 'district_distribution.png', \n",
    "            dpi=300, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Geographic distribution figures saved\")\n",
    "print(f\"\\nGeocoding coverage: {len(df_geo) / len(df_analysis) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate geographic summary statistics\n",
    "geo_stats = pd.DataFrame({\n",
    "    'District': district_counts.index,\n",
    "    'Total Incidents': district_counts.values,\n",
    "    'Percentage of Total': (district_counts.values / district_counts.sum() * 100).round(2)\n",
    "})\n",
    "\n",
    "print(\"=== Geographic Distribution Summary ===\")\n",
    "print(geo_stats.to_string(index=False))\n",
    "\n",
    "# Save district statistics\n",
    "geo_stats.to_csv(OUTPUT_TABLES_DIR / 'district_summary.csv', index=False)\n",
    "print(f\"\\n✓ District summary saved to {OUTPUT_TABLES_DIR / 'district_summary.csv'}\")\n",
    "\n",
    "# Key insights\n",
    "print(\"\\n=== Key Geographic Insights ===\")\n",
    "print(f\"• Highest volume district: District {district_counts.index[0]} ({district_counts.iloc[0]:,} incidents)\")\n",
    "print(f\"• Lowest volume district: District {district_counts.index[-1]} ({district_counts.iloc[-1]:,} incidents)\")\n",
    "ratio = district_counts.iloc[0] / district_counts.iloc[-1]\n",
    "print(f\"• Ratio (highest/lowest): {ratio:.1f}x\")\n",
    "print(f\"• Mean incidents per district: {district_counts.mean():.0f}\")\n",
    "print(f\"• Std dev: {district_counts.std():.0f} (CV: {district_counts.std()/district_counts.mean()*100:.1f}%)\")\n",
    "\n",
    "# Coordinate bounds\n",
    "print(f\"\\nCoordinate bounds (valid geocoded records):\")\n",
    "print(f\"• Latitude: {df_geo[config.COL_LAT].min():.4f} to {df_geo[config.COL_LAT].max():.4f}\")\n",
    "print(f\"• Longitude: {df_geo[config.COL_LON].min():.4f} to {df_geo[config.COL_LON].max():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Univariate Distributions: Offense Variables\n",
    "\n",
    "Analyze the distribution of crime by offense type and UCR classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UCR code distribution\n",
    "ucr_counts = df_analysis[config.COL_UCR_GENERAL].value_counts().sort_index()\n",
    "\n",
    "# Top offense descriptions\n",
    "offense_counts = df_analysis[config.COL_TEXT_GENERAL].value_counts().head(15)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 14))\n",
    "\n",
    "# 1. UCR Code distribution\n",
    "axes[0, 0].bar(ucr_counts.index.astype(str), ucr_counts.values, color='steelblue', alpha=0.8)\n",
    "axes[0, 0].set_xlabel('UCR General Code', fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Number of Incidents', fontweight='bold')\n",
    "axes[0, 0].set_title('Crime Incidents by UCR Code', fontweight='bold')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 2. Offense category pie chart\n",
    "category_counts = df_analysis['offense_category'].value_counts()\n",
    "colors_pie = ['#ff9999', '#66b3ff', '#99ff99']\n",
    "wedges, texts, autotexts = axes[0, 1].pie(\n",
    "    category_counts.values, \n",
    "    labels=category_counts.index,\n",
    "    autopct='%1.1f%%',\n",
    "    colors=colors_pie,\n",
    "    startangle=90\n",
    ")\n",
    "axes[0, 1].set_title('Distribution by Offense Category', fontweight='bold')\n",
    "\n",
    "# 3. Top 15 offense types\n",
    "axes[1, 0].barh(range(len(offense_counts)), offense_counts.values, color='forestgreen', alpha=0.8)\n",
    "axes[1, 0].set_yticks(range(len(offense_counts)))\n",
    "axes[1, 0].set_yticklabels([label[:40] + '...' if len(label) > 40 else label \n",
    "                             for label in offense_counts.index], fontsize=8)\n",
    "axes[1, 0].set_xlabel('Number of Incidents', fontweight='bold')\n",
    "axes[1, 0].set_title('Top 15 Offense Types', fontweight='bold')\n",
    "axes[1, 0].grid(True, alpha=0.3, axis='x')\n",
    "axes[1, 0].invert_yaxis()\n",
    "\n",
    "# 4. UCR distribution over time (top 5 codes)\n",
    "top_5_ucr = df_analysis[config.COL_UCR_GENERAL].value_counts().head(5).index\n",
    "for ucr in top_5_ucr:\n",
    "    yearly_ucr = df_analysis[df_analysis[config.COL_UCR_GENERAL] == ucr].groupby('year').size()\n",
    "    axes[1, 1].plot(yearly_ucr.index, yearly_ucr.values, marker='o', label=f'UCR {ucr}', linewidth=2)\n",
    "\n",
    "axes[1, 1].set_xlabel('Year', fontweight='bold')\n",
    "axes[1, 1].set_ylabel('Number of Incidents', fontweight='bold')\n",
    "axes[1, 1].set_title('Top 5 UCR Codes: Trends Over Time', fontweight='bold')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig(OUTPUT_FIGURES_DIR / 'offense_distributions.png', \n",
    "            dpi=300, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Offense distribution figures saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate offense summary statistics\n",
    "print(\"=== UCR Code Distribution ===\")\n",
    "ucr_stats = pd.DataFrame({\n",
    "    'UCR Code': ucr_counts.index,\n",
    "    'Count': ucr_counts.values,\n",
    "    'Percentage': (ucr_counts.values / ucr_counts.sum() * 100).round(2)\n",
    "})\n",
    "print(ucr_stats.to_string(index=False))\n",
    "\n",
    "print(\"\\n=== Top 15 Offense Descriptions ===\")\n",
    "offense_stats = pd.DataFrame({\n",
    "    'Offense Type': offense_counts.index,\n",
    "    'Count': offense_counts.values,\n",
    "    'Percentage': (offense_counts.values / len(df_analysis) * 100).round(2)\n",
    "})\n",
    "print(offense_stats.to_string(index=False))\n",
    "\n",
    "# Save offense statistics\n",
    "ucr_stats.to_csv(OUTPUT_TABLES_DIR / 'ucr_distribution.csv', index=False)\n",
    "offense_stats.to_csv(OUTPUT_TABLES_DIR / 'top_offenses.csv', index=False)\n",
    "print(f\"\\n✓ Offense statistics saved\")\n",
    "\n",
    "# Key insights\n",
    "print(\"\\n=== Key Offense Insights ===\")\n",
    "print(f\"• Most common UCR code: {ucr_counts.index[0]} ({ucr_counts.iloc[0]:,} incidents, {ucr_counts.iloc[0]/len(df_analysis)*100:.1f}%)\")\n",
    "print(f\"• Most common offense: {offense_counts.index[0]}\")\n",
    "print(f\"• Top 5 offenses account for {offense_counts.head(5).sum()/len(df_analysis)*100:.1f}% of all incidents\")\n",
    "print(f\"• Unique offense descriptions: {df_analysis[config.COL_TEXT_GENERAL].nunique()}\")\n",
    "print(f\"• Unique UCR codes: {df_analysis[config.COL_UCR_GENERAL].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Bivariate Analysis: Cross-Tabulations and Correlations\n",
    "\n",
    "Explore relationships between key variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cross-tabulation: District × Offense Category\n",
    "district_offense_crosstab = pd.crosstab(\n",
    "    df_analysis[config.COL_DISTRICT],\n",
    "    df_analysis['offense_category'],\n",
    "    normalize='index'\n",
    ") * 100\n",
    "\n",
    "# Create hour × day heatmap\n",
    "hourly_daily = df_analysis.groupby(['day_of_week', 'hour']).size().unstack(fill_value=0)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 14))\n",
    "\n",
    "# 1. District × Offense Category heatmap\n",
    "sns.heatmap(district_offense_crosstab, \n",
    "            annot=True, \n",
    "            fmt='.1f',\n",
    "            cmap='YlOrRd',\n",
    "            cbar_kws={'label': 'Percentage within District'},\n",
    "            ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Offense Category Distribution by District (%)', fontweight='bold', pad=15)\n",
    "axes[0, 0].set_xlabel('Offense Category', fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Police District', fontweight='bold')\n",
    "\n",
    "# 2. Hour × Day of Week heatmap\n",
    "day_labels = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "sns.heatmap(hourly_daily,\n",
    "            cmap='YlOrRd',\n",
    "            cbar_kws={'label': 'Number of Incidents'},\n",
    "            xticklabels=range(0, 24, 2),\n",
    "            yticklabels=day_labels,\n",
    "            ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Crime Incidents by Hour and Day of Week', fontweight='bold', pad=15)\n",
    "axes[0, 1].set_xlabel('Hour of Day', fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Day of Week', fontweight='bold')\n",
    "\n",
    "# 3. Year × Offense Category trends\n",
    "yearly_offense = df_analysis.groupby(['year', 'offense_category']).size().unstack(fill_value=0)\n",
    "for category in yearly_offense.columns:\n",
    "    axes[1, 0].plot(yearly_offense.index, yearly_offense[category], \n",
    "                    marker='o', label=category, linewidth=2)\n",
    "axes[1, 0].set_xlabel('Year', fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Number of Incidents', fontweight='bold')\n",
    "axes[1, 0].set_title('Crime Trends by Offense Category', fontweight='bold')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Monthly pattern by offense category\n",
    "monthly_offense = df_analysis.groupby(['month', 'offense_category']).size().unstack(fill_value=0)\n",
    "monthly_offense_pct = monthly_offense.div(monthly_offense.sum(axis=1), axis=0) * 100\n",
    "monthly_offense_pct.plot(kind='bar', stacked=True, ax=axes[1, 1], \n",
    "                         color=['#ff9999', '#66b3ff', '#99ff99'])\n",
    "axes[1, 1].set_xlabel('Month', fontweight='bold')\n",
    "axes[1, 1].set_ylabel('Percentage of Incidents', fontweight='bold')\n",
    "axes[1, 1].set_title('Monthly Distribution by Offense Category (%)', fontweight='bold')\n",
    "axes[1, 1].legend(title='Offense Category')\n",
    "axes[1, 1].set_xticklabels(['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n",
    "                           'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'], rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig(OUTPUT_FIGURES_DIR / 'bivariate_analysis.png', \n",
    "            dpi=300, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Bivariate analysis figures saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cross-tabulations\n",
    "district_offense_crosstab.to_csv(OUTPUT_TABLES_DIR / 'cross_tab_district_offense.csv')\n",
    "hourly_daily.to_csv(OUTPUT_TABLES_DIR / 'hour_day_crosstab.csv')\n",
    "yearly_offense.to_csv(OUTPUT_TABLES_DIR / 'year_offense_crosstab.csv')\n",
    "\n",
    "print(\"✓ Cross-tabulation tables saved\")\n",
    "\n",
    "# Statistical tests\n",
    "print(\"\\n=== Statistical Tests ===\")\n",
    "\n",
    "# Chi-square test: District vs Offense Category\n",
    "contingency_district_offense = pd.crosstab(\n",
    "    df_analysis[config.COL_DISTRICT],\n",
    "    df_analysis['offense_category']\n",
    ")\n",
    "chi2, p_val, dof, expected = chi2_contingency(contingency_district_offense)\n",
    "cramers_v = np.sqrt(chi2 / (len(df_analysis) * (min(contingency_district_offense.shape) - 1)))\n",
    "\n",
    "print(f\"\\nChi-square test: District vs Offense Category\")\n",
    "print(f\"  Chi-square statistic: {chi2:.2f}\")\n",
    "print(f\"  p-value: {p_val:.2e}\")\n",
    "print(f\"  Cramer's V (effect size): {cramers_v:.3f}\")\n",
    "print(f\"  Interpretation: {'Significant association' if p_val < 0.05 else 'No significant association'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Correlation Analysis\n",
    "\n",
    "Calculate and visualize correlations between numeric variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create numeric features for correlation analysis\n",
    "# Aggregate to monthly level for meaningful correlations\n",
    "monthly_features = df_analysis.groupby(['year', 'month']).agg({\n",
    "    'cartodb_id': 'count',  # Total incidents\n",
    "    config.COL_LAT: 'mean',  # Mean latitude\n",
    "    config.COL_LON: 'mean',  # Mean longitude\n",
    "    'hour': 'mean',  # Mean hour\n",
    "    config.COL_DISTRICT: lambda x: x.nunique(),  # Number of active districts\n",
    "}).rename(columns={\n",
    "    'cartodb_id': 'total_incidents',\n",
    "    config.COL_LAT: 'mean_lat',\n",
    "    config.COL_LON: 'mean_lon',\n",
    "    'hour': 'mean_hour',\n",
    "    config.COL_DISTRICT: 'active_districts'\n",
    "})\n",
    "\n",
    "# Add offense category counts\n",
    "offense_monthly = pd.crosstab(\n",
    "    [df_analysis['year'], df_analysis['month']],\n",
    "    df_analysis['offense_category']\n",
    ")\n",
    "monthly_features = monthly_features.join(offense_monthly, how='left')\n",
    "\n",
    "# Calculate correlation matrix\n",
    "corr_matrix = monthly_features.corr()\n",
    "\n",
    "# Create correlation heatmap\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "sns.heatmap(corr_matrix, \n",
    "            mask=mask,\n",
    "            annot=True, \n",
    "            fmt='.2f',\n",
    "            cmap='coolwarm',\n",
    "            center=0,\n",
    "            square=True,\n",
    "            cbar_kws={'label': 'Correlation Coefficient'},\n",
    "            ax=ax)\n",
    "ax.set_title('Monthly Feature Correlation Matrix', fontweight='bold', pad=15)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig(OUTPUT_FIGURES_DIR / 'correlation_matrix.png', \n",
    "            dpi=300, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "\n",
    "# Save correlation matrix\n",
    "corr_matrix.to_csv(OUTPUT_TABLES_DIR / 'correlation_matrix.csv')\n",
    "print(f\"✓ Correlation matrix saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify strongest correlations\n",
    "print(\"=== Strongest Correlations (|r| > 0.5) ===\")\n",
    "\n",
    "# Extract upper triangle of correlation matrix\n",
    "corr_pairs = []\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i+1, len(corr_matrix.columns)):\n",
    "        var1 = corr_matrix.columns[i]\n",
    "        var2 = corr_matrix.columns[j]\n",
    "        corr_val = corr_matrix.iloc[i, j]\n",
    "        if abs(corr_val) > 0.5:\n",
    "            corr_pairs.append((var1, var2, corr_val))\n",
    "\n",
    "# Sort by absolute correlation\n",
    "corr_pairs.sort(key=lambda x: abs(x[2]), reverse=True)\n",
    "\n",
    "for var1, var2, corr in corr_pairs:\n",
    "    direction = \"positive\" if corr > 0 else \"negative\"\n",
    "    strength = \"strong\" if abs(corr) > 0.7 else \"moderate\"\n",
    "    print(f\"• {var1} ↔ {var2}: r={corr:.3f} ({strength} {direction})\")\n",
    "\n",
    "if not corr_pairs:\n",
    "    print(\"No correlations with |r| > 0.5 found.\")\n",
    "\n",
    "# Save correlation pairs\n",
    "if corr_pairs:\n",
    "    corr_df = pd.DataFrame(corr_pairs, columns=['Variable 1', 'Variable 2', 'Correlation'])\n",
    "    corr_df.to_csv(OUTPUT_TABLES_DIR / 'strong_correlations.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary Statistics Compilation\n",
    "\n",
    "Compile comprehensive summary statistics for all variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive summary statistics\n",
    "summary_stats = pd.DataFrame({\n",
    "    'Variable': [],\n",
    "    'Count': [],\n",
    "    'Mean': [],\n",
    "    'Std': [],\n",
    "    'Min': [],\n",
    "    '25%': [],\n",
    "    '50%': [],\n",
    "    '75%': [],\n",
    "    'Max': [],\n",
    "    'Missing': [],\n",
    "    'Missing %': []\n",
    "})\n",
    "\n",
    "# Numeric variables summary\n",
    "numeric_cols = ['year', 'month', 'hour', config.COL_DISTRICT, config.COL_LAT, config.COL_LON]\n",
    "\n",
    "for col in numeric_cols:\n",
    "    if col in df_analysis.columns:\n",
    "        desc = df_analysis[col].describe()\n",
    "        missing = df_analysis[col].isna().sum()\n",
    "        missing_pct = missing / len(df_analysis) * 100\n",
    "        \n",
    "        summary_stats = pd.concat([summary_stats, pd.DataFrame({\n",
    "            'Variable': [col],\n",
    "            'Count': [int(desc['count'])],\n",
    "            'Mean': [desc['mean']],\n",
    "            'Std': [desc['std']],\n",
    "            'Min': [desc['min']],\n",
    "            '25%': [desc['25%']],\n",
    "            '50%': [desc['50%']],\n",
    "            '75%': [desc['75%']],\n",
    "            'Max': [desc['max']],\n",
    "            'Missing': [missing],\n",
    "            'Missing %': [missing_pct]\n",
    "        })], ignore_index=True)\n",
    "\n",
    "# Categorical variables summary\n",
    "cat_cols = ['offense_category', config.COL_TEXT_GENERAL]\n",
    "\n",
    "for col in cat_cols:\n",
    "    if col in df_analysis.columns:\n",
    "        unique = df_analysis[col].nunique()\n",
    "        mode = df_analysis[col].mode()[0] if not df_analysis[col].mode().empty else 'N/A'\n",
    "        mode_freq = df_analysis[col].value_counts().iloc[0] if not df_analysis[col].value_counts().empty else 0\n",
    "        missing = df_analysis[col].isna().sum()\n",
    "        missing_pct = missing / len(df_analysis) * 100\n",
    "        \n",
    "        summary_stats = pd.concat([summary_stats, pd.DataFrame({\n",
    "            'Variable': [col],\n",
    "            'Count': [len(df_analysis)],\n",
    "            'Mean': [f'Unique: {unique}'],\n",
    "            'Std': [f'Mode: {mode[:20]}...' if len(str(mode)) > 20 else f'Mode: {mode}'],\n",
    "            'Min': [f'Mode freq: {mode_freq}'],\n",
    "            '25%': ['N/A'],\n",
    "            '50%': ['N/A'],\n",
    "            '75%': ['N/A'],\n",
    "            'Max': ['N/A'],\n",
    "            'Missing': [missing],\n",
    "            'Missing %': [missing_pct]\n",
    "        })], ignore_index=True)\n",
    "\n",
    "print(\"=== Comprehensive Summary Statistics ===\")\n",
    "print(summary_stats.to_string(index=False))\n",
    "\n",
    "# Save summary statistics\n",
    "summary_stats.to_csv(OUTPUT_TABLES_DIR / 'summary_stats.csv', index=False)\n",
    "print(f\"\\n✓ Summary statistics saved to {OUTPUT_TABLES_DIR / 'summary_stats.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Hypothesis Generation\n",
    "\n",
    "Based on the exploratory analysis, document testable hypotheses for downstream investigation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document hypotheses based on observed patterns\n",
    "hypotheses = [\n",
    "    {\n",
    "        'id': 'H1',\n",
    "        'hypothesis': 'Violent crime shows stronger seasonal variation than property crime',\n",
    "        'rationale': 'Summer months show elevated incident counts; violent crimes may be more weather-dependent',\n",
    "        'test_method': 'Compare coefficient of variation by month across offense categories',\n",
    "        'data_needed': 'Monthly counts by offense category'\n",
    "    },\n",
    "    {\n",
    "        'id': 'H2',\n",
    "        'hypothesis': 'Certain districts have disproportionate violent crime rates relative to total incidents',\n",
    "        'rationale': 'District analysis shows varying offense mix patterns',\n",
    "        'test_method': 'Chi-square test for district-offense independence; standardized residuals',\n",
    "        'data_needed': 'District × Offense cross-tabulation'\n",
    "    },\n",
    "    {\n",
    "        'id': 'H3',\n",
    "        'hypothesis': 'Weekend crime patterns differ significantly from weekday patterns in timing',\n",
    "        'rationale': 'Hourly distributions may shift on weekends due to different activity patterns',\n",
    "        'test_method': 'Compare weekend vs weekday hourly distributions using KS test',\n",
    "        'data_needed': 'Hourly counts by day type'\n",
    "    },\n",
    "    {\n",
    "        'id': 'H4',\n",
    "        'hypothesis': 'Long-term crime trends vary by offense category',\n",
    "        'rationale': 'Different crime types may respond differently to social/economic factors',\n",
    "        'test_method': 'Separate trend analysis (linear regression) by offense category',\n",
    "        'data_needed': 'Annual counts by offense category'\n",
    "    },\n",
    "    {\n",
    "        'id': 'H5',\n",
    "        'hypothesis': 'Geographic clustering of crime types exists beyond random distribution',\n",
    "        'rationale': 'Certain offense types may concentrate in specific areas',\n",
    "        'test_method': 'Spatial autocorrelation (Moran I) by offense category',\n",
    "        'data_needed': 'Geocoded incidents by offense type'\n",
    "    },\n",
    "    {\n",
    "        'id': 'H6',\n",
    "        'hypothesis': 'Temporal patterns differ significantly between high-volume and low-volume districts',\n",
    "        'rationale': 'Districts with different baseline activity may have different temporal rhythms',\n",
    "        'test_method': 'Compare hourly patterns between top 5 and bottom 5 districts',\n",
    "        'data_needed': 'Hourly counts by district'\n",
    "    },\n",
    "    {\n",
    "        'id': 'H7',\n",
    "        'hypothesis': 'Missing coordinate patterns are non-random and correlate with offense type',\n",
    "        'rationale': 'Certain crimes may be more likely to have missing location data',\n",
    "        'test_method': 'Chi-square test: missing coords vs offense type',\n",
    "        'data_needed': 'Missing coordinate indicator by offense type'\n",
    "    },\n",
    "    {\n",
    "        'id': 'H8',\n",
    "        'hypothesis': 'Seasonal patterns have remained stable over the 20-year period',\n",
    "        'rationale': 'Fundamental seasonal drivers (weather, holidays) should be consistent',\n",
    "        'test_method': 'Compare monthly distributions across decade (2006-2015 vs 2016-2025)',\n",
    "        'data_needed': 'Monthly counts by year group'\n",
    "    },\n",
    "    {\n",
    "        'id': 'H9',\n",
    "        'hypothesis': 'Peak crime hours vary by offense category',\n",
    "        'rationale': 'Different crimes may occur at different times of day',\n",
    "        'test_method': 'Compare peak hours across offense categories',\n",
    "        'data_needed': 'Hourly distribution by offense category'\n",
    "    },\n",
    "    {\n",
    "        'id': 'H10',\n",
    "        'hypothesis': 'Reporting lag varies by offense severity',\n",
    "        'rationale': 'More serious crimes may be reported faster than minor incidents',\n",
    "        'test_method': 'Analyze dispatch_date vs report_date lag by offense category',\n",
    "        'data_needed': 'Date fields by offense category'\n",
    "    }\n",
    "]\n",
    "\n",
    "hypotheses_df = pd.DataFrame(hypotheses)\n",
    "print(\"=== Testable Hypotheses for Downstream Analysis ===\")\n",
    "print(hypotheses_df.to_string(index=False))\n",
    "\n",
    "# Save hypotheses\n",
    "hypotheses_df.to_csv(OUTPUT_TABLES_DIR / 'hypotheses.csv', index=False)\n",
    "print(f\"\\n✓ Hypotheses saved to {OUTPUT_TABLES_DIR / 'hypotheses.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Data Quality Flags and Recommendations\n",
    "\n",
    "Document anomalies and concerns for downstream notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify anomalies and data quality concerns\n",
    "print(\"=== Data Quality Flags ===\")\n",
    "\n",
    "flags = []\n",
    "\n",
    "# Check for outliers in yearly counts\n",
    "yearly_zscores = np.abs(stats.zscore(yearly_counts))\n",
    "outlier_years = yearly_counts[yearly_zscores > 2].index.tolist()\n",
    "if outlier_years:\n",
    "    flags.append(f\"Years with unusual counts (|z| > 2): {outlier_years}\")\n",
    "    print(f\"⚠ FLAG: Unusual yearly counts detected in years: {outlier_years}\")\n",
    "\n",
    "# Check for coordinate outliers\n",
",
    "if config.COL_LAT in df_analysis.columns and config.COL_LON in df_analysis.columns:\n",
    "    lat_q1, lat_q3 = df_analysis[config.COL_LAT].quantile([0.25, 0.75])\n",
    "    lon_q1, lon_q3 = df_analysis[config.COL_LON].quantile([0.25, 0.75])\n",
    "    lat_iqr = lat_q3 - lat_q1\n",
    "    lon_iqr = lon_q3 - lon_q1\n",
    "    \n",
    "    lat_outliers = df_analysis[\n",
    "        (df_analysis[config.COL_LAT] < lat_q1 - 1.5*lat_iqr) |\n",
    "        (df_analysis[config.COL_LAT] > lat_q3 + 1.5*lat_iqr)\n",
    "    ]\n",
    "    \n",
    "    if len(lat_outliers) > 0:\n",
    "        flags.append(f\"Coordinate outliers: {len(lat_outliers)} records\")\n",
    "        print(f\"⚠ FLAG: {len(lat_outliers)} records with outlier coordinates\")\n",
    "\n",
    "# Check for missing value patterns\n",
    "high_missing = missing_stats[missing_stats['missing_pct'] > 5]\n",
    "if not high_missing.empty:\n",
    "    for _, row in high_missing.iterrows():\n",
    "        flags.append(f\"High missing rate: {row['column']} ({row['missing_pct']:.1f}%)\")\n",
    "        print(f\"⚠ FLAG: {row['column']} has {row['missing_pct']:.1f}% missing values\")\n",
    "\n",
    "# Check for temporal gaps\n",
    "date_range = pd.date_range(\n",
    "    start=df_analysis[config.COL_DATE].min(),\n",
    "    end=df_analysis[config.COL_DATE].max(),\n",
    "    freq='D'\n",
    ")\n",
    "daily_counts_check = df_analysis.groupby(df_analysis[config.COL_DATE].dt.date).size()\n",
    "missing_dates = [d for d in date_range if d.date() not in daily_counts_check.index]\n",
    "if len(missing_dates) > 10:  # Allow for some missing days\n",
    "    flags.append(f\"Temporal gaps: {len(missing_dates)} days with no records\")\n",
    "    print(f\"⚠ FLAG: {len(missing_dates)} days have no recorded incidents\")\n",
    "\n",
    "if not flags:\n",
    "    print(\"✓ No major data quality flags identified\")\n",
    "\n",
    "# Save flags\n",
    "if flags:\n",
    "    flags_df = pd.DataFrame({'Flag': flags})\n",
    "    flags_df.to_csv(OUTPUT_TABLES_DIR / 'data_quality_flags.csv', index=False)\n",
    "    print(f\"\\n✓ Data quality flags saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate recommendations for downstream analyses\n",
    "print(\"\\n=== Recommendations for Downstream Notebooks ===\")\n",
    "print(\"\"\"\n",
    "1. TEMPORAL ANALYSIS (Notebook 03):\n",
    "   - Focus on STL decomposition for seasonality extraction\n",
    "   - Analyze trend significance with 95% confidence intervals\n",
    "   - Investigate any anomalous years identified above\n",
    "   - Compare seasonal patterns across offense categories\n",
    "\n",
    "2. GEOGRAPHIC ANALYSIS (Notebook 04):\n",
    "   - Use only geocoded records (filter out missing coordinates)\n",
    "   - Consider spatial autocorrelation (Moran's I) for formal clustering tests\n",
    "   - Address MAUP by analyzing at multiple scales\n",
    "   - Document coordinate outliers if present\n",
    "\n",
    "3. OFFENSE ANALYSIS (Notebook 05):\n",
    "   - Validate UCR code mappings against FBI standards\n",
    "   - Analyze trends separately for each major category\n",
    "   - Consider offense-specific seasonal patterns\n",
    "   - Document any offense types with unusual temporal patterns\n",
    "\n",
    "4. CROSS-FACTOR ANALYSIS (Notebook 07):\n",
    "   - Test hypotheses H1-H10 documented above\n",
    "   - Apply Bonferroni correction for multiple comparisons\n",
    "   - Report effect sizes (Cramer's V, correlation coefficients)\n",
    "   - Document any unexpected interactions\n",
    "\n",
    "5. GENERAL BEST PRACTICES:\n",
    "   - Always exclude last 30 days (reporting lag)\n",
    "   - Report 95% confidence intervals for all estimates\n",
    "   - Document limitations and assumptions explicitly\n",
    "   - Use consistent color schemes and figure formats\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Conclusion\n",
    "\n",
    "### Summary of Key Findings\n",
    "\n",
    "This exploratory analysis of 3.5M Philadelphia crime incidents (2006-2026) has established:\n",
    "\n",
    "**Data Quality:**\n",
    "- High geocoding coverage enables spatial analysis\n",
    "- Missing value patterns documented and interpreted\n",
    "- Reporting lag handled via 30-day exclusion\n",
    "\n",
    "**Temporal Patterns:**\n",
    "- Clear seasonal variation with summer peaks\n",
    "- Weekend vs weekday differences observed\n",
    "- Long-term trends vary by offense category\n",
    "\n",
    "**Geographic Patterns:**\n",
    "- Significant variation across police districts\n",
    "- Spatial clustering evident\n",
    "- Offense mix varies by geography\n",
    "\n",
    "**Offense Characteristics:**\n",
    "- Diverse offense types with clear hierarchy\n",
    "- Category-specific temporal patterns\n",
    "- Stable UCR code distribution over time\n",
    "\n",
    "**Deliverables Produced:**\n",
    "- 8+ publication-quality figures\n",
    "- Summary statistics tables\n",
    "- Correlation matrix\n",
    "- 10 testable hypotheses\n",
    "- Data quality flags\n",
    "\n",
    "**Next Steps:**\n",
    "Proceed to specialized notebooks (03-07) for detailed temporal, geographic, and offense-specific analyses."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
