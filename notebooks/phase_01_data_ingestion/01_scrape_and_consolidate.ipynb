{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1: Data Ingestion - Scrape and Consolidate\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook downloads monthly crime incident data from the OpenDataPhilly Carto API and consolidates it into a single optimized Parquet file. Run this notebook to refresh your dataset or create an initial dataset for analysis.\n",
    "\n",
    "### Steps\n",
    "1. **Scrape**: Download monthly CSVs from the OpenDataPhilly API\n",
    "2. **Consolidate**: Merge all months into a single DataFrame\n",
    "3. **Optimize**: Reduce file size by optimizing data types\n",
    "4. **Save**: Export to Parquet format for efficient storage and loading\n",
    "\n",
    "### Expected Output\n",
    "- Processed data file: `data/processed/crime_incidents_combined.parquet`\n",
    "- File size: ~100-200 MB (compressed with Parquet)\n",
    "- Records: ~3.5M+ (varies by API availability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Import and Configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "PROJECT_ROOT = Path.cwd().parent.parent\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Python version: {sys.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Run Scraper\n",
    "\n",
    "This cell calls the scraper script to download monthly crime data from OpenDataPhilly API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the scraper\n",
    "scraper_script = PROJECT_ROOT / \"scripts\" / \"helper\" / \"scrape.py\"\n",
    "\n",
    "print(\"Starting data scrape from OpenDataPhilly API...\")\n",
    "print(\"This may take several minutes depending on API response times.\\n\")\n",
    "\n",
    "result = subprocess.run([sys.executable, str(scraper_script)], capture_output=True, text=True)\n",
    "\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"STDERR:\", result.stderr)\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(\"\\n✓ Scrape completed successfully\")\n",
    "else:\n",
    "    print(f\"\\n✗ Scrape failed with return code {result.returncode}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Run Consolidation and Optimization\n",
    "\n",
    "This cell consolidates all monthly CSV files into a single optimized Parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the consolidation script\n",
    "consolidate_script = PROJECT_ROOT / \"scripts\" / \"helper\" / \"csv_to_parquet.py\"\n",
    "\n",
    "print(\"Consolidating CSV files and optimizing...\")\n",
    "print(\"This may take a few minutes for large datasets.\\n\")\n",
    "\n",
    "result = subprocess.run([sys.executable, str(consolidate_script)], capture_output=True, text=True)\n",
    "\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"STDERR:\", result.stderr)\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(\"\\n✓ Consolidation completed successfully\")\n",
    "else:\n",
    "    print(f\"\\n✗ Consolidation failed with return code {result.returncode}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Verify Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from src.data import loader\n",
    "\n",
    "# Load the consolidated data\n",
    "try:\n",
    "    df = loader.load_crime_data()\n",
    "    print(f\"✓ Successfully loaded {len(df):,} crime records\")\n",
    "    print(f\"\\nDataFrame shape: {df.shape}\")\n",
    "    print(f\"\\nColumns: {list(df.columns)}\")\n",
    "    print(f\"\\nData types:\\n{df.dtypes}\")\n",
    "    print(f\"\\nFirst few rows:\")\n",
    "    print(df.head())\n",
    "    print(f\"\\nDate range: {df['date'].min()} to {df['date'].max()}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error loading data: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "✓ **Data ingestion complete!** The consolidated dataset is ready for analysis.\n",
    "\n",
    "### What's Next?\n",
    "- Proceed to **Phase 2: Exploration** (`phase_02_exploration/01_data_overview.ipynb`) to understand the data structure and quality\n",
    "- Or skip directly to later phases if you've already completed exploration\n",
    "\n",
    "### Data Location\n",
    "- **Consolidated file**: `data/processed/crime_incidents_combined.parquet`\n",
    "- **Raw monthly CSVs**: `data/raw/` (organized by year/month)\n",
    "\n",
    "### To Refresh Data in Future Sessions\n",
    "- Simply re-run all cells in this notebook\n",
    "- Or create a scheduled task to run `scripts/helper/refresh_data.py` periodically"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
