{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3: Processing - Data Cleaning\n",
    "\n",
    "## Overview\n",
    "\n",
    "Clean and standardize the crime data for analysis.\n",
    "\n",
    "### Objectives\n",
    "1. Handle missing values (deletion vs. imputation)\n",
    "2. Remove or flag duplicates\n",
    "3. Parse and standardize date/time formats\n",
    "4. Fix geographic coordinate issues\n",
    "5. Standardize categorical fields (crime types, districts)\n",
    "6. Save cleaned data for next phases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Add project root to path\n",
    "PROJECT_ROOT = Path.cwd().parent.parent\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from src.data import loader\n",
    "from src.utils.config import get_processed_data_path\n",
    "\n",
    "print(\"Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = loader.load_crime_data()\n",
    "print(f\"Initial records: {len(df):,}\")\n",
    "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Handling missing values...\\n\")\n",
    "\n",
    "# Document missing values\n",
    "missing_before = df.isnull().sum()\n",
    "print(f\"Missing values before cleaning:\")\n",
    "print(missing_before[missing_before > 0])\n",
    "\n",
    "# Strategy: Drop rows with missing critical fields\n",
    "critical_cols = ['date', 'latitude', 'longitude']\n",
    "critical_missing = [col for col in critical_cols if col in df.columns]\n",
    "df_cleaned = df.dropna(subset=critical_missing)\n",
    "\n",
    "print(f\"\\nRows removed due to missing critical fields: {len(df) - len(df_cleaned):,}\")\nprint(f\"Records after dropping critical missing: {len(df_cleaned):,}\")\n",
    "\n",
    "df = df_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Remove Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Checking for duplicates...\\n\")\n",
    "\n",
    "before_dedup = len(df)\n",
    "df = df.drop_duplicates()\n",
    "after_dedup = len(df)\n",
    "\n",
    "print(f\"Duplicate rows removed: {before_dedup - after_dedup:,}\")\nprint(f\"Records after deduplication: {after_dedup:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Standardize Date/Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Standardizing date/time fields...\\n\")\n",
    "\n",
    "if 'date' in df.columns:\n",
    "    df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "    invalid_dates = df['date'].isnull().sum()\n",
    "    if invalid_dates > 0:\n",
    "        print(f\"⚠ {invalid_dates:,} invalid date values found (set to NaT)\")\n",
    "        df = df.dropna(subset=['date'])\n",
    "    print(f\"Date range: {df['date'].min()} to {df['date'].max()}\")\n\nif 'time_24hr' in df.columns:\n",
    "    print(f\"\\nTime range: {df['time_24hr'].min()} to {df['time_24hr'].max()}\")\n    \nprint(f\"\\nRecords after date standardization: {len(df):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Fix Geographic Coordinates"
   ]
  },
  {
   "cell_type": {"cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Fix Geographic Coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Validating geographic coordinates...\\n\")\n",
    "\n",
    "# Philadelphia bounds\n",
    "PHI_LAT_MIN, PHI_LAT_MAX = 39.8, 40.1\n",
    "PHI_LON_MIN, PHI_LON_MAX = -75.3, -74.9\n",
    "\n",
    "if 'latitude' in df.columns and 'longitude' in df.columns:\n",
    "    invalid_before = len(df[\n",
    "        (df['latitude'] < PHI_LAT_MIN) | (df['latitude'] > PHI_LAT_MAX) |\n",
    "        (df['longitude'] < PHI_LON_MIN) | (df['longitude'] > PHI_LON_MAX)\n",
    "    ])\n",
    "    \n",
    "    # Remove records with invalid coordinates\n",
    "    df = df[\n",
    "        (df['latitude'] >= PHI_LAT_MIN) & (df['latitude'] <= PHI_LAT_MAX) &\n",
    "        (df['longitude'] >= PHI_LON_MIN) & (df['longitude'] <= PHI_LON_MAX)\n",
    "    ]\n",
    "    \n",
    "    print(f\"Records with invalid coordinates removed: {invalid_before:,}\")\n    print(f\"Coordinates range:\")\n    print(f\"  Latitude: {df['latitude'].min():.4f} to {df['latitude'].max():.4f}\")\n    print(f\"  Longitude: {df['longitude'].min():.4f} to {df['longitude'].max():.4f}\")\n    print(f\"\\nRecords after geo validation: {len(df):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Standardize Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Standardizing categorical fields...\\n\")\n",
    "\n",
    "# Standardize text fields (strip whitespace, lowercase for consistency)\n",
    "text_cols = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "for col in text_cols:\n",
    "    if col not in ['date', 'time_24hr']:  # Skip date/time\n",
    "        df[col] = df[col].str.strip().str.title()  # Title case for display\n",
    "        \n",
    "print(\"✓ Text fields standardized\")\n",
    "print(f\"\\nRecords after cleaning: {len(df):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: Save Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned data\n",
    "try:\n",
    "    processed_path = get_processed_data_path()\n",
    "    cleaned_file = processed_path / \"crime_incidents_cleaned.parquet\"\n",
    "    \n",
    "    df.to_parquet(cleaned_file, engine='pyarrow', compression='snappy')\n",
    "    \n",
    "    file_size_mb = cleaned_file.stat().st_size / 1024**2\n",
    "    print(f\"✓ Cleaned data saved to {cleaned_file}\")\n",
    "    print(f\"  File size: {file_size_mb:.2f} MB\")\n",
    "    print(f\"  Records: {len(df):,}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error saving cleaned data: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "✓ **Data cleaning complete!**\n",
    "\n",
    "### Changes Made\n",
    "- Removed rows with missing critical fields\n",
    "- Removed duplicate records\n",
    "- Standardized date/time formats\n",
    "- Validated and cleaned geographic coordinates\n",
    "- Standardized categorical text fields\n",
    "\n",
    "### Output\n",
    "- Cleaned data: `data/processed/crime_incidents_cleaned.parquet`\n",
    "\n",
    "### Next Steps\n",
    "- Proceed to **02_feature_engineering.ipynb** to create analytical features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
