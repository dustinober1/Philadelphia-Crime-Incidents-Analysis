{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3: Processing - Data Cleaning\n",
    "\n",
    "## Overview\n",
    "\n",
    "Clean and standardize the crime data for analysis.\n",
    "\n",
    "### Objectives\n",
    "1. Handle missing values (deletion vs. imputation)\n",
    "2. Remove or flag duplicates\n",
    "3. Parse and standardize date/time formats\n",
    "4. Fix geographic coordinate issues\n",
    "5. Standardize categorical fields (crime types, districts)\n",
    "6. Save cleaned data for next phases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports successful\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Add project root to path\n",
    "PROJECT_ROOT = Path.cwd().parent.parent\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from src.data import loader\n",
    "from src.utils.config import get_processed_data_path\n",
    "\n",
    "print(\"Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial records: 3,496,353\n",
      "Memory usage: 1124.02 MB\n"
     ]
    }
   ],
   "source": [
    "df = loader.load_crime_data()\n",
    "print(f\"Initial records: {len(df):,}\")\n",
    "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling missing values...\n",
      "\n",
      "Missing values before cleaning:\n",
      "the_geom                 55810\n",
      "the_geom_webmercator     55927\n",
      "psa                       1296\n",
      "hour                    102245\n",
      "location_block             187\n",
      "point_x                  55912\n",
      "point_y                  55912\n",
      "dtype: int64\n",
      "\n",
      "Rows removed due to missing critical fields: 0\n",
      "Records after dropping critical missing: 3,496,353\n"
     ]
    }
   ],
   "source": [
    "print(\"Handling missing values...\\n\")\n",
    "\n",
    "# Document missing values\n",
    "missing_before = df.isnull().sum()\n",
    "print(f\"Missing values before cleaning:\")\n",
    "print(missing_before[missing_before > 0])\n",
    "\n",
    "# Strategy: Drop rows with missing critical fields\n",
    "critical_cols = ['date', 'latitude', 'longitude']\n",
    "critical_missing = [col for col in critical_cols if col in df.columns]\n",
    "df_cleaned = df.dropna(subset=critical_missing)\n",
    "\n",
    "print(f\"\\nRows removed due to missing critical fields: {len(df) - len(df_cleaned):,}\")\n",
    "print(f\"Records after dropping critical missing: {len(df_cleaned):,}\")\n",
    "\n",
    "df = df_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Remove Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for duplicates...\n",
      "\n",
      "Duplicate rows removed: 0\n",
      "Records after deduplication: 3,496,353\n"
     ]
    }
   ],
   "source": [
    "print(\"Checking for duplicates...\\n\")\n",
    "\n",
    "before_dedup = len(df)\n",
    "df = df.drop_duplicates()\n",
    "after_dedup = len(df)\n",
    "\n",
    "print(f\"Duplicate rows removed: {before_dedup - after_dedup:,}\")\n",
    "print(f\"Records after deduplication: {after_dedup:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Standardize Date/Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardizing date/time fields...\n",
      "\n",
      "\n",
      "Records after date standardization: 3,496,353\n"
     ]
    }
   ],
   "source": [
    "print(\"Standardizing date/time fields...\\n\")\n",
    "\n",
    "if 'date' in df.columns:\n",
    "    df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "    invalid_dates = df['date'].isnull().sum()\n",
    "    if invalid_dates > 0:\n",
    "        print(f\"⚠ {invalid_dates:,} invalid date values found (set to NaT)\")\n",
    "        df = df.dropna(subset=['date'])\n",
    "    print(f\"Date range: {df['date'].min()} to {df['date'].max()}\")\n",
    "\n",
    "if 'time_24hr' in df.columns:\n",
    "    print(f\"\\nTime range: {df['time_24hr'].min()} to {df['time_24hr'].max()}\")\n",
    "    \n",
    "print(f\"\\nRecords after date standardization: {len(df):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Fix Geographic Coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Fix Geographic Coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating geographic coordinates...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Validating geographic coordinates...\\n\")\n",
    "\n",
    "# Philadelphia bounds\n",
    "PHI_LAT_MIN, PHI_LAT_MAX = 39.8, 40.1\n",
    "PHI_LON_MIN, PHI_LON_MAX = -75.3, -74.9\n",
    "\n",
    "if 'latitude' in df.columns and 'longitude' in df.columns:\n",
    "    invalid_before = len(df[\n",
    "        (df['latitude'] < PHI_LAT_MIN) | (df['latitude'] > PHI_LAT_MAX) |\n",
    "        (df['longitude'] < PHI_LON_MIN) | (df['longitude'] > PHI_LON_MAX)\n",
    "    ])\n",
    "    \n",
    "    # Remove records with invalid coordinates\n",
    "    df = df[\n",
    "        (df['latitude'] >= PHI_LAT_MIN) & (df['latitude'] <= PHI_LAT_MAX) &\n",
    "        (df['longitude'] >= PHI_LON_MIN) & (df['longitude'] <= PHI_LON_MAX)\n",
    "    ]\n",
    "    \n",
    "    print(f\"Records with invalid coordinates removed: {invalid_before:,}\")\n",
    "    print(f\"Coordinates range:\")\n",
    "    print(f\"  Latitude: {df['latitude'].min():.4f} to {df['latitude'].max():.4f}\")\n",
    "    print(f\"  Longitude: {df['longitude'].min():.4f} to {df['longitude'].max():.4f}\")\n",
    "    print(f\"\\nRecords after geo validation: {len(df):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Standardize Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardizing categorical fields...\n",
      "\n",
      "✓ Text fields standardized\n",
      "\n",
      "Records after cleaning: 3,496,353\n"
     ]
    }
   ],
   "source": [
    "print(\"Standardizing categorical fields...\\n\")\n",
    "\n",
    "# Standardize text fields (strip whitespace, title case)\n",
    "# Handle PyArrow-backed strings by converting to regular strings first\n",
    "text_cols = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "for col in text_cols:\n",
    "    if col not in ['date', 'time_24hr']:  # Skip date/time\n",
    "        try:\n",
    "            # Convert to string if needed, apply string operations, then convert back to category if it was categorical\n",
    "            if hasattr(df[col], 'dtype') and df[col].dtype.name.startswith('string'):\n",
    "                df[col] = df[col].astype(str).str.strip().str.title()\n",
    "            elif hasattr(df[col], 'dtype') and 'string' in str(df[col].dtype):\n",
    "                df[col] = df[col].astype(str).str.strip().str.title()\n",
    "        except:\n",
    "            pass  # Skip columns that can't be standardized\n",
    "        \n",
    "print(\"✓ Text fields standardized\")\n",
    "print(f\"\\nRecords after cleaning: {len(df):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: Save Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Cleaned data saved to /Users/dustinober/Projects/Crime Incidents Philadelphia/data/processed/crime_incidents_cleaned.parquet\n",
      "  File size: 269.44 MB\n",
      "  Records: 3,496,353\n"
     ]
    }
   ],
   "source": [
    "# Save cleaned data\n",
    "try:\n",
    "    processed_path = get_processed_data_path()\n",
    "    cleaned_file = processed_path / \"crime_incidents_cleaned.parquet\"\n",
    "    \n",
    "    df.to_parquet(cleaned_file, engine='pyarrow', compression='snappy')\n",
    "    \n",
    "    file_size_mb = cleaned_file.stat().st_size / 1024**2\n",
    "    print(f\"✓ Cleaned data saved to {cleaned_file}\")\n",
    "    print(f\"  File size: {file_size_mb:.2f} MB\")\n",
    "    print(f\"  Records: {len(df):,}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error saving cleaned data: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "✓ **Data cleaning complete!**\n",
    "\n",
    "### Changes Made\n",
    "- Removed rows with missing critical fields\n",
    "- Removed duplicate records\n",
    "- Standardized date/time formats\n",
    "- Validated and cleaned geographic coordinates\n",
    "- Standardized categorical text fields\n",
    "\n",
    "### Output\n",
    "- Cleaned data: `data/processed/crime_incidents_cleaned.parquet`\n",
    "\n",
    "### Next Steps\n",
    "- Proceed to **02_feature_engineering.ipynb** to create analytical features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
