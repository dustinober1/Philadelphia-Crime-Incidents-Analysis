"""Tests for pipeline refresh operations (pipeline/refresh_data.py)."""

from __future__ import annotations

import json
from pathlib import Path
from unittest.mock import patch

import pytest
from typer.testing import CliRunner

from pipeline.refresh_data import (
    _REQUIRED_FILES,
    _assert_reproducible,
    _canonical_json,
    _load_json,
    _validate_artifacts,
    app,
)

runner = CliRunner()


def _create_minimal_valid_files(output_dir: Path) -> None:
    """Create minimal valid files for all required exports."""
    valid_metadata = {
        "total_incidents": 100000,
        "date_start": "2006-01-01",
        "date_end": "2024-12-31",
        "last_updated": "2025-01-15T10:30:00Z",
        "source": "Philadelphia Police Department",
        "colors": {"crime": "#FF5733"},
    }
    valid_annual_trends = [{"year": 2020, "incidents": 10000}]
    valid_forecast = {"historical": [{"date": "2023-01-01", "value": 100}], "forecast": [{"date": "2024-01-01", "value": 110}]}

    for file_path in _REQUIRED_FILES:
        full_path = output_dir / file_path
        full_path.parent.mkdir(parents=True, exist_ok=True)
        if file_path == "metadata.json":
            full_path.write_text(json.dumps(valid_metadata))
        elif file_path == "annual_trends.json":
            full_path.write_text(json.dumps(valid_annual_trends))
        elif file_path == "forecast.json":
            full_path.write_text(json.dumps(valid_forecast))
        else:
            full_path.write_text("{}")


class TestValidateArtifactsSuccess:
    """Tests for _validate_artifacts success cases."""

    def test_validate_artifacts_passes_with_complete_exports(self, tmp_path: Path) -> None:
        """Should pass when all required export files exist."""
        # Create minimal valid files
        _create_minimal_valid_files(tmp_path)

        # Should not raise any exception
        _validate_artifacts(tmp_path)

    def test_validate_artifacts_passes_with_valid_metadata(self, tmp_path: Path) -> None:
        """Should pass when metadata.json has all required keys."""
        # Create all required files with valid metadata
        _create_minimal_valid_files(tmp_path)

        # Should not raise any exception
        _validate_artifacts(tmp_path)

    def test_validate_artifacts_passes_with_valid_annual_trends(self, tmp_path: Path) -> None:
        """Should pass when annual_trends.json is a non-empty list."""
        # Create all required files with valid annual trends
        _create_minimal_valid_files(tmp_path)

        # Should not raise any exception
        _validate_artifacts(tmp_path)

    def test_validate_artifacts_passes_with_valid_forecast(self, tmp_path: Path) -> None:
        """Should pass when forecast.json contains historical and forecast keys."""
        # Create all required files with valid forecast
        _create_minimal_valid_files(tmp_path)

        # Should not raise any exception
        _validate_artifacts(tmp_path)


class TestValidateArtifactsFailure:
    """Tests for _validate_artifacts failure cases."""

    def test_validate_artifacts_raises_missing_files(self, tmp_path: Path) -> None:
        """Should raise RuntimeError when required files are missing."""
        # Create only metadata.json to trigger missing file error
        (tmp_path / "metadata.json").write_text(json.dumps({
            "total_incidents": 100000,
            "date_start": "2006-01-01",
            "date_end": "2024-12-31",
            "last_updated": "2025-01-15T10:30:00Z",
            "source": "Philadelphia Police Department",
            "colors": {"crime": "#FF5733"},
        }))

        with pytest.raises(RuntimeError, match="Missing required export files"):
            _validate_artifacts(tmp_path)

    def test_validate_artifacts_raises_invalid_metadata_keys(self, tmp_path: Path) -> None:
        """Should raise RuntimeError when metadata.json missing required keys."""
        # Create all files with invalid metadata
        for file_path in _REQUIRED_FILES:
            full_path = tmp_path / file_path
            full_path.parent.mkdir(parents=True, exist_ok=True)
            if file_path == "metadata.json":
                full_path.write_text(json.dumps({"total_incidents": 100000}))  # Missing required keys
            elif file_path == "annual_trends.json":
                full_path.write_text(json.dumps([{"year": 2020, "incidents": 10000}]))
            elif file_path == "forecast.json":
                full_path.write_text(json.dumps({
                    "historical": [{"date": "2023-01-01", "value": 100}],
                    "forecast": [{"date": "2024-01-01", "value": 110}],
                }))
            else:
                full_path.write_text("{}")

        with pytest.raises(RuntimeError, match="metadata.json is missing required keys"):
            _validate_artifacts(tmp_path)

    def test_validate_artifacts_raises_invalid_annual_trends(self, tmp_path: Path) -> None:
        """Should raise RuntimeError when annual_trends.json is not a list."""
        # Create all files with invalid annual_trends
        for file_path in _REQUIRED_FILES:
            full_path = tmp_path / file_path
            full_path.parent.mkdir(parents=True, exist_ok=True)
            if file_path == "metadata.json":
                full_path.write_text(json.dumps({
                    "total_incidents": 100000,
                    "date_start": "2006-01-01",
                    "date_end": "2024-12-31",
                    "last_updated": "2025-01-15T10:30:00Z",
                    "source": "Philadelphia Police Department",
                    "colors": {"crime": "#FF5733"},
                }))
            elif file_path == "annual_trends.json":
                full_path.write_text(json.dumps({"not": "a list"}))  # Not a list
            elif file_path == "forecast.json":
                full_path.write_text(json.dumps({
                    "historical": [{"date": "2023-01-01", "value": 100}],
                    "forecast": [{"date": "2024-01-01", "value": 110}],
                }))
            else:
                full_path.write_text("{}")

        with pytest.raises(RuntimeError, match="annual_trends.json must be a non-empty list"):
            _validate_artifacts(tmp_path)

    def test_validate_artifacts_raises_empty_annual_trends(self, tmp_path: Path) -> None:
        """Should raise RuntimeError when annual_trends.json is empty list."""
        # Create all files with empty annual_trends
        for file_path in _REQUIRED_FILES:
            full_path = tmp_path / file_path
            full_path.parent.mkdir(parents=True, exist_ok=True)
            if file_path == "metadata.json":
                full_path.write_text(json.dumps({
                    "total_incidents": 100000,
                    "date_start": "2006-01-01",
                    "date_end": "2024-12-31",
                    "last_updated": "2025-01-15T10:30:00Z",
                    "source": "Philadelphia Police Department",
                    "colors": {"crime": "#FF5733"},
                }))
            elif file_path == "annual_trends.json":
                full_path.write_text(json.dumps([]))  # Empty list
            elif file_path == "forecast.json":
                full_path.write_text(json.dumps({
                    "historical": [{"date": "2023-01-01", "value": 100}],
                    "forecast": [{"date": "2024-01-01", "value": 110}],
                }))
            else:
                full_path.write_text("{}")

        with pytest.raises(RuntimeError, match="annual_trends.json must be a non-empty list"):
            _validate_artifacts(tmp_path)

    def test_validate_artifacts_raises_invalid_forecast_structure(self, tmp_path: Path) -> None:
        """Should raise RuntimeError when forecast.json missing historical/forecast."""
        # Create all files with invalid forecast structure
        for file_path in _REQUIRED_FILES:
            full_path = tmp_path / file_path
            full_path.parent.mkdir(parents=True, exist_ok=True)
            if file_path == "metadata.json":
                full_path.write_text(json.dumps({
                    "total_incidents": 100000,
                    "date_start": "2006-01-01",
                    "date_end": "2024-12-31",
                    "last_updated": "2025-01-15T10:30:00Z",
                    "source": "Philadelphia Police Department",
                    "colors": {"crime": "#FF5733"},
                }))
            elif file_path == "annual_trends.json":
                full_path.write_text(json.dumps([{"year": 2020, "incidents": 10000}]))
            elif file_path == "forecast.json":
                full_path.write_text(json.dumps({"only": "historical"}))  # Missing forecast key
            else:
                full_path.write_text("{}")

        with pytest.raises(RuntimeError, match="forecast.json must contain historical and forecast fields"):
            _validate_artifacts(tmp_path)


class TestLoadJson:
    """Tests for _load_json helper."""

    def test_load_json_parses_valid_file(self, tmp_path: Path) -> None:
        """Should parse valid JSON file and return dict."""
        test_data = {"key": "value", "number": 42, "nested": {"a": 1}}
        test_file = tmp_path / "test.json"
        test_file.write_text(json.dumps(test_data))

        result = _load_json(test_file)
        assert result == test_data
        assert result["key"] == "value"
        assert result["number"] == 42
        assert result["nested"]["a"] == 1

    def test_load_json_handles_unicode(self, tmp_path: Path) -> None:
        """Should handle Unicode characters in JSON."""
        test_data = {
            "emoji": "ðŸš¨ Crime Data",
            "international": "çŠ¯ç½ªäº‹ä»¶",
            "special_chars": "Â©Â®â„¢Â£â‚¬Â¢Â¥",
        }
        test_file = tmp_path / "unicode.json"
        test_file.write_text(json.dumps(test_data), encoding="utf-8")

        result = _load_json(test_file)
        assert result["emoji"] == "ðŸš¨ Crime Data"
        assert result["international"] == "çŠ¯ç½ªäº‹ä»¶"
        assert result["special_chars"] == "Â©Â®â„¢Â£â‚¬Â¢Â¥"


class TestCanonicalJson:
    """Tests for _canonical_json helper."""

    def test_canonical_json_sorts_keys(self, tmp_path: Path) -> None:
        """Should return JSON with sorted keys for comparison."""
        test_file = tmp_path / "test.json"
        # Create JSON with keys in random order
        test_file.write_text(json.dumps({"z": 1, "a": 2, "m": 3}))

        result = _canonical_json(test_file)
        # Keys should be sorted alphabetically
        assert result == '{"a":2,"m":3,"z":1}'

    def test_canonical_json_removes_whitespace(self, tmp_path: Path) -> None:
        """Should return compact JSON with no extra whitespace."""
        test_file = tmp_path / "test.json"
        # Create JSON with formatting
        test_data = {"key": "value", "nested": {"a": 1, "b": 2}}
        test_file.write_text(json.dumps(test_data, indent=2))

        result = _canonical_json(test_file)
        # Should have no extra whitespace
        assert "  " not in result  # No indentation spaces
        assert "\n" not in result  # No newlines


class TestAssertReproducible:
    """Tests for _assert_reproducible reproducibility verification."""

    @patch("pipeline.refresh_data.export_all")
    def test_assert_reproducible_passes_deterministic_exports(self, mock_export: patch, tmp_path: Path) -> None:
        """Should pass when export_all returns same data twice."""
        # Create deterministic export directory with same content
        run_dir = tmp_path / "run"
        run_dir.mkdir(parents=True, exist_ok=True)

        # Create identical files for both runs
        def mock_export_func(output_dir: Path) -> Path:
            for file_path in _REQUIRED_FILES:
                full_path = output_dir / file_path
                full_path.parent.mkdir(parents=True, exist_ok=True)
                # Write deterministic content
                full_path.write_text('{"same":"data"}')
            return output_dir

        mock_export.side_effect = mock_export_func

        # Should not raise any exception
        _assert_reproducible()

        assert mock_export.call_count == 2

    @patch("pipeline.refresh_data.export_all")
    def test_assert_reproducible_detects_differences(self, mock_export: patch, tmp_path: Path) -> None:
        """Should raise RuntimeError when exports differ."""
        call_count = [0]

        def mock_export_func(output_dir: Path) -> Path:
            call_count[0] += 1
            for file_path in _REQUIRED_FILES:
                full_path = output_dir / file_path
                full_path.parent.mkdir(parents=True, exist_ok=True)
                # Write different content on each call
                full_path.write_text(f'{{"run":{call_count[0]}}}')
            return output_dir

        mock_export.side_effect = mock_export_func

        with pytest.raises(RuntimeError, match="Reproducibility check failed"):
            _assert_reproducible()

        assert mock_export.call_count == 2

    @patch("pipeline.refresh_data.export_all")
    def test_assert_reproducible_identifies_differing_files(self, mock_export: patch, tmp_path: Path) -> None:
        """Should list which files differ in error message."""
        call_count = [0]

        def mock_export_func(output_dir: Path) -> Path:
            call_count[0] += 1
            for file_path in _REQUIRED_FILES:
                full_path = output_dir / file_path
                full_path.parent.mkdir(parents=True, exist_ok=True)
                # Write different content on each call
                full_path.write_text(f'{{"run":{call_count[0]}}}')
            return output_dir

        mock_export.side_effect = mock_export_func

        with pytest.raises(RuntimeError) as exc_info:
            _assert_reproducible()

        # Verify error message contains at least one of the differing files
        error_msg = str(exc_info.value)
        # All files should differ since they all have different content
        assert "metadata.json" in error_msg or "annual_trends.json" in error_msg

        assert mock_export.call_count == 2


    @patch("pipeline.refresh_data.export_all")
    def test_assert_reproducible_compares_all_required_files(self, mock_export: patch, tmp_path: Path) -> None:
        """Should compare each file in _REQUIRED_FILES."""
        def mock_export_func(output_dir: Path) -> Path:
            # Create all required files
            for file_path in _REQUIRED_FILES:
                full_path = output_dir / file_path
                full_path.parent.mkdir(parents=True, exist_ok=True)
                full_path.write_text('{"data":"value"}')
            return output_dir

        mock_export.side_effect = mock_export_func

        # Should not raise when all files match
        _assert_reproducible()

        # Verify export_all was called twice
        assert mock_export.call_count == 2


class TestRefreshCliRun:
    """Tests for refresh CLI run command."""

    @patch("pipeline.refresh_data.export_all")
    def test_refresh_run_creates_exports(self, mock_export: patch, tmp_path: Path) -> None:
        """Should create exports when run command invoked."""
        def mock_export_func(output_dir: Path) -> Path:
            _create_minimal_valid_files(output_dir)
            return output_dir

        mock_export.side_effect = mock_export_func

        result = runner.invoke(app, ["--output-dir", str(tmp_path)])

        assert result.exit_code == 0
        assert "Validated exports:" in result.stdout
        assert str(tmp_path) in result.stdout
        mock_export.assert_called_once()

    @patch("pipeline.refresh_data.export_all")
    def test_refresh_run_validates_artifacts(self, mock_export: patch, tmp_path: Path) -> None:
        """Should validate artifacts after export."""
        def mock_export_func(output_dir: Path) -> Path:
            _create_minimal_valid_files(output_dir)
            return output_dir

        mock_export.side_effect = mock_export_func

        result = runner.invoke(app, ["--output-dir", str(tmp_path)])

        assert result.exit_code == 0
        # Validation passes (no error message in stdout)
        assert "Validated exports:" in result.stdout

    @patch("pipeline.refresh_data.export_all")
    def test_refresh_run_custom_output_dir(self, mock_export: patch, tmp_path: Path) -> None:
        """Should use custom output directory when specified."""
        custom_dir = tmp_path / "custom_output"

        def mock_export_func(output_dir: Path) -> Path:
            _create_minimal_valid_files(output_dir)
            return output_dir

        mock_export.side_effect = mock_export_func

        result = runner.invoke(app, ["--output-dir", str(custom_dir)])

        assert result.exit_code == 0
        assert str(custom_dir) in result.stdout
        mock_export.assert_called_once()

    @patch("pipeline.refresh_data.export_all")
    @patch("pipeline.refresh_data._assert_reproducible")
    def test_refresh_run_with_verify_reproducibility(self, mock_repro: patch, mock_export: patch, tmp_path: Path) -> None:
        """Should run reproducibility check when flag provided."""
        def mock_export_func(output_dir: Path) -> Path:
            _create_minimal_valid_files(output_dir)
            return output_dir

        mock_export.side_effect = mock_export_func

        result = runner.invoke(app, ["--output-dir", str(tmp_path), "--verify-reproducibility"])

        assert result.exit_code == 0
        assert "Validated exports:" in result.stdout
        assert "Reproducibility check passed" in result.stdout
        mock_repro.assert_called_once()


# =============================================================================
# Task 4: Corrupt Artifact Detection Tests
# =============================================================================


class TestCorruptArtifactDetection:
    """Test detection of corrupt or malformed export artifacts."""

    def test_validate_artifacts_raises_invalid_json(self, tmp_path: Path) -> None:
        """Verify error when JSON file is malformed."""
        # Create minimal files for all but one
        _create_minimal_valid_files(tmp_path)

        # Corrupt metadata.json with invalid JSON
        (tmp_path / "metadata.json").write_text('{"invalid": json}')

        # Current behavior: JSONDecodeError is raised (not converted to RuntimeError)
        with pytest.raises(json.JSONDecodeError):
            _validate_artifacts(tmp_path)

    def test_validate_artifacts_raises_wrong_type_metadata(self, tmp_path: Path) -> None:
        """Verify error when metadata.json is not a dict."""
        # Create minimal files
        _create_minimal_valid_files(tmp_path)

        # Write metadata as a list instead of dict
        (tmp_path / "metadata.json").write_text(json.dumps(["not", "a", "dict"]))

        # Current behavior: AttributeError on list.keys() (not ideal but that's what happens)
        with pytest.raises(AttributeError, match="keys"):
            _validate_artifacts(tmp_path)

    def test_validate_artifacts_raises_missing_nested_keys(self, tmp_path: Path) -> None:
        """Verify RuntimeError when nested keys missing (e.g., forecast.historical)."""
        # Create minimal files
        _create_minimal_valid_files(tmp_path)

        # Write forecast without required nested keys
        (tmp_path / "forecast.json").write_text(json.dumps({"historical": [], "other": "data"}))

        with pytest.raises(RuntimeError, match="forecast.json must contain historical and forecast fields"):
            _validate_artifacts(tmp_path)

    def test_load_json_handles_invalid_json(self, tmp_path: Path) -> None:
        """Verify _load_json raises JSONDecodeError for malformed JSON."""
        test_file = tmp_path / "invalid.json"
        test_file.write_text('{"malformed": json}')

        with pytest.raises(json.JSONDecodeError):
            _load_json(test_file)


class TestRefreshEnvVar:
    """Tests for environment variable configuration."""

    @patch("pipeline.refresh_data.export_all")
    def test_refresh_run_respects_explicit_output_dir_over_env(self, mock_export: patch, tmp_path: Path) -> None:
        """Explicit --output-dir should take precedence over env var."""
        env_dir = tmp_path / "env_output"
        explicit_dir = tmp_path / "explicit_output"

        def mock_export_func(output_dir: Path) -> Path:
            _create_minimal_valid_files(output_dir)
            return output_dir

        mock_export.side_effect = mock_export_func

        # Set env var but also pass explicit --output-dir
        result = runner.invoke(
            app,
            ["--output-dir", str(explicit_dir)],
            env={"PIPELINE_OUTPUT_DIR": str(env_dir)}
        )

        assert result.exit_code == 0
        # Verify export was called with explicit directory, not env directory
        mock_export.assert_called_once()
        call_path = mock_export.call_args[0][0]
        # Should use explicit dir
        assert explicit_dir == call_path or str(explicit_dir) in str(call_path)


# =============================================================================
# Task 6: CLI Error Handling Tests
# =============================================================================


class TestCliErrorHandling:
    """Test CLI error handling and exit codes."""

    @patch("pipeline.refresh_data.export_all")
    def test_refresh_run_exits_nonzero_on_validation_error(self, mock_export: patch, tmp_path: Path) -> None:
        """Verify exit_code != 0 when validation fails."""
        # Mock export to create incomplete set of files (missing forecast.json)
        def incomplete_export_func(output_dir: Path) -> Path:
            (output_dir / "metadata.json").parent.mkdir(parents=True, exist_ok=True)
            (output_dir / "metadata.json").write_text(json.dumps({
                "total_incidents": 100000,
                "date_start": "2006-01-01",
                "date_end": "2024-12-31",
                "last_updated": "2025-01-15T10:30:00Z",
                "source": "Philadelphia Police Department",
                "colors": {"crime": "#FF5733"},
            }))
            # Don't create all required files - this will trigger validation error
            return output_dir

        mock_export.side_effect = incomplete_export_func

        result = runner.invoke(app, ["run", "--output-dir", str(tmp_path)])

        # Should exit with non-zero code
        assert result.exit_code != 0

    @patch("pipeline.refresh_data.export_all")
    @patch("pipeline.refresh_data._assert_reproducible")
    def test_refresh_run_exits_nonzero_on_reproducibility_failure(
        self, mock_repro: patch, mock_export: patch, tmp_path: Path
    ) -> None:
        """Verify exit_code != 0 when reproducibility check fails."""
        def mock_export_func(output_dir: Path) -> Path:
            _create_minimal_valid_files(output_dir)
            return output_dir

        mock_export.side_effect = mock_export_func
        # Mock reproducibility check to fail
        mock_repro.side_effect = RuntimeError("Reproducibility check failed for: metadata.json")

        result = runner.invoke(app, ["run", "--output-dir", str(tmp_path), "--verify-reproducibility"])

        # Should exit with non-zero code
        assert result.exit_code != 0

    @patch("pipeline.refresh_data.export_all")
    def test_refresh_run_shows_error_message(self, mock_export: patch, tmp_path: Path) -> None:
        """Verify stderr contains error message on failure."""
        # Mock export to create incomplete files
        def incomplete_export_func(output_dir: Path) -> Path:
            (output_dir / "metadata.json").parent.mkdir(parents=True, exist_ok=True)
            (output_dir / "metadata.json").write_text(json.dumps({
                "total_incidents": 100000,
                "date_start": "2006-01-01",
                "date_end": "2024-12-31",
                "last_updated": "2025-01-15T10:30:00Z",
                "source": "Philadelphia Police Department",
                "colors": {"crime": "#FF5733"},
            }))
            return output_dir

        mock_export.side_effect = incomplete_export_func

        result = runner.invoke(app, ["run", "--output-dir", str(tmp_path)])

        # Should show error message
        assert result.exit_code != 0
        # Error may be in stdout or stderr depending on Typer configuration
        error_output = result.stdout + result.stderr
        assert "Missing required export files" in error_output or len(error_output) > 0

    def test_validate_artifacts_zero_incidents(self, tmp_path: Path) -> None:
        """Verify validation passes when total_incidents is 0."""
        # Create all required files with zero incidents
        for file_path in _REQUIRED_FILES:
            full_path = tmp_path / file_path
            full_path.parent.mkdir(parents=True, exist_ok=True)
            if file_path == "metadata.json":
                full_path.write_text(json.dumps({
                    "total_incidents": 0,  # Zero incidents is valid
                    "date_start": "2006-01-01",
                    "date_end": "2024-12-31",
                    "last_updated": "2025-01-15T10:30:00Z",
                    "source": "Philadelphia Police Department",
                    "colors": {"crime": "#FF5733"},
                }))
            elif file_path == "annual_trends.json":
                full_path.write_text(json.dumps([{"year": 2020, "count": 0}]))  # Empty but valid structure
            elif file_path == "forecast.json":
                full_path.write_text(json.dumps({
                    "historical": [],
                    "forecast": [],
                }))
            else:
                full_path.write_text("{}")

        # Should validate successfully even with 0 incidents
        _validate_artifacts(tmp_path)
